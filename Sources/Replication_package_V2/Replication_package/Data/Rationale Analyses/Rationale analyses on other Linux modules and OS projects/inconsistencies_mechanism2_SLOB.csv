Decision1;Rationale1;Decision2;Rationale2;Relationship;Alpha;similarity_rationales
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8184511065483093;0.3307259976863861
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.9546252489089966;0.550234317779541
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9623862504959106;0.38673120737075806
For most allocations, this works well, as the vast majority of callers are not expecting to use more memory than what they asked for;The vast majority of callers are not expecting to use more memory than what they asked for;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.966339647769928;0.3904288709163666
For most allocations, this works well, as the vast majority of callers are not expecting to use more memory than what they asked for;The vast majority of callers are not expecting to use more memory than what they asked for;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9795199632644652;0.3544168174266815
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;Contradicts;0.9056867957115172;0.43925222754478455
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9757909178733826;0.18863071501255035
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Contradicts;0.8599486351013184;0.4066273272037506
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9037435054779052;0.4111992418766022
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8492074608802795;0.6179263591766357
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8476876616477966;0.620715856552124
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9802173376083374;0.5195223093032837
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9744083881378174;0.4854811728000641
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.912096917629242;0.3100763261318207
kmalloc(size) might give us more room than requested;might give us more room than requested;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9772728085517884;0.251278817653656
kmalloc(size) might give us more room than requested;might give us more room than requested;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.8100513219833374;0.4381863474845886
kmalloc(size) might give us more room than requested;might give us more room than requested;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9337910413742064;0.4110824167728424
kmalloc(size) might give us more room than requested;might give us more room than requested;Cleanup zeroing allocations;Slab allocators;Contradicts;0.907028317451477;0.39945483207702637
kmalloc(size) might give us more room than requested;might give us more room than requested;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8679873943328857;0.48807618021965027
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.8502709865570068;0.210945725440979
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.92641019821167;0.3576965034008026
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.909446895122528;0.28461599349975586
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8094385862350464;0.40353554487228394
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;patch ensures alignment on all arches and cache sizes;Still;Contradicts;0.933911383152008;0.5714247822761536
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9524972438812256;0.3554353713989258
"reordering the use of the ""actual size"" information";fix this;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9083121418952942;0.4016026258468628
"Code can ask ""how large an allocation would I get for a given size?""";instead;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9846328496932985;0.09782098978757858
"Code can ask ""how large an allocation would I get for a given size?""";instead;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8734322190284729;0.4168884754180908
"Code can ask ""how large an allocation would I get for a given size?""";instead;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8188288807868958;0.872414231300354
"Code can ask ""how large an allocation would I get for a given size?""";instead;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9324833154678344;0.6700795888900757
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.8906416296958923;0.21561479568481445
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.8381441831588745;0.673957109451294
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.9264259338378906;0.5682151317596436
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9250497221946716;0.678627073764801
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.8365750908851624;0.5859299898147583
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.908570408821106;0.31249547004699707
unify NUMA and UMA version of tracepoints;mm/slab_common;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8351673483848572;0.5674521923065186
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9137552976608276;-0.023391034454107285
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9267287850379944;0.6270622611045837
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8069032430648804;0.434836208820343
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Cleanup zeroing allocations;Slab allocators;Contradicts;0.873107373714447;0.4686316251754761
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.898834228515625;0.517853319644928
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9781634211540222;0.1357274204492569
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9275133609771729;0.40242764353752136
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9352408051490784;0.546926736831665
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.8928127884864807;0.5493872165679932
waiting for quiescence after a full Android boot;with a kernel built with CONFIG_KASAN_HW_TAGS;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9376110434532166;0.3422982096672058
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8587898015975952;0.3834032416343689
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9707039594650269;0.042896874248981476
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.9647458791732788;0.5393664836883545
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Cleanup zeroing allocations;Slab allocators;Contradicts;0.8333035111427307;0.67307049036026
Remove unnecessary page_mapcount_reset() function call;unnecessary;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Contradicts;0.8416944742202759;0.7350378036499023
Use unused field for units;instead;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8524829745292664;0.4168884754180908
use struct folio instead of struct page;Where non-slab page can appear;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9491190910339355;0.16083963215351105
use struct folio instead of struct page;Where non-slab page can appear;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Contradicts;0.9774367809295654;0.528471827507019
use struct folio instead of struct page;Where non-slab page can appear;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.944092333316803;0.5203257203102112
use struct folio instead of struct page;Where non-slab page can appear;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8705927133560181;0.5591379404067993
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Contradicts;0.8636457920074463;0.402469664812088
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Contradicts;0.8497216105461121;0.37991204857826233
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.962471842765808;0.6024171113967896
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Contradicts;0.9691905975341796;0.3663308322429657
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.9512932896614076;0.27794161438941956
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9260067343711852;0.3150734603404999
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Contradicts;0.8542044758796692;0.5955944061279297
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Contradicts;0.8807080984115601;0.43803051114082336
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Introduce krealloc();reallocates memory while keeping the contents unchanged;Contradicts;0.8021864891052246;0.5279307961463928
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.9579660892486572;0.5576825141906738
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9537898898124696;0.3604636788368225
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9784733057022096;0.47508326172828674
Add mem_dump_obj() to print source of memory block;to print source of memory block;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9758762717247008;0.1394931972026825
Add mem_dump_obj() to print source of memory block;to print source of memory block;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8074954152107239;0.46367454528808594
Add mem_dump_obj() to print source of memory block;to print source of memory block;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8011776208877563;0.68255215883255
Add mem_dump_obj() to print source of memory block;to print source of memory block;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8919273614883423;0.571667492389679
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.9435585141181946;0.7107151746749878
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8028975129127502;0.3817563056945801
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8899502754211426;0.47170260548591614
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9698386788368224;0.21989013254642487
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Contradicts;0.8366007208824158;0.5273250341415405
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.9900942444801332;0.28041478991508484
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9990659356117249;0.36524465680122375
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.8965041041374207;0.4909248948097229
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.8760831952095032;0.2832043766975403
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Contradicts;0.9510998129844666;0.4418749213218689
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9818750023841858;0.41773879528045654
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9342201948165894;0.4443759620189667
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9939306974411012;0.3088061809539795
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.856963574886322;0.4111190140247345
The information printed can depend on kernel configuration;depend on kernel configuration;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8987814784049988;0.39135852456092834
The information printed can depend on kernel configuration;depend on kernel configuration;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8838474154472351;0.5703030824661255
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9128775596618652;0.5767441987991333
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9881039261817932;0.162156879901886
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.8505476713180542;0.49659499526023865
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8358720541000366;0.5742344260215759
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.989068567752838;0.6332048177719116
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.8253954648971558;0.40130814909935
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9674761295318604;0.13585993647575378
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.9028729796409608;0.5905473828315735
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;remove bigblock tracking;slob;Contradicts;0.8014223575592041;0.5042703747749329
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.891501247882843;0.6572788953781128
have some use eventually for annotations in drivers/gpu;I might;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.9094549417495728;0.547309160232544
have some use eventually for annotations in drivers/gpu;I might;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9979288578033448;0.07283854484558105
have some use eventually for annotations in drivers/gpu;I might;They are no longer needed;They have become so simple;Contradicts;0.9902605414390564;0.5591809749603271
have some use eventually for annotations in drivers/gpu;I might;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9369899034500122;0.5038456320762634
have some use eventually for annotations in drivers/gpu;I might;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8265944123268127;0.7624736428260803
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9059980511665344;0.4226154685020447
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9611005783081056;0.25817951560020447
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;They are no longer needed;They have become so simple;Contradicts;0.8145231604576111;0.2242254763841629
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.964247703552246;0.5306829214096069
The size of slab memory shouldnt exceed 4Gb on 32-bit machines;it will fit into atomic_long_t we use for vmstats;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8914187550544739;0.48781511187553406
The size of slab memory shouldnt exceed 4Gb on 32-bit machines;it will fit into atomic_long_t we use for vmstats;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9921751618385316;0.5748453736305237
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9993209838867188;0.08416923880577087
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9068729877471924;0.3580186665058136
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9303285479545592;0.5668988227844238
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;patch ensures alignment on all arches and cache sizes;Still;Contradicts;0.9797922372817992;0.610467255115509
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Contradicts;0.8060001730918884;0.4618017077445984
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8010009527206421;0.42371150851249695
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.986481785774231;0.6060459613800049
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.953311800956726;0.4121634364128113
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Contradicts;0.9639650583267212;0.5444033145904541
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;They are no longer needed;They have become so simple;Contradicts;0.9176321029663086;0.6102646589279175
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.8713335990905762;0.41960301995277405
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Contradicts;0.8567066192626953;0.6256098747253418
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.845459520816803;0.33931517601013184
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8488527536392212;0.34085357189178467
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9121804237365724;0.3582954406738281
patch ensures alignment on all arches and cache sizes;Still;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.8399131298065186;0.07694251090288162
patch ensures alignment on all arches and cache sizes;Still;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Contradicts;0.8828772902488708;0.7368803024291992
patch ensures alignment on all arches and cache sizes;Still;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.969732403755188;0.41912055015563965
patch ensures alignment on all arches and cache sizes;Still;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9167347550392152;0.4825071096420288
improve memory accounting;improve;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8786198496818542;0.35866987705230713
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Make dead caches discard free slabs immediately;slub;Contradicts;0.909539759159088;0.6960606575012207
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9943108558654784;0.20286136865615845
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9480730295181274;0.39369678497314453
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.929329514503479;0.36562031507492065
"SLAB doesnt actually use page allocator directly
";no change there;Make dead caches discard free slabs immediately;slub;Contradicts;0.8475188612937927;0.6858303546905518
add three helpers, convert the appropriate places;these three patches;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.9141575694084167;0.2694389522075653
add three helpers, convert the appropriate places;these three patches;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8615742921829224;0.3830276429653168
add three helpers, convert the appropriate places;these three patches;Cleanup zeroing allocations;Slab allocators;Contradicts;0.8855900168418884;0.5132333040237427
to find out the size of a potentially huge page;Its unnecessarily hard;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9473156332969666;0.27943894267082214
union of slab_list and lru;slab_list and lru are the same bits;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.904276728630066;0.4136781692504883
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9652844071388244;0.1206282377243042
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.8581426739692688;0.4853910505771637
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.8893717527389526;0.646524965763092
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9612968564033508;0.2955676317214966
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9930033087730408;0.3253893256187439
configure and build (select SLOB allocator);SLOB allocator;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9937343001365662;0.25916576385498047
configure and build (select SLOB allocator);SLOB allocator;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8081879615783691;0.5859980583190918
configure and build (select SLOB allocator);SLOB allocator;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8666681051254272;0.6735293865203857
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.8736962676048279;0.724648654460907
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9878675937652588;0.05575859174132347
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Contradicts;0.9552050828933716;0.6172048449516296
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.9920023679733276;0.4693133234977722
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8461611270904541;0.5453370809555054
Add a return parameter to slob_page_alloc();to signal that the list was modified;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9072942733764648;0.21248973906040192
Add a return parameter to slob_page_alloc();to signal that the list was modified;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8916429281234741;0.3884209394454956
Add a return parameter to slob_page_alloc();to signal that the list was modified;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8074012398719788;0.6523714065551758
Add a return parameter to slob_page_alloc();to signal that the list was modified;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9924339056015016;0.702843964099884
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9131327271461488;0.5194425582885742
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8327754735946655;0.2772951126098633
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9885826110839844;0.37244245409965515
remove an unnecessary check for __GFP_ZERO;unnecessary;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8558506965637207;0.33589956164360046
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;to be applied to the file;a file by file comparison of the scanner;Contradicts;0.8967819809913635;0.4807174801826477
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9125401377677916;0.30248475074768066
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9123870730400084;0.3750241696834564
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.881475031375885;0.5591545104980469
to be applied to the file;a file by file comparison of the scanner;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8295587301254272;0.44043993949890137
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9449973106384276;0.5035398006439209
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8804388046264648;0.38367539644241333
All documentation files were explicitly excluded;explicitly excluded;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.886951744556427;0.4127066135406494
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9373584985733032;0.33622217178344727
For non */uapi/* files;that summary was;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.909846305847168;0.5019676685333252
Kate also obtained a third independent scan of the 4.13 code base from FOSSology, and compared selected files where the other two scanners disagreed against that SPDX file;to see if there was new insights;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9215519428253174;0.4058196544647217
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9306683540344238;0.08386922627687454
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8930091261863708;0.6123212575912476
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9157143235206604;0.6835777759552002
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9583714604377748;0.3040386140346527
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8730669617652893;0.37954095005989075
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8965219259262085;0.292860746383667
"reported chasing a bug
";their assumption that SLAB_DESTROY_BY_RCU provided an existence guarantee;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9131593704223632;0.2563188374042511
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Make dead caches discard free slabs immediately;slub;Contradicts;0.8704993724822998;0.7638450860977173
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9276356101036072;0.4168884754180908
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9816285371780396;0.6700795888900757
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9215783476829528;0.4167324900627136
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.937440037727356;0.2840615510940552
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9146199822425842;0.4595785140991211
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9579802751541138;0.28365883231163025
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.8857367634773254;0.40875035524368286
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8204348087310791;0.3344602584838867
allow future extension of the bulk alloc API;is done to;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.8308543562889099;0.0587177649140358
allow future extension of the bulk alloc API;is done to;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9428834915161132;0.4551857113838196
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Make dead caches discard free slabs immediately;slub;Contradicts;0.8043422698974609;0.49546343088150024
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9188286662101746;0.08927911520004272
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.9814220666885376;0.5106552243232727
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.9362034797668456;0.7206461429595947
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Remove kmemtrace ftrace plugin;tracing;Contradicts;0.8332684636116028;0.7168364524841309
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.8292407989501953;0.3910718560218811
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Drop it  ;if you want;Contradicts;0.8553001880645752;0.7400645017623901
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Contradicts;0.8780235648155212;0.18506284058094025
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Cleanup zeroing allocations;Slab allocators;Contradicts;0.8735116124153137;0.48759734630584717
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.9301564693450928;0.459592342376709
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9918644428253174;0.4208729565143585
hiding potentially buggy callers;except temporarily;record page flag overlays explicitly;slob;Contradicts;0.9269780516624452;0.6851822137832642
hiding potentially buggy callers;except temporarily;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9941297769546508;0.49605339765548706
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9656724333763124;0.15168149769306183
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.9246424436569214;0.49163609743118286
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;They are no longer needed;They have become so simple;Contradicts;0.901145040988922;0.48370853066444397
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.826890230178833;0.4454779028892517
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8134469389915466;0.4278808832168579
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8087613582611084;0.3808425962924957
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Make dead caches discard free slabs immediately;slub;Contradicts;0.8838971257209778;0.31616804003715515
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9388223886489868;0.5710241198539734
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Contradicts;0.8595430254936218;0.22450077533721924
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.824357807636261;0.17575693130493164
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9211874008178712;0.36586809158325195
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8764486908912659;0.3410108685493469
Make dead caches discard free slabs immediately;slub;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9501873254776;0.3825390934944153
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9389086365699768;0.4600560665130615
resurrects approach first proposed in [1];To fix this issue;Drop it  ;if you want;Contradicts;0.8065934181213379;0.7172035574913025
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.945139467716217;0.3226194679737091
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.8495438098907471;0.22146962583065033
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8596804738044739;0.27824369072914124
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8418518900871277;0.23172485828399658
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9799485206604004;0.4220465421676636
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9665731191635132;0.3337828814983368
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8058668375015259;0.37145504355430603
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Contradicts;0.8056074380874634;0.49254652857780457
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9176263809204102;0.4277815818786621
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8234112858772278;0.1733766794204712
"Introduce much more complexity to both SLUB and memcg
";than this small patch;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Contradicts;0.9426321387290956;0.5225690603256226
"Introduce much more complexity to both SLUB and memcg
";than this small patch;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Contradicts;0.9651522040367126;0.5982613563537598
"Introduce much more complexity to both SLUB and memcg
";than this small patch;They are no longer needed;They have become so simple;Contradicts;0.8024079203605652;0.7288636565208435
"Introduce much more complexity to both SLUB and memcg
";than this small patch;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.8566843867301941;0.6601384282112122
"Introduce much more complexity to both SLUB and memcg
";than this small patch;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.8074542880058289;0.73944091796875
"Introduce much more complexity to both SLUB and memcg
";than this small patch;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.8738677501678467;0.51095050573349
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Drop it  ;if you want;Contradicts;0.8790267109870911;0.6396075487136841
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Cleanup zeroing allocations;Slab allocators;Contradicts;0.9148642420768738;0.4637831449508667
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Contradicts;0.8960066437721252;0.7425098419189453
"Introduce much more complexity to both SLUB and memcg
";than this small patch;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Contradicts;0.95666241645813;0.66280198097229
"Introduce much more complexity to both SLUB and memcg
";than this small patch;This patch cleans up the slab header definitions;cleans up the slab header definitions;Contradicts;0.9131921529769896;0.31123268604278564
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9968679547309875;0.547675609588623
"Introduce much more complexity to both SLUB and memcg
";than this small patch;simplifies SLOB;at this point slob may be broken;Contradicts;0.9974291920661926;0.606464684009552
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8664659857749939;0.3554342985153198
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8551166653633118;0.4101543426513672
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.8829828500747681;0.4260159730911255
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.9354886412620544;0.5182249546051025
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8945327401161194;0.4395959675312042
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;remove bigblock tracking;slob;Contradicts;0.8292170166969299;0.4352835416793823
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8872312307357788;0.534710168838501
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;Contradicts;0.8956598043441772;0.35957714915275574
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8894119262695312;0.3280473053455353
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9051123857498168;0.33257535099983215
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8720118403434753;0.4461100399494171
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8932028412818909;0.39979982376098633
unioned together;Conveniently;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.816429853439331;0.40352070331573486
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8764646649360657;0.2718597650527954
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9720414876937866;0.46480727195739746
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8588226437568665;0.5668063163757324
The new rule is: page->lru is what you use;if you want to keep your page on a list;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.941838800907135;0.3574228286743164
function naming changes;requires some;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9381111860275269;0.49930769205093384
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8510309457778931;0.5535233616828918
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8365981578826904;0.42323851585388184
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9604132771492004;0.47832104563713074
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8926836252212524;0.3223220109939575
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9139598608016968;0.43996956944465637
a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9675874710083008;0.4545358419418335
a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Contradicts;0.9282041788101196;0.532365083694458
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;They are no longer needed;They have become so simple;Contradicts;0.8122329711914062;0.5568979978561401
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;Drop it  ;if you want;Contradicts;0.8578987717628479;0.4269436001777649
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Contradicts;0.9372754693031312;0.1766074150800705
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8147100806236267;0.2997826933860779
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9600780606269836;0.4018104374408722
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.995528519153595;0.6076065897941589
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.950813889503479;0.39898717403411865
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.952382504940033;0.4053282141685486
Improve trace accuracy;by correctly tracing reported size;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8202458024024963;0.4112611413002014
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8395751714706421;0.605022668838501
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.8930714130401611;0.4828137457370758
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;remove bigblock tracking;slob;Contradicts;0.8842617869377136;0.5274611711502075
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8849815130233765;0.4326738119125366
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8774846792221069;0.6079463362693787
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.955847144126892;0.4896213114261627
kmalloc_track_caller() is correctly implemented;tracing the specified caller;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.8039181232452393;0.5512838363647461
kmalloc_track_caller() is correctly implemented;tracing the specified caller;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8014075756072998;0.44545406103134155
This will allow us to push more processing into common code later;improve readability;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.951223611831665;0.3586750030517578
This will allow us to push more processing into common code later;improve readability;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.855420708656311;0.5770800709724426
This will allow us to push more processing into common code later;improve readability;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8584675788879395;0.4068819284439087
This affects RCU handling;somewhat;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9551873207092284;0.5189645886421204
This affects RCU handling;somewhat;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8406051397323608;0.6251810789108276
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8603705167770386;0.4176274240016937
Fix early boot kernel crash;Slob;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.951394259929657;0.44211703538894653
They are no longer needed;They have become so simple;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9024390578269958;0.38190051913261414
No need to zero mapping since it is no longer in use;it is no longer in use;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9066739082336426;0.41963303089141846
No need to zero mapping since it is no longer in use;it is no longer in use;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.8315215706825256;0.7114046216011047
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.8638598918914795;0.33364182710647583
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9150137305259703;0.3711410164833069
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8447117209434509;0.556863009929657
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Contradicts;0.9106647968292236;0.40622979402542114
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9913896918296814;0.4492502808570862
cleans up some bitrot in slob.c;Also;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9581893086433412;0.45754310488700867
cleans up some bitrot in slob.c;Also;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Contradicts;0.8544133305549622;0.36261171102523804
cleans up some bitrot in slob.c;Also;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8053083419799805;0.6235278248786926
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9009993076324464;0.39553582668304443
Fix gfp flags passed to lockdep;lockdep;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8140078186988831;0.529131293296814
Ran a ktest.pl config_bisect;Came up with this config as the problem;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8436808586120605;0.4064028561115265
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8692144751548767;0.4979187846183777
The bit should be passed to trace_kmalloc_node()  ;as well;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8413199186325073;0.45772209763526917
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8314471244812012;0.3323270082473755
Remove kmemtrace ftrace plugin;tracing;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8844156265258789;0.4919910728931427
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Contradicts;0.870836079120636;0.4991486370563507
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9393587708473206;0.3971797823905945
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9136688113212584;0.3016936779022217
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9327415227890016;0.33045411109924316
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9752838015556335;0.47162187099456787
refactor code for future changes;Impact;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9304190874099731;0.375029981136322
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9273539185523988;0.6365243196487427
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8945503830909729;0.529131293296814
fix typo in mm/slob.c;build fix;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9427270889282228;0.30446943640708923
fix typo in mm/slob.c;build fix;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8657596707344055;0.5821948647499084
fix lockup in slob_free()  ;lockup;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8766599297523499;0.5121827721595764
fix lockup in slob_free()  ;lockup;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8968831300735474;0.6841239333152771
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8391439914703369;0.44429492950439453
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8328971862792969;0.5852676630020142
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.823966920375824;0.6684971451759338
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9417216777801514;0.32429155707359314
enable and use this tracer;To enable and use this tracer;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8442893028259277;0.6002691984176636
enable and use this tracer;To enable and use this tracer;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8092226386070251;0.5804175734519958
I find it more readable  ;personal opinion;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8624059557914734;0.43214648962020874
Drop it  ;if you want;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8768030405044556;0.4278916120529175
Drop it  ;if you want;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Contradicts;0.9322961568832396;0.5780878067016602
fix bogus ksize calculation fix  ;SLOB;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8521612286567688;0.44211703538894653
fixes the previous fix  ;completely wrong on closer inspection;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9093944430351256;0.24719423055648804
fix bogus ksize calculation;bogus;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8897520303726196;0.3830661177635193
record page flag overlays explicitly;slob;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.895017147064209;0.44211703538894653
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.984571933746338;0.38244208693504333
reduce external fragmentation by using three free lists;slob;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.954697847366333;0.44211703538894653
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9646127223968506;0.28230008482933044
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8450557589530945;0.48355334997177124
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.935022234916687;0.5138874053955078
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9720154404640198;0.23936347663402557
fix memory corruption;memory corruption;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9242939352989196;0.33073604106903076
Handle that separately in krealloc();separately;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.9680533409118652;0.5177901983261108
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9586025476455688;0.28832247853279114
reduce list scanning;slob;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.93037611246109;0.44211703538894653
starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9313117265701294;0.2914445698261261
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Contradicts;0.8057064414024353;0.4241412878036499
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.919784665107727;0.4409262537956238
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8795691728591919;0.40056729316711426
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.956187903881073;0.538993239402771
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9893428087234496;0.43248867988586426
Allocation size is stored in page->private;makes ksize more accurate than it previously was;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.9744053483009338;0.45923471450805664
"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8264259696006775;0.5529754161834717
"Taking a spinlock in a destructor is a bit risky
";since the slab allocators may run the destructors anytime they decide a slab is no longer needed;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9191607236862184;0.31157809495925903
"
Use a destructor will BUG()
";"
Any attempt to";Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8971897959709167;0.6136640906333923
The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9983367919921876;0.46677330136299133
handle SLAB_PANIC flag;slob;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.903165638446808;0.698976993560791
Introduce krealloc();reallocates memory while keeping the contents unchanged;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.86277174949646;0.37763094902038574
[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9730241894721984;0.6141707897186279
"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9905718564987184;0.6239951848983765
"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Contradicts;0.860260009765625;0.47953203320503235
This patch cleans up the slab header definitions;cleans up the slab header definitions;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9885872602462769;0.46858519315719604
add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9510678648948668;0.4523807764053345
