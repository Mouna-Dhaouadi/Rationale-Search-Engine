Decision1;Rationale1;Decision2;Rationale2;Relationship;Alpha
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;Similar;0.53487229347229
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;kmalloc(size) might give us more room than requested;might give us more room than requested;Similar;0.6418723464012146
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Similar;0.5135068297386169
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Similar;0.5518302917480469
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;"reordering the use of the ""actual size"" information";fix this;Similar;0.7456427812576294
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;"Code can ask ""how large an allocation would I get for a given size?""";instead;Similar;0.7900899648666382
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Similar;0.8149130344390869
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Similar;0.5278264284133911
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;unify NUMA and UMA version of tracepoints;mm/slab_common;Similar;0.6375168561935425
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Similar;0.6348006129264832
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Similar;0.5794300436973572
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Similar;0.7645056843757629
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;use struct folio instead of struct page;Where non-slab page can appear;Similar;0.5969138145446777
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Similar;0.5309123992919922
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Add mem_dump_obj() to print source of memory block;to print source of memory block;Similar;0.6775524020195007
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Similar;0.5332908034324646
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.6482151746749878
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;The information printed can depend on kernel configuration;depend on kernel configuration;Similar;0.6425695419311523
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Similar;0.5700172185897827
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.6406086087226868
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;have some use eventually for annotations in drivers/gpu;I might;Similar;0.5858138799667358
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.638927698135376
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.6966656446456909
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;Similar;0.5260738134384155
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;patch ensures alignment on all arches and cache sizes;Still;Similar;0.5372630953788757
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;improve memory accounting;improve;Similar;0.550098180770874
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;to find out the size of a potentially huge page;Its unnecessarily hard;Similar;0.653404712677002
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.6404297351837158
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;configure and build (select SLOB allocator);SLOB allocator;Similar;0.6407488584518433
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.5233994722366333
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.7181423902511597
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.7181423902511597
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;to be applied to the file;a file by file comparison of the scanner;Similar;0.6749247312545776
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Similar;0.5978010892868042
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Similar;0.5162255764007568
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5847480297088623
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.543074369430542
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5093967318534851
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;allow future extension of the bulk alloc API;is done to;Similar;0.6130133271217346
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.5038427114486694
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5411922931671143
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;resurrects approach first proposed in [1];To fix this issue;Similar;0.549001157283783
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5377745032310486
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5481233596801758
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5704849362373352
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;unioned together;Conveniently;Similar;0.560960054397583
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5354721546173096
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.6630275249481201
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;function naming changes;requires some;Similar;0.6526179313659668
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.6219478845596313
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.6548805236816406
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.6736059784889221
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.7269774675369263
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.6888301968574524
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;This affects RCU handling;somewhat;Similar;0.542669415473938
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6257460117340088
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Fix gfp flags passed to lockdep;lockdep;Similar;0.622543454170227
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6746690273284912
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Adding this mask;fixes the bug;Similar;0.608916699886322
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5772527456283569
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6526294946670532
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5369778871536255
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;refactor code for future changes;Impact;Similar;0.5245339870452881
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5999486446380615
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;use tracepoints;kmemtrace;Similar;0.5477080345153809
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5333104133605957
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;fix typo in mm/slob.c;build fix;Similar;0.6271324753761292
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.646416425704956
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5378988981246948
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;enable and use this tracer;To enable and use this tracer;Similar;0.5964012742042542
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;I find it more readable  ;personal opinion;Similar;0.5146876573562622
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;record page flag overlays explicitly;slob;Similar;0.5745140314102173
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.514187753200531
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;improved alignment handling;improved;Similar;0.5249128341674805
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5835871696472168
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;remove bigblock tracking;slob;Similar;0.5479899644851685
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5363712310791016
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;rework freelist handling;slob;Similar;0.532584547996521
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;Similar;0.6162749528884888
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;"align them to word size
";it is best in practice;Similar;0.7722823619842529
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5621172189712524
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Code size goes up about 1K;dynamic memory usage should be lower even on relatively small memory systems;Similar;0.5046942234039307
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5016177892684937
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;SLOB to be used on SMP  ;allows;Similar;0.6122455596923828
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;handle SLAB_PANIC flag;slob;Similar;0.5178688168525696
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6341201066970825
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Redo a lot of comments;Also;Similar;0.6142286062240601
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5194777250289917
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;simplifies SLOB;at this point slob may be broken;Similar;0.5614193081855774
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5607537627220154
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;SLOB saving nearly half a megabyte of RAM;Comparison for otherwise identical builds;Similar;0.5128682851791382
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.7865117788314819
For most allocations, this works well, as the vast majority of callers are not expecting to use more memory than what they asked for;The vast majority of callers are not expecting to use more memory than what they asked for;If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;Similar;0.5255404710769653
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;Similar;0.5581967830657959
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Similar;0.5183627605438232
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Similar;0.5632324814796448
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Remove unnecessary page_mapcount_reset() function call;unnecessary;Similar;0.6231162548065186
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Similar;0.5602830052375793
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Similar;0.5011668801307678
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Similar;0.5820350646972656
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Similar;0.6052390336990356
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Similar;0.6014446020126343
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.5635098218917847
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Similar;0.5665580630302429
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.5199547410011292
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.5264018177986145
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.5583944320678711
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Similar;0.5098210573196411
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Kate also obtained a third independent scan of the 4.13 code base from FOSSology, and compared selected files where the other two scanners disagreed against that SPDX file;to see if there was new insights;Similar;0.5247316956520081
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Similar;0.570640504360199
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.585532546043396
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Similar;0.524257481098175
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5950440764427185
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.602195143699646
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.6918448209762573
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.696395218372345
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5879398584365845
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5941486358642578
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.5795729756355286
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.7008281350135803
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.5404223203659058
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5334984660148621
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5720484852790833
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5197278261184692
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5657142400741577
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5848822593688965
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;This will allow us to push more processing into common code later;improve readability;Similar;0.532930850982666
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.533106803894043
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6256011128425598
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5077905654907227
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.6259344816207886
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5349162817001343
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5476274490356445
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Fix to return wrong pointer;Fix;Similar;0.5386062860488892
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5732494592666626
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.6203266978263855
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5368134379386902
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5548582077026367
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.534186840057373
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.695011556148529
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5662966966629028
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;fix page order calculation on not 4KB page;-;Similar;0.5220228433609009
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6013851761817932
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5278199911117554
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5752583146095276
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6270201206207275
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5131001472473145
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6116943359375
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5160397291183472
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5362991690635681
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5232478976249695
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Similar;0.5433547496795654
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;"reordering the use of the ""actual size"" information";fix this;Similar;0.5422347784042358
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;"Code can ask ""how large an allocation would I get for a given size?""";instead;Similar;0.5068126320838928
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Similar;0.517796516418457
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Similar;0.6014919281005859
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Similar;0.5373569130897522
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Similar;0.5106972455978394
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;Similar;0.626137912273407
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;patch ensures alignment on all arches and cache sizes;Still;Similar;0.5950875282287598
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.580970048904419
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;We always end up returning some objects at the cost of another cmpxchg  ;Else;Similar;0.5580302476882935
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;Make dead caches discard free slabs immediately;slub;Similar;0.5457409620285034
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5279383063316345
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.5312817096710205
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;Similar;0.5121419429779053
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5190740823745728
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Similar;0.5483883619308472
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.513995885848999
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;Adding this mask;fixes the bug;Similar;0.5269274711608887
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;enable and use this tracer;To enable and use this tracer;Similar;0.5884134769439697
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;record page flag overlays explicitly;slob;Similar;0.5213975310325623
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5407443046569824
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5157613754272461
kmalloc(size) might give us more room than requested;might give us more room than requested;"reordering the use of the ""actual size"" information";fix this;Similar;0.5339617729187012
kmalloc(size) might give us more room than requested;might give us more room than requested;"Code can ask ""how large an allocation would I get for a given size?""";instead;Similar;0.6906415224075317
kmalloc(size) might give us more room than requested;might give us more room than requested;Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Similar;0.5254981517791748
kmalloc(size) might give us more room than requested;might give us more room than requested;build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Similar;0.5017803907394409
kmalloc(size) might give us more room than requested;might give us more room than requested;have some use eventually for annotations in drivers/gpu;I might;Similar;0.5677507519721985
kmalloc(size) might give us more room than requested;might give us more room than requested;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.5757149457931519
kmalloc(size) might give us more room than requested;might give us more room than requested;to find out the size of a potentially huge page;Its unnecessarily hard;Similar;0.6730000972747803
kmalloc(size) might give us more room than requested;might give us more room than requested;allow future extension of the bulk alloc API;is done to;Similar;0.6447312831878662
kmalloc(size) might give us more room than requested;might give us more room than requested;We always end up returning some objects at the cost of another cmpxchg  ;Else;Similar;0.603534996509552
kmalloc(size) might give us more room than requested;might give us more room than requested;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5563830137252808
kmalloc(size) might give us more room than requested;might give us more room than requested;a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;Similar;0.597774863243103
kmalloc(size) might give us more room than requested;might give us more room than requested;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5634777545928955
kmalloc(size) might give us more room than requested;might give us more room than requested;This will allow us to push more processing into common code later;improve readability;Similar;0.6452856063842773
kmalloc(size) might give us more room than requested;might give us more room than requested;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.5022925138473511
kmalloc(size) might give us more room than requested;might give us more room than requested;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5588717460632324
kmalloc(size) might give us more room than requested;might give us more room than requested;refactor code for future changes;Impact;Similar;0.5385567545890808
kmalloc(size) might give us more room than requested;might give us more room than requested;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5951026082038879
kmalloc(size) might give us more room than requested;might give us more room than requested;I find it more readable  ;personal opinion;Similar;0.6084995269775391
kmalloc(size) might give us more room than requested;might give us more room than requested;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5430884957313538
kmalloc(size) might give us more room than requested;might give us more room than requested;improved alignment handling;improved;Similar;0.5262608528137207
kmalloc(size) might give us more room than requested;might give us more room than requested;can be slightly faster too;skip almost-full freelist pages completely;Similar;0.6882462501525879
kmalloc(size) might give us more room than requested;might give us more room than requested;"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;Similar;0.5768510103225708
kmalloc(size) might give us more room than requested;might give us more room than requested;"align them to word size
";it is best in practice;Similar;0.5156923532485962
kmalloc(size) might give us more room than requested;might give us more room than requested;Code size goes up about 1K;dynamic memory usage should be lower even on relatively small memory systems;Similar;0.5484232902526855
kmalloc(size) might give us more room than requested;might give us more room than requested;Redo a lot of comments;Also;Similar;0.5015202164649963
kmalloc(size) might give us more room than requested;might give us more room than requested;simplifies SLOB;at this point slob may be broken;Similar;0.5022121667861938
kmalloc(size) might give us more room than requested;might give us more room than requested;SLOB saving nearly half a megabyte of RAM;Comparison for otherwise identical builds;Similar;0.5245898365974426
kmalloc(size) might give us more room than requested;might give us more room than requested;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.55916428565979
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Similar;0.577438473701477
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;"reordering the use of the ""actual size"" information";fix this;Similar;0.5735207796096802
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Similar;0.6383203268051147
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Similar;0.588596761226654
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;unify NUMA and UMA version of tracepoints;mm/slab_common;Similar;0.628648042678833
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Similar;0.6083906888961792
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Similar;0.5734495520591736
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Similar;0.5171782970428467
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;Similar;0.5889534950256348
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Similar;0.596767783164978
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Remove unnecessary page_mapcount_reset() function call;unnecessary;Similar;0.5585864782333374
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Similar;0.5567308664321899
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Similar;0.5890443325042725
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;kmem_cache_free;We can use it to understand what the RCU core is going to free;Similar;0.6181938648223877
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Add mem_dump_obj() to print source of memory block;to print source of memory block;Similar;0.5478424429893494
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.5589754581451416
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.5672862529754639
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.6255633234977722
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.6117326021194458
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;Similar;0.5330582857131958
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;patch ensures alignment on all arches and cache sizes;Still;Similar;0.5074681639671326
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.5609683990478516
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;configure and build (select SLOB allocator);SLOB allocator;Similar;0.5534678101539612
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.5973488688468933
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.5973488688468933
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.5900890827178955
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.5169918537139893
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;to be applied to the file;a file by file comparison of the scanner;Similar;0.52436363697052
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.5591649413108826
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5658798217773438
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.5761611461639404
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.6757611632347107
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.5904647707939148
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;We always end up returning some objects at the cost of another cmpxchg  ;Else;Similar;0.5157432556152344
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Make dead caches discard free slabs immediately;slub;Similar;0.5272687077522278
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;resurrects approach first proposed in [1];To fix this issue;Similar;0.5846315622329712
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5372194647789001
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5065260529518127
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5176730155944824
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5297330021858215
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.5592705011367798
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;unioned together;Conveniently;Similar;0.5122091770172119
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5459517240524292
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5290882587432861
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.5535762310028076
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5833787322044373
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5738677978515625
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5334697365760803
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5871962308883667
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5271918177604675
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Similar;0.5612707734107971
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6848275065422058
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Fix gfp flags passed to lockdep;lockdep;Similar;0.6121653318405151
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6146008968353271
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Adding this mask;fixes the bug;Similar;0.514626145362854
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5308730006217957
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.683756411075592
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5094397664070129
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5576207637786865
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Remove kmemtrace ftrace plugin;tracing;Similar;0.6254438161849976
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.608295202255249
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6072932481765747
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5608859062194824
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;use tracepoints;kmemtrace;Similar;0.646367609500885
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5845552682876587
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;fix typo in mm/slob.c;build fix;Similar;0.6045336723327637
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;fix lockup in slob_free()  ;lockup;Similar;0.6247497200965881
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5074997544288635
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5316159725189209
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;enable and use this tracer;To enable and use this tracer;Similar;0.6071093082427979
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Drop it  ;if you want;Similar;0.5227268934249878
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;fix bogus ksize calculation fix  ;SLOB;Similar;0.5930243730545044
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5644811391830444
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;fix bogus ksize calculation;bogus;Similar;0.5952184796333313
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5038458108901978
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.532404899597168
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;record page flag overlays explicitly;slob;Similar;0.5572656393051147
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5640139579772949
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Handle that separately in krealloc();separately;Similar;0.5405313968658447
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5122247338294983
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;reduce list scanning;slob;Similar;0.5312297344207764
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5612519979476929
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Cleanup zeroing allocations;Slab allocators;Similar;0.5757632255554199
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.628538966178894
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5657843351364136
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;remove bigblock tracking;slob;Similar;0.6369802951812744
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;rework freelist handling;slob;Similar;0.5545855164527893
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5020965337753296
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;"align them to word size
";it is best in practice;Similar;0.5210979580879211
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6756417751312256
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5450358390808105
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;SLOB to be used on SMP  ;allows;Similar;0.59138023853302
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6470956802368164
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;handle SLAB_PANIC flag;slob;Similar;0.5506325960159302
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5887369513511658
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5024719834327698
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5125977396965027
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.522535502910614
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5065271258354187
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;simplifies SLOB;at this point slob may be broken;Similar;0.6053692102432251
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5108157992362976
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7384662628173828
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;fix;SLOB=y && SMP=y fix;Similar;0.5252430438995361
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5184100866317749
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;"reordering the use of the ""actual size"" information";fix this;Similar;0.6629445552825928
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;"Code can ask ""how large an allocation would I get for a given size?""";instead;Similar;0.6616283059120178
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Similar;0.5777120590209961
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Similar;0.5105377435684204
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;unify NUMA and UMA version of tracepoints;mm/slab_common;Similar;0.5290595293045044
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Similar;0.5340603590011597
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Similar;0.5190901160240173
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Use unused field for units;instead;Similar;0.5493319034576416
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;use struct folio instead of struct page;Where non-slab page can appear;Similar;0.5158435106277466
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Similar;0.5978596806526184
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.5862278938293457
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;The information printed can depend on kernel configuration;depend on kernel configuration;Similar;0.5395413637161255
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;have some use eventually for annotations in drivers/gpu;I might;Similar;0.5132941007614136
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.5680733919143677
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.5169836282730103
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.5332580208778381
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;improve memory accounting;improve;Similar;0.5328237414360046
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;add three helpers, convert the appropriate places;these three patches;Similar;0.5202577114105225
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.5051947832107544
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;configure and build (select SLOB allocator);SLOB allocator;Similar;0.5616490840911865
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.5058215856552124
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.630028486251831
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.630028486251831
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;to be applied to the file;a file by file comparison of the scanner;Similar;0.6516969203948975
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.5801428556442261
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.6289811134338379
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.6937273740768433
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.5439776182174683
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5744452476501465
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;We always end up returning some objects at the cost of another cmpxchg  ;Else;Similar;0.6344200372695923
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;resurrects approach first proposed in [1];To fix this issue;Similar;0.7147942781448364
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6071614027023315
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.6343398690223694
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;unioned together;Conveniently;Similar;0.637427806854248
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.6075025796890259
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5267509818077087
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;function naming changes;requires some;Similar;0.6270523071289062
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.6800421476364136
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5339057445526123
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Improve trace accuracy;by correctly tracing reported size;Similar;0.5889549255371094
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5894218683242798
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;This will allow us to push more processing into common code later;improve readability;Similar;0.6253798007965088
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5869141817092896
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;This affects RCU handling;somewhat;Similar;0.6048128604888916
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.5344797968864441
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Remove various small accessors;various small;Similar;0.5769989490509033
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;They are no longer needed;They have become so simple;Similar;0.5146584510803223
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Fix gfp flags passed to lockdep;lockdep;Similar;0.572192907333374
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6325283050537109
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Adding this mask;fixes the bug;Similar;0.5407518148422241
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.7452461123466492
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.548993706703186
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5192252397537231
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5942671298980713
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;refactor code for future changes;Impact;Similar;0.6618075966835022
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;use tracepoints;kmemtrace;Similar;0.645638108253479
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.534064531326294
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;fix typo in mm/slob.c;build fix;Similar;0.571672260761261
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5992895364761353
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;enable and use this tracer;To enable and use this tracer;Similar;0.6755326986312866
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;I find it more readable  ;personal opinion;Similar;0.5521548390388489
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Drop it  ;if you want;Similar;0.5860556364059448
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;fix bogus ksize calculation fix  ;SLOB;Similar;0.6071906089782715
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.7420717477798462
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;fix bogus ksize calculation;bogus;Similar;0.620890200138092
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5444510579109192
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Fix to return wrong pointer;Fix;Similar;0.6066686511039734
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Handle that separately in krealloc();separately;Similar;0.6225085258483887
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5439924001693726
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;reduce list scanning;slob;Similar;0.5604143738746643
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.6469128727912903
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5785214900970459
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;remove bigblock tracking;slob;Similar;0.5368164777755737
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;rework freelist handling;slob;Similar;0.5807889699935913
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;"align them to word size
";it is best in practice;Similar;0.5940945148468018
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5487800240516663
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5229681730270386
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;SLOB to be used on SMP  ;allows;Similar;0.5965341925621033
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5659397840499878
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6210116147994995
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5056779384613037
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5221133232116699
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Redo a lot of comments;Also;Similar;0.5993614196777344
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;simplifies SLOB;at this point slob may be broken;Similar;0.5753244161605835
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5344507694244385
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;fix;SLOB=y && SMP=y fix;Similar;0.650517463684082
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5664331912994385
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Similar;0.5078063011169434
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Similar;0.5372082591056824
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;Similar;0.6311858296394348
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;"SLAB doesnt actually use page allocator directly
";no change there;Similar;0.5539189577102661
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5678238868713379
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5424914360046387
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5287387371063232
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5151668787002563
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;Similar;0.6043024063110352
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;This affects RCU handling;somewhat;Similar;0.5488046407699585
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.6770774126052856
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;Fix to return wrong pointer;Fix;Similar;0.5000193119049072
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;fix memory corruption;memory corruption;Similar;0.5237002372741699
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6157867908477783
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;"Taking a spinlock in a destructor is a bit risky
";since the slab allocators may run the destructors anytime they decide a slab is no longer needed;Similar;0.6788941621780396
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5059555768966675
"reordering the use of the ""actual size"" information";fix this;"Code can ask ""how large an allocation would I get for a given size?""";instead;Similar;0.6732527017593384
"reordering the use of the ""actual size"" information";fix this;Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Similar;0.7222229838371277
"reordering the use of the ""actual size"" information";fix this;Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Similar;0.5546035766601562
"reordering the use of the ""actual size"" information";fix this;unify NUMA and UMA version of tracepoints;mm/slab_common;Similar;0.5901267528533936
"reordering the use of the ""actual size"" information";fix this;Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Similar;0.5465521812438965
"reordering the use of the ""actual size"" information";fix this;Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Similar;0.5630778074264526
"reordering the use of the ""actual size"" information";fix this;Remove unnecessary page_mapcount_reset() function call;unnecessary;Similar;0.507079005241394
"reordering the use of the ""actual size"" information";fix this;Use unused field for units;instead;Similar;0.5767992734909058
"reordering the use of the ""actual size"" information";fix this;use struct folio instead of struct page;Where non-slab page can appear;Similar;0.6308447122573853
"reordering the use of the ""actual size"" information";fix this;By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Similar;0.5765587687492371
"reordering the use of the ""actual size"" information";fix this;Add mem_dump_obj() to print source of memory block;to print source of memory block;Similar;0.5516421794891357
"reordering the use of the ""actual size"" information";fix this;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.522950291633606
"reordering the use of the ""actual size"" information";fix this;The information printed can depend on kernel configuration;depend on kernel configuration;Similar;0.5504538416862488
"reordering the use of the ""actual size"" information";fix this;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.5283768177032471
"reordering the use of the ""actual size"" information";fix this;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.5465496778488159
"reordering the use of the ""actual size"" information";fix this;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.5031570196151733
"reordering the use of the ""actual size"" information";fix this;improve memory accounting;improve;Similar;0.5634616017341614
"reordering the use of the ""actual size"" information";fix this;to find out the size of a potentially huge page;Its unnecessarily hard;Similar;0.5836173295974731
"reordering the use of the ""actual size"" information";fix this;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.5644733905792236
"reordering the use of the ""actual size"" information";fix this;configure and build (select SLOB allocator);SLOB allocator;Similar;0.5490992069244385
"reordering the use of the ""actual size"" information";fix this;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.5477075576782227
"reordering the use of the ""actual size"" information";fix this;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.6302210092544556
"reordering the use of the ""actual size"" information";fix this;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.6302210092544556
"reordering the use of the ""actual size"" information";fix this;to be applied to the file;a file by file comparison of the scanner;Similar;0.6700612306594849
"reordering the use of the ""actual size"" information";fix this;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.5794985294342041
"reordering the use of the ""actual size"" information";fix this;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5837228298187256
"reordering the use of the ""actual size"" information";fix this;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.5942869186401367
"reordering the use of the ""actual size"" information";fix this;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.5841497182846069
"reordering the use of the ""actual size"" information";fix this;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5279562473297119
"reordering the use of the ""actual size"" information";fix this;We always end up returning some objects at the cost of another cmpxchg  ;Else;Similar;0.5327924489974976
"reordering the use of the ""actual size"" information";fix this;resurrects approach first proposed in [1];To fix this issue;Similar;0.6961888074874878
"reordering the use of the ""actual size"" information";fix this;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5453236103057861
"reordering the use of the ""actual size"" information";fix this;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5289185047149658
"reordering the use of the ""actual size"" information";fix this;unioned together;Conveniently;Similar;0.6049212217330933
"reordering the use of the ""actual size"" information";fix this;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5394278764724731
"reordering the use of the ""actual size"" information";fix this;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5626044273376465
"reordering the use of the ""actual size"" information";fix this;function naming changes;requires some;Similar;0.6367502212524414
"reordering the use of the ""actual size"" information";fix this;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.5121307373046875
"reordering the use of the ""actual size"" information";fix this;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5019713640213013
"reordering the use of the ""actual size"" information";fix this;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5753464102745056
"reordering the use of the ""actual size"" information";fix this;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5784884691238403
"reordering the use of the ""actual size"" information";fix this;Improve trace accuracy;by correctly tracing reported size;Similar;0.5722055435180664
"reordering the use of the ""actual size"" information";fix this;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.6442514061927795
"reordering the use of the ""actual size"" information";fix this;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5807015895843506
"reordering the use of the ""actual size"" information";fix this;can be done in __kmem_cache_shutdown;What is done there;Similar;0.513070285320282
"reordering the use of the ""actual size"" information";fix this;This affects RCU handling;somewhat;Similar;0.6024911403656006
"reordering the use of the ""actual size"" information";fix this;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.5325325727462769
"reordering the use of the ""actual size"" information";fix this;Remove various small accessors;various small;Similar;0.5515305399894714
"reordering the use of the ""actual size"" information";fix this;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5018018484115601
"reordering the use of the ""actual size"" information";fix this;Fix gfp flags passed to lockdep;lockdep;Similar;0.5563534498214722
"reordering the use of the ""actual size"" information";fix this;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6182249188423157
"reordering the use of the ""actual size"" information";fix this;Adding this mask;fixes the bug;Similar;0.6829855442047119
"reordering the use of the ""actual size"" information";fix this;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5260831117630005
"reordering the use of the ""actual size"" information";fix this;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6482082605361938
"reordering the use of the ""actual size"" information";fix this;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5644240379333496
"reordering the use of the ""actual size"" information";fix this;Remove kmemtrace ftrace plugin;tracing;Similar;0.5168251395225525
"reordering the use of the ""actual size"" information";fix this;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5537174344062805
"reordering the use of the ""actual size"" information";fix this;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5017082691192627
"reordering the use of the ""actual size"" information";fix this;refactor code for future changes;Impact;Similar;0.5043457746505737
"reordering the use of the ""actual size"" information";fix this;use tracepoints;kmemtrace;Similar;0.6701611280441284
"reordering the use of the ""actual size"" information";fix this;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5369431972503662
"reordering the use of the ""actual size"" information";fix this;fix typo in mm/slob.c;build fix;Similar;0.5726251602172852
"reordering the use of the ""actual size"" information";fix this;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5599642992019653
"reordering the use of the ""actual size"" information";fix this;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5229887962341309
"reordering the use of the ""actual size"" information";fix this;enable and use this tracer;To enable and use this tracer;Similar;0.6680511236190796
"reordering the use of the ""actual size"" information";fix this;I find it more readable  ;personal opinion;Similar;0.5762861967086792
"reordering the use of the ""actual size"" information";fix this;Drop it  ;if you want;Similar;0.5250398516654968
"reordering the use of the ""actual size"" information";fix this;fix bogus ksize calculation fix  ;SLOB;Similar;0.5838088393211365
"reordering the use of the ""actual size"" information";fix this;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5922219753265381
"reordering the use of the ""actual size"" information";fix this;fix bogus ksize calculation;bogus;Similar;0.601675808429718
"reordering the use of the ""actual size"" information";fix this;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5434690117835999
"reordering the use of the ""actual size"" information";fix this;record page flag overlays explicitly;slob;Similar;0.6066333055496216
"reordering the use of the ""actual size"" information";fix this;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5846900939941406
"reordering the use of the ""actual size"" information";fix this;Handle that separately in krealloc();separately;Similar;0.5686638355255127
"reordering the use of the ""actual size"" information";fix this;reduce list scanning;slob;Similar;0.5668275952339172
"reordering the use of the ""actual size"" information";fix this;improved alignment handling;improved;Similar;0.5296933650970459
"reordering the use of the ""actual size"" information";fix this;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5404736995697021
"reordering the use of the ""actual size"" information";fix this;remove bigblock tracking;slob;Similar;0.6262026429176331
"reordering the use of the ""actual size"" information";fix this;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5050877928733826
"reordering the use of the ""actual size"" information";fix this;rework freelist handling;slob;Similar;0.5861518979072571
"reordering the use of the ""actual size"" information";fix this;"align them to word size
";it is best in practice;Similar;0.7781693339347839
"reordering the use of the ""actual size"" information";fix this;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5615370273590088
"reordering the use of the ""actual size"" information";fix this;SLOB to be used on SMP  ;allows;Similar;0.5960037708282471
"reordering the use of the ""actual size"" information";fix this;fix page order calculation on not 4KB page;-;Similar;0.5285524725914001
"reordering the use of the ""actual size"" information";fix this;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5106425285339355
"reordering the use of the ""actual size"" information";fix this;handle SLAB_PANIC flag;slob;Similar;0.5106413960456848
"reordering the use of the ""actual size"" information";fix this;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6759533286094666
"reordering the use of the ""actual size"" information";fix this;Redo a lot of comments;Also;Similar;0.577954888343811
"reordering the use of the ""actual size"" information";fix this;simplifies SLOB;at this point slob may be broken;Similar;0.6379426717758179
"reordering the use of the ""actual size"" information";fix this;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5888729095458984
"reordering the use of the ""actual size"" information";fix this;fix;SLOB=y && SMP=y fix;Similar;0.6319682598114014
"reordering the use of the ""actual size"" information";fix this;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6553946733474731
"Code can ask ""how large an allocation would I get for a given size?""";instead;Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Similar;0.6791644096374512
"Code can ask ""how large an allocation would I get for a given size?""";instead;Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Similar;0.5331739187240601
"Code can ask ""how large an allocation would I get for a given size?""";instead;Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Similar;0.6136487722396851
"Code can ask ""how large an allocation would I get for a given size?""";instead;Add mem_dump_obj() to print source of memory block;to print source of memory block;Similar;0.5153558254241943
"Code can ask ""how large an allocation would I get for a given size?""";instead;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.5435880422592163
"Code can ask ""how large an allocation would I get for a given size?""";instead;The information printed can depend on kernel configuration;depend on kernel configuration;Similar;0.6136558055877686
"Code can ask ""how large an allocation would I get for a given size?""";instead;build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Similar;0.5451307892799377
"Code can ask ""how large an allocation would I get for a given size?""";instead;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.5623403787612915
"Code can ask ""how large an allocation would I get for a given size?""";instead;have some use eventually for annotations in drivers/gpu;I might;Similar;0.5392193794250488
"Code can ask ""how large an allocation would I get for a given size?""";instead;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.5076056122779846
"Code can ask ""how large an allocation would I get for a given size?""";instead;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.6290963888168335
"Code can ask ""how large an allocation would I get for a given size?""";instead;to find out the size of a potentially huge page;Its unnecessarily hard;Similar;0.7234177589416504
"Code can ask ""how large an allocation would I get for a given size?""";instead;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.5378137230873108
"Code can ask ""how large an allocation would I get for a given size?""";instead;configure and build (select SLOB allocator);SLOB allocator;Similar;0.5552985072135925
"Code can ask ""how large an allocation would I get for a given size?""";instead;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.6013187170028687
"Code can ask ""how large an allocation would I get for a given size?""";instead;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.6013187170028687
"Code can ask ""how large an allocation would I get for a given size?""";instead;to be applied to the file;a file by file comparison of the scanner;Similar;0.6010802984237671
"Code can ask ""how large an allocation would I get for a given size?""";instead;Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Similar;0.5682380199432373
"Code can ask ""how large an allocation would I get for a given size?""";instead;allow future extension of the bulk alloc API;is done to;Similar;0.5572510957717896
"Code can ask ""how large an allocation would I get for a given size?""";instead;We always end up returning some objects at the cost of another cmpxchg  ;Else;Similar;0.5535967350006104
"Code can ask ""how large an allocation would I get for a given size?""";instead;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5134148597717285
"Code can ask ""how large an allocation would I get for a given size?""";instead;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5152611136436462
"Code can ask ""how large an allocation would I get for a given size?""";instead;unioned together;Conveniently;Similar;0.5011498928070068
"Code can ask ""how large an allocation would I get for a given size?""";instead;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5569913387298584
"Code can ask ""how large an allocation would I get for a given size?""";instead;function naming changes;requires some;Similar;0.5083003044128418
"Code can ask ""how large an allocation would I get for a given size?""";instead;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.6508801579475403
"Code can ask ""how large an allocation would I get for a given size?""";instead;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5814030170440674
"Code can ask ""how large an allocation would I get for a given size?""";instead;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5678408145904541
"Code can ask ""how large an allocation would I get for a given size?""";instead;This affects RCU handling;somewhat;Similar;0.5365746021270752
"Code can ask ""how large an allocation would I get for a given size?""";instead;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5143098831176758
"Code can ask ""how large an allocation would I get for a given size?""";instead;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5879525542259216
"Code can ask ""how large an allocation would I get for a given size?""";instead;refactor code for future changes;Impact;Similar;0.5032562017440796
"Code can ask ""how large an allocation would I get for a given size?""";instead;fix typo in mm/slob.c;build fix;Similar;0.50128173828125
"Code can ask ""how large an allocation would I get for a given size?""";instead;enable and use this tracer;To enable and use this tracer;Similar;0.5184810757637024
"Code can ask ""how large an allocation would I get for a given size?""";instead;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5054322481155396
"Code can ask ""how large an allocation would I get for a given size?""";instead;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5580313205718994
"Code can ask ""how large an allocation would I get for a given size?""";instead;"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;Similar;0.5322028994560242
"Code can ask ""how large an allocation would I get for a given size?""";instead;"align them to word size
";it is best in practice;Similar;0.6571955680847168
"Code can ask ""how large an allocation would I get for a given size?""";instead;SLOB to be used on SMP  ;allows;Similar;0.5422300100326538
"Code can ask ""how large an allocation would I get for a given size?""";instead;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5413175821304321
"Code can ask ""how large an allocation would I get for a given size?""";instead;Redo a lot of comments;Also;Similar;0.5401828289031982
"Code can ask ""how large an allocation would I get for a given size?""";instead;SLOB saving nearly half a megabyte of RAM;Comparison for otherwise identical builds;Similar;0.5354668498039246
"Code can ask ""how large an allocation would I get for a given size?""";instead;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6925407648086548
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Similar;0.6396620869636536
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";unify NUMA and UMA version of tracepoints;mm/slab_common;Similar;0.7337067723274231
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Similar;0.7772455215454102
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Similar;0.5684810876846313
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;Similar;0.5465779900550842
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Similar;0.7928802967071533
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Remove unnecessary page_mapcount_reset() function call;unnecessary;Similar;0.5766663551330566
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";use struct folio instead of struct page;Where non-slab page can appear;Similar;0.5979686379432678
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Add mem_dump_obj() to print source of memory block;to print source of memory block;Similar;0.691567599773407
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.654901385307312
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";The information printed can depend on kernel configuration;depend on kernel configuration;Similar;0.5530201196670532
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Similar;0.534170925617218
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.715484082698822
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.7470756769180298
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.5865445137023926
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.5170906782150269
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";union of slab_list and lru;slab_list and lru are the same bits;Similar;0.7103263139724731
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";configure and build (select SLOB allocator);SLOB allocator;Similar;0.6893914937973022
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.5511651039123535
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.7925420999526978
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.7925420999526978
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";to be applied to the file;a file by file comparison of the scanner;Similar;0.6243141889572144
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.5168818831443787
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5875443816184998
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.5855474472045898
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.6441648602485657
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.5805855393409729
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5405073165893555
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";allow future extension of the bulk alloc API;is done to;Similar;0.5874961018562317
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5134837627410889
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";resurrects approach first proposed in [1];To fix this issue;Similar;0.6559193134307861
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5870232582092285
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5692855715751648
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5361372828483582
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";unioned together;Conveniently;Similar;0.5427826642990112
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.6667309999465942
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";function naming changes;requires some;Similar;0.6484702825546265
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.6506484746932983
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.7172868251800537
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.6265889406204224
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.6698011159896851
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.8136509656906128
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5700930953025818
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";can be done in __kmem_cache_shutdown;What is done there;Similar;0.5885491967201233
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";This affects RCU handling;somewhat;Similar;0.509819507598877
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6146615743637085
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Fix gfp flags passed to lockdep;lockdep;Similar;0.6570508480072021
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.7746623754501343
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Adding this mask;fixes the bug;Similar;0.6109825372695923
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5697826147079468
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.7563878893852234
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5568473935127258
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Remove kmemtrace ftrace plugin;tracing;Similar;0.6035622358322144
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5570785999298096
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5626834630966187
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";refactor code for future changes;Impact;Similar;0.5208531618118286
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.6717244386672974
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";use tracepoints;kmemtrace;Similar;0.6304110288619995
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6287345886230469
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";fix typo in mm/slob.c;build fix;Similar;0.7011029720306396
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";fix lockup in slob_free()  ;lockup;Similar;0.5317986011505127
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.642903208732605
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5420792698860168
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";enable and use this tracer;To enable and use this tracer;Similar;0.5995609760284424
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";fix bogus ksize calculation fix  ;SLOB;Similar;0.5710639953613281
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";fix bogus ksize calculation;bogus;Similar;0.5592830777168274
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";record page flag overlays explicitly;slob;Similar;0.5284641981124878
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Handle that separately in krealloc();separately;Similar;0.5795137882232666
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5853952169418335
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5976850986480713
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";remove bigblock tracking;slob;Similar;0.6562385559082031
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";rework freelist handling;slob;Similar;0.5590770244598389
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";"align them to word size
";it is best in practice;Similar;0.7123129367828369
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.7137261629104614
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";SLOB to be used on SMP  ;allows;Similar;0.6694926023483276
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5680086612701416
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";handle SLAB_PANIC flag;slob;Similar;0.6307448744773865
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.813710629940033
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.55975741147995
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5260260105133057
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Redo a lot of comments;Also;Similar;0.5310486555099487
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5214345455169678
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";simplifies SLOB;at this point slob may be broken;Similar;0.6015483140945435
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5161875486373901
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6674020290374756
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";fix;SLOB=y && SMP=y fix;Similar;0.5641406178474426
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.7738783955574036
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;unify NUMA and UMA version of tracepoints;mm/slab_common;Similar;0.596216082572937
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Similar;0.5167988538742065
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Similar;0.7361083030700684
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Similar;0.575852632522583
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Remove unnecessary page_mapcount_reset() function call;unnecessary;Similar;0.7636756896972656
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Use unused field for units;instead;Similar;0.527011513710022
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;use struct folio instead of struct page;Where non-slab page can appear;Similar;0.649999737739563
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Similar;0.6328728199005127
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Similar;0.679208517074585
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Similar;0.6110216379165649
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Similar;0.734376072883606
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Similar;0.5796647071838379
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Add mem_dump_obj() to print source of memory block;to print source of memory block;Similar;0.6757795810699463
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.6484131217002869
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Similar;0.5796287059783936
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.5874868631362915
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.6443052291870117
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.5423822402954102
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.7747225761413574
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.596869945526123
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;configure and build (select SLOB allocator);SLOB allocator;Similar;0.5183596014976501
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.7054174542427063
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.6764050722122192
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.6764050722122192
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.6359946131706238
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.6580652594566345
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Similar;0.5197816491127014
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;to be applied to the file;a file by file comparison of the scanner;Similar;0.5167951583862305
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;For non */uapi/* files;that summary was;Similar;0.5999361276626587
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.7118117809295654
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.7285565137863159
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Similar;0.5296841859817505
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.7765388488769531
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.5585038661956787
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.7951642274856567
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.660494327545166
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;allow future extension of the bulk alloc API;is done to;Similar;0.5034214854240417
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Make dead caches discard free slabs immediately;slub;Similar;0.5162908434867859
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5267683863639832
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;resurrects approach first proposed in [1];To fix this issue;Similar;0.5558794736862183
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5520118474960327
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5607185363769531
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5396847128868103
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.6779109239578247
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5821400880813599
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.7856588363647461
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5257325768470764
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;function naming changes;requires some;Similar;0.6509772539138794
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.6719221472740173
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.7153897285461426
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5114777088165283
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5941398739814758
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5632336139678955
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5887877345085144
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5530993938446045
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;can be done in __kmem_cache_shutdown;What is done there;Similar;0.7373167276382446
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Remove various small accessors;various small;Similar;0.6020622253417969
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.59918212890625
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Fix gfp flags passed to lockdep;lockdep;Similar;0.6145991086959839
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6492580771446228
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5526440143585205
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6515079736709595
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5847660303115845
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Remove kmemtrace ftrace plugin;tracing;Similar;0.8384854793548584
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.7789393067359924
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.671162486076355
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;refactor code for future changes;Impact;Similar;0.5178128480911255
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.513651967048645
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6358298063278198
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;fix typo in mm/slob.c;build fix;Similar;0.6550744771957397
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;fix lockup in slob_free()  ;lockup;Similar;0.5407114028930664
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6869605779647827
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5057793259620667
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Drop it  ;if you want;Similar;0.6420703530311584
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;fix bogus ksize calculation fix  ;SLOB;Similar;0.5641783475875854
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;fix bogus ksize calculation;bogus;Similar;0.5765953660011292
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.7263451814651489
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Fix to return wrong pointer;Fix;Similar;0.5260617733001709
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5336044430732727
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;remove useless ctor parameter and reorder parameters;useless;Similar;0.7021095156669617
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Handle that separately in krealloc();separately;Similar;0.6114822626113892
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5103118419647217
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;reduce list scanning;slob;Similar;0.7084519267082214
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5628746747970581
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Cleanup zeroing allocations;Slab allocators;Similar;0.632339358329773
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5407769680023193
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5721657276153564
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5021553635597229
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;remove bigblock tracking;slob;Similar;0.7850024700164795
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;rework freelist handling;slob;Similar;0.5030987858772278
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.583411455154419
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6355389952659607
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5460383296012878
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;SLOB to be used on SMP  ;allows;Similar;0.5773252248764038
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;fix page order calculation on not 4KB page;-;Similar;0.5635306239128113
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.550316333770752
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.8083182573318481
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;handle SLAB_PANIC flag;slob;Similar;0.6459118127822876
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5733029842376709
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.6045860648155212
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.533646821975708
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6832747459411621
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.7373393774032593
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.7062056064605713
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5156270265579224
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7324743270874023
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5782325267791748
unify NUMA and UMA version of tracepoints;mm/slab_common;Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Similar;0.631744921207428
unify NUMA and UMA version of tracepoints;mm/slab_common;Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Similar;0.575432300567627
unify NUMA and UMA version of tracepoints;mm/slab_common;Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Similar;0.6758701801300049
unify NUMA and UMA version of tracepoints;mm/slab_common;use struct folio instead of struct page;Where non-slab page can appear;Similar;0.5141088962554932
unify NUMA and UMA version of tracepoints;mm/slab_common;By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Similar;0.5047457218170166
unify NUMA and UMA version of tracepoints;mm/slab_common;Add mem_dump_obj() to print source of memory block;to print source of memory block;Similar;0.580057680606842
unify NUMA and UMA version of tracepoints;mm/slab_common;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.6348394751548767
unify NUMA and UMA version of tracepoints;mm/slab_common;The information printed can depend on kernel configuration;depend on kernel configuration;Similar;0.5380178093910217
unify NUMA and UMA version of tracepoints;mm/slab_common;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.5885288715362549
unify NUMA and UMA version of tracepoints;mm/slab_common;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.6407449245452881
unify NUMA and UMA version of tracepoints;mm/slab_common;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.6091365218162537
unify NUMA and UMA version of tracepoints;mm/slab_common;make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;Similar;0.5301804542541504
unify NUMA and UMA version of tracepoints;mm/slab_common;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.7601293325424194
unify NUMA and UMA version of tracepoints;mm/slab_common;configure and build (select SLOB allocator);SLOB allocator;Similar;0.6342678070068359
unify NUMA and UMA version of tracepoints;mm/slab_common;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.5279285907745361
unify NUMA and UMA version of tracepoints;mm/slab_common;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.6836338043212891
unify NUMA and UMA version of tracepoints;mm/slab_common;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.6836338043212891
unify NUMA and UMA version of tracepoints;mm/slab_common;to be applied to the file;a file by file comparison of the scanner;Similar;0.6350842714309692
unify NUMA and UMA version of tracepoints;mm/slab_common;For non */uapi/* files;that summary was;Similar;0.5684139728546143
unify NUMA and UMA version of tracepoints;mm/slab_common;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.5389336943626404
unify NUMA and UMA version of tracepoints;mm/slab_common;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5687888860702515
unify NUMA and UMA version of tracepoints;mm/slab_common;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.6323869824409485
unify NUMA and UMA version of tracepoints;mm/slab_common;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.5519652366638184
unify NUMA and UMA version of tracepoints;mm/slab_common;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5430318117141724
unify NUMA and UMA version of tracepoints;mm/slab_common;allow future extension of the bulk alloc API;is done to;Similar;0.5929074883460999
unify NUMA and UMA version of tracepoints;mm/slab_common;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5744298696517944
unify NUMA and UMA version of tracepoints;mm/slab_common;resurrects approach first proposed in [1];To fix this issue;Similar;0.642396867275238
unify NUMA and UMA version of tracepoints;mm/slab_common;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5304606556892395
unify NUMA and UMA version of tracepoints;mm/slab_common;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.571872353553772
unify NUMA and UMA version of tracepoints;mm/slab_common;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5293228626251221
unify NUMA and UMA version of tracepoints;mm/slab_common;unioned together;Conveniently;Similar;0.6883302927017212
unify NUMA and UMA version of tracepoints;mm/slab_common;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5266085863113403
unify NUMA and UMA version of tracepoints;mm/slab_common;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.6005542874336243
unify NUMA and UMA version of tracepoints;mm/slab_common;function naming changes;requires some;Similar;0.5971659421920776
unify NUMA and UMA version of tracepoints;mm/slab_common;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5757690072059631
unify NUMA and UMA version of tracepoints;mm/slab_common;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.6226328611373901
unify NUMA and UMA version of tracepoints;mm/slab_common;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5765160322189331
unify NUMA and UMA version of tracepoints;mm/slab_common;Improve trace accuracy;by correctly tracing reported size;Similar;0.5161845088005066
unify NUMA and UMA version of tracepoints;mm/slab_common;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5374974012374878
unify NUMA and UMA version of tracepoints;mm/slab_common;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.6661322116851807
unify NUMA and UMA version of tracepoints;mm/slab_common;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5373862981796265
unify NUMA and UMA version of tracepoints;mm/slab_common;This affects RCU handling;somewhat;Similar;0.526604175567627
unify NUMA and UMA version of tracepoints;mm/slab_common;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6146054267883301
unify NUMA and UMA version of tracepoints;mm/slab_common;Fix gfp flags passed to lockdep;lockdep;Similar;0.6495811939239502
unify NUMA and UMA version of tracepoints;mm/slab_common;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.7040094137191772
unify NUMA and UMA version of tracepoints;mm/slab_common;Adding this mask;fixes the bug;Similar;0.5781834125518799
unify NUMA and UMA version of tracepoints;mm/slab_common;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5342223644256592
unify NUMA and UMA version of tracepoints;mm/slab_common;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6925638914108276
unify NUMA and UMA version of tracepoints;mm/slab_common;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5094083547592163
unify NUMA and UMA version of tracepoints;mm/slab_common;Remove kmemtrace ftrace plugin;tracing;Similar;0.5595695376396179
unify NUMA and UMA version of tracepoints;mm/slab_common;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5319526195526123
unify NUMA and UMA version of tracepoints;mm/slab_common;refactor code for future changes;Impact;Similar;0.5706046223640442
unify NUMA and UMA version of tracepoints;mm/slab_common;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.6828420758247375
unify NUMA and UMA version of tracepoints;mm/slab_common;use tracepoints;kmemtrace;Similar;0.7111728191375732
unify NUMA and UMA version of tracepoints;mm/slab_common;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6006513833999634
unify NUMA and UMA version of tracepoints;mm/slab_common;fix typo in mm/slob.c;build fix;Similar;0.6672669649124146
unify NUMA and UMA version of tracepoints;mm/slab_common;fix lockup in slob_free()  ;lockup;Similar;0.5544843673706055
unify NUMA and UMA version of tracepoints;mm/slab_common;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.668960452079773
unify NUMA and UMA version of tracepoints;mm/slab_common;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6001458168029785
unify NUMA and UMA version of tracepoints;mm/slab_common;enable and use this tracer;To enable and use this tracer;Similar;0.6694478988647461
unify NUMA and UMA version of tracepoints;mm/slab_common;fix bogus ksize calculation fix  ;SLOB;Similar;0.5666710138320923
unify NUMA and UMA version of tracepoints;mm/slab_common;fix bogus ksize calculation;bogus;Similar;0.5568022727966309
unify NUMA and UMA version of tracepoints;mm/slab_common;record page flag overlays explicitly;slob;Similar;0.5328174829483032
unify NUMA and UMA version of tracepoints;mm/slab_common;Handle that separately in krealloc();separately;Similar;0.6159582138061523
unify NUMA and UMA version of tracepoints;mm/slab_common;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6184895038604736
unify NUMA and UMA version of tracepoints;mm/slab_common;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5064346194267273
unify NUMA and UMA version of tracepoints;mm/slab_common;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5651763677597046
unify NUMA and UMA version of tracepoints;mm/slab_common;remove bigblock tracking;slob;Similar;0.5414159297943115
unify NUMA and UMA version of tracepoints;mm/slab_common;rework freelist handling;slob;Similar;0.6513453722000122
unify NUMA and UMA version of tracepoints;mm/slab_common;"align them to word size
";it is best in practice;Similar;0.6151452660560608
unify NUMA and UMA version of tracepoints;mm/slab_common;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5884187817573547
unify NUMA and UMA version of tracepoints;mm/slab_common;SLOB to be used on SMP  ;allows;Similar;0.6906129717826843
unify NUMA and UMA version of tracepoints;mm/slab_common;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5229039192199707
unify NUMA and UMA version of tracepoints;mm/slab_common;handle SLAB_PANIC flag;slob;Similar;0.5614783763885498
unify NUMA and UMA version of tracepoints;mm/slab_common;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.7609577178955078
unify NUMA and UMA version of tracepoints;mm/slab_common;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5243538618087769
unify NUMA and UMA version of tracepoints;mm/slab_common;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6172754764556885
unify NUMA and UMA version of tracepoints;mm/slab_common;Redo a lot of comments;Also;Similar;0.5010070204734802
unify NUMA and UMA version of tracepoints;mm/slab_common;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5047745704650879
unify NUMA and UMA version of tracepoints;mm/slab_common;simplifies SLOB;at this point slob may be broken;Similar;0.6040403842926025
unify NUMA and UMA version of tracepoints;mm/slab_common;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5086279511451721
unify NUMA and UMA version of tracepoints;mm/slab_common;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6203327775001526
unify NUMA and UMA version of tracepoints;mm/slab_common;fix;SLOB=y && SMP=y fix;Similar;0.5830594301223755
unify NUMA and UMA version of tracepoints;mm/slab_common;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.7452638149261475
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Similar;0.6260138750076294
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Similar;0.5186580419540405
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Similar;0.798859179019928
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Remove unnecessary page_mapcount_reset() function call;unnecessary;Similar;0.6245149374008179
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;use struct folio instead of struct page;Where non-slab page can appear;Similar;0.5215979814529419
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Similar;0.5412400960922241
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Add mem_dump_obj() to print source of memory block;to print source of memory block;Similar;0.6742929816246033
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Similar;0.5734006762504578
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Similar;0.5524998903274536
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.6750808954238892
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;The information printed can depend on kernel configuration;depend on kernel configuration;Similar;0.5545434951782227
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Similar;0.5245233774185181
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.7221811413764954
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.7182687520980835
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.5158435106277466
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Similar;0.5645560622215271
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.5159515142440796
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.6121776103973389
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;configure and build (select SLOB allocator);SLOB allocator;Similar;0.6999988555908203
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.6087238788604736
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.7919121980667114
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.7919121980667114
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;to be applied to the file;a file by file comparison of the scanner;Similar;0.562465488910675
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Similar;0.5088799595832825
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.5097290277481079
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.6283365488052368
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.5552623271942139
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.631086528301239
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.6149556636810303
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.5681818127632141
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5697442293167114
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;resurrects approach first proposed in [1];To fix this issue;Similar;0.5599355697631836
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.6132012605667114
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.7137643098831177
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5872519016265869
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;function naming changes;requires some;Similar;0.6170849800109863
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.7161325216293335
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.6972216367721558
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5811758041381836
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5425162315368652
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.9312577247619629
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.7608323693275452
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Similar;0.5963587760925293
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5696732997894287
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;cleans up some bitrot in slob.c;Also;Similar;0.540305495262146
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5858282446861267
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Fix gfp flags passed to lockdep;lockdep;Similar;0.662148654460907
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.7821609973907471
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Adding this mask;fixes the bug;Similar;0.5198239088058472
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5504796504974365
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.7734861373901367
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Remove kmemtrace ftrace plugin;tracing;Similar;0.5847524404525757
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6300612688064575
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5512471199035645
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.7075661420822144
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;use tracepoints;kmemtrace;Similar;0.6070694327354431
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6954253911972046
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;fix typo in mm/slob.c;build fix;Similar;0.6667240858078003
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;fix lockup in slob_free()  ;lockup;Similar;0.5760045051574707
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.680789589881897
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6769114136695862
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;enable and use this tracer;To enable and use this tracer;Similar;0.5861604809761047
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;fix bogus ksize calculation fix  ;SLOB;Similar;0.538678765296936
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;fix bogus ksize calculation;bogus;Similar;0.5210723876953125
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Fix to return wrong pointer;Fix;Similar;0.5457477569580078
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Handle that separately in krealloc();separately;Similar;0.539013683795929
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5763990879058838
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5747997164726257
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;remove bigblock tracking;slob;Similar;0.5321803092956543
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5142594575881958
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;rework freelist handling;slob;Similar;0.5424951314926147
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.7190017700195312
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5570946335792542
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;SLOB to be used on SMP  ;allows;Similar;0.5791633725166321
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5767892599105835
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5301647782325745
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;handle SLAB_PANIC flag;slob;Similar;0.5291037559509277
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6667220592498779
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5019788146018982
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5707842111587524
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5261777639389038
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5582504272460938
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.50836181640625
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5836466550827026
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.653742253780365
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.611816942691803
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;Similar;0.530235767364502
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;Similar;0.541799008846283
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Similar;0.6005783081054688
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Remove unnecessary page_mapcount_reset() function call;unnecessary;Similar;0.7727892398834229
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Use unused field for units;instead;Similar;0.5357239842414856
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;use struct folio instead of struct page;Where non-slab page can appear;Similar;0.6146215200424194
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Similar;0.6799006462097168
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Similar;0.5514088869094849
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Similar;0.742691695690155
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Similar;0.714486837387085
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Similar;0.5965723991394043
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;kmem_cache_free;We can use it to understand what the RCU core is going to free;Similar;0.5793212652206421
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Add mem_dump_obj() to print source of memory block;to print source of memory block;Similar;0.593978762626648
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Similar;0.500349760055542
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.6393711566925049
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Similar;0.5876488089561462
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.6789743900299072
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.6767343282699585
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Similar;0.5031290054321289
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.738903284072876
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.5505489110946655
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;configure and build (select SLOB allocator);SLOB allocator;Similar;0.5034841299057007
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.7166885137557983
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.6284257173538208
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.6284257173538208
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.7116378545761108
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.6862474679946899
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Similar;0.6671454310417175
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;All documentation files were explicitly excluded;explicitly excluded;Similar;0.5181430578231812
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;For non */uapi/* files;that summary was;Similar;0.7143315076828003
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.6923322081565857
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.6894118785858154
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.6679452657699585
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.642996072769165
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.6027733087539673
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5370875597000122
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5674322843551636
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.6205688714981079
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.59818035364151
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.7872492074966431
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.6075825691223145
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.7691503763198853
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5140296816825867
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;function naming changes;requires some;Similar;0.5770984888076782
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.6410108804702759
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.597973644733429
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.6302313804626465
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.6125731468200684
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.6244117021560669
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;can be done in __kmem_cache_shutdown;What is done there;Similar;0.7520061731338501
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Remove various small accessors;various small;Similar;0.6082526445388794
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.5477550625801086
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;cleans up some bitrot in slob.c;Also;Similar;0.5256669521331787
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5743845701217651
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Fix gfp flags passed to lockdep;lockdep;Similar;0.6445430517196655
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6292714476585388
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5888656377792358
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6489915251731873
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.6575526595115662
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Remove kmemtrace ftrace plugin;tracing;Similar;0.8214273452758789
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.7198537588119507
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.7221509218215942
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5522811412811279
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.5342440605163574
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.6097328662872314
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6504829525947571
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;fix typo in mm/slob.c;build fix;Similar;0.6805784702301025
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;fix lockup in slob_free()  ;lockup;Similar;0.6217870712280273
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.594107449054718
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5283282995223999
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5156658887863159
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Drop it  ;if you want;Similar;0.5445301532745361
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;fix bogus ksize calculation fix  ;SLOB;Similar;0.5948274731636047
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;fix bogus ksize calculation;bogus;Similar;0.6057236194610596
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.7077279686927795
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Fix to return wrong pointer;Fix;Similar;0.5724450349807739
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5377223491668701
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;reduce external fragmentation by using three free lists;slob;Similar;0.5112483501434326
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5031110644340515
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5152748227119446
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;fix memory corruption;memory corruption;Similar;0.5215448141098022
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;remove useless ctor parameter and reorder parameters;useless;Similar;0.7389318346977234
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Handle that separately in krealloc();separately;Similar;0.6766920685768127
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6414363384246826
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;reduce list scanning;slob;Similar;0.7185475826263428
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Cleanup zeroing allocations;Slab allocators;Similar;0.6374738812446594
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5767377018928528
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6927238702774048
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;remove bigblock tracking;slob;Similar;0.6564468145370483
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5353875160217285
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6844977140426636
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6408984661102295
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5274335145950317
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5308612585067749
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;SLOB to be used on SMP  ;allows;Similar;0.5812253952026367
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;fix page order calculation on not 4KB page;-;Similar;0.6437878608703613
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6209392547607422
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.7460723519325256
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;handle SLAB_PANIC flag;slob;Similar;0.5358836650848389
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5972200632095337
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6278698444366455
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5774566531181335
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.787392795085907
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5706890821456909
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.649722695350647
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6820746064186096
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7601180076599121
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5550515651702881
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5620551109313965
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Similar;0.6989985704421997
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Similar;0.5850745439529419
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Add mem_dump_obj() to print source of memory block;to print source of memory block;Similar;0.6495010852813721
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Similar;0.5685099959373474
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Similar;0.5251400470733643
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.589097261428833
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Similar;0.5871891975402832
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Similar;0.6830247044563293
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.5699847340583801
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.6342095732688904
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;Similar;0.5573880076408386
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;patch ensures alignment on all arches and cache sizes;Still;Similar;0.712043285369873
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;configure and build (select SLOB allocator);SLOB allocator;Similar;0.5356224775314331
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.5109105110168457
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.5673249363899231
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.5673249363899231
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Similar;0.5126540064811707
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Similar;0.5281001925468445
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.5099807381629944
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Similar;0.515803873538971
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.538288950920105
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.534162163734436
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5135828256607056
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Make dead caches discard free slabs immediately;slub;Similar;0.5708613991737366
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.560051679611206
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5596234798431396
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5550199747085571
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5906939506530762
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5702928900718689
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5218773484230042
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5181890726089478
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5285884737968445
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;Similar;0.5572235584259033
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5545159578323364
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5735472440719604
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5334454774856567
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5028020143508911
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.6322966814041138
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5496476888656616
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5041799545288086
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5666205883026123
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5348256826400757
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6470746994018555
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5803508758544922
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;record page flag overlays explicitly;slob;Similar;0.5727896690368652
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5059934258460999
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.6650450229644775
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.6611860990524292
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.537655234336853
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5003271698951721
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5784475803375244
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5362090468406677
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5672861337661743
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5658203959465027
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6158920526504517
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5759404897689819
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5177809000015259
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.5383941531181335
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.5535397529602051
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;The size of slab memory shouldnt exceed 4Gb on 32-bit machines;it will fit into atomic_long_t we use for vmstats;Similar;0.5215215682983398
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.5188642144203186
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.5066312551498413
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.5066312551498413
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.5778231620788574
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;allow future extension of the bulk alloc API;is done to;Similar;0.5209817290306091
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5139098167419434
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5115637183189392
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5437177419662476
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5195741653442383
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5267974138259888
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.548123836517334
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;remove bigblock tracking;slob;Similar;0.5188978910446167
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5246599316596985
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;SLOB saving nearly half a megabyte of RAM;Comparison for otherwise identical builds;Similar;0.5256710052490234
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5359774827957153
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.52234947681427
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;Remove unnecessary page_mapcount_reset() function call;unnecessary;Similar;0.5246168375015259
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Similar;0.5327520370483398
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Similar;0.583149790763855
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Similar;0.5104658603668213
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;kmem_cache_free;We can use it to understand what the RCU core is going to free;Similar;0.5843799114227295
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;The information printed can depend on kernel configuration;depend on kernel configuration;Similar;0.5096306204795837
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.5637097954750061
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.5452951192855835
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;Similar;0.5093724727630615
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.5946071147918701
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.5591416358947754
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;For non */uapi/* files;that summary was;Similar;0.6068297624588013
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.5078092217445374
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5659686923027039
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Similar;0.5889368057250977
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.5369116067886353
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.5465212464332581
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;We always end up returning some objects at the cost of another cmpxchg  ;Else;Similar;0.6191884279251099
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5735229253768921
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5678051710128784
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Similar;0.5073032379150391
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.5635918378829956
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5138710737228394
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5307660698890686
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.5733392238616943
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;Similar;0.5046770572662354
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5299603939056396
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5159693956375122
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6270220279693604
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;This affects RCU handling;somewhat;Similar;0.5253701210021973
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.572080135345459
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5166486501693726
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5350774526596069
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5620202422142029
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;use tracepoints;kmemtrace;Similar;0.5034644603729248
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;fix lockup in slob_free()  ;lockup;Similar;0.5848628282546997
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;enable and use this tracer;To enable and use this tracer;Similar;0.5025440454483032
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;fix bogus ksize calculation fix  ;SLOB;Similar;0.5275682210922241
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5387231707572937
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;fix bogus ksize calculation;bogus;Similar;0.5390391945838928
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5466447472572327
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5792439579963684
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;Fix to return wrong pointer;Fix;Similar;0.5283880829811096
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5174360275268555
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;remove useless ctor parameter and reorder parameters;useless;Similar;0.5183190107345581
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;Handle that separately in krealloc();separately;Similar;0.5467542409896851
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5746797919273376
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.535557746887207
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5419158935546875
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;rework freelist handling;slob;Similar;0.5505776405334473
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.544741153717041
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5194872617721558
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;fix page order calculation on not 4KB page;-;Similar;0.5417226552963257
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5158066153526306
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5281410813331604
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5278288125991821
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6579123735427856
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5818238854408264
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.595384955406189
waiting for quiescence after a full Android boot;with a kernel built with CONFIG_KASAN_HW_TAGS;have some use eventually for annotations in drivers/gpu;I might;Similar;0.5521044135093689
waiting for quiescence after a full Android boot;with a kernel built with CONFIG_KASAN_HW_TAGS;verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Similar;0.5121221542358398
waiting for quiescence after a full Android boot;with a kernel built with CONFIG_KASAN_HW_TAGS;allow future extension of the bulk alloc API;is done to;Similar;0.5114171504974365
waiting for quiescence after a full Android boot;with a kernel built with CONFIG_KASAN_HW_TAGS;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.5117610692977905
waiting for quiescence after a full Android boot;with a kernel built with CONFIG_KASAN_HW_TAGS;Fix early boot kernel crash;Slob;Similar;0.5401062965393066
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Similar;0.6730292439460754
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Similar;0.6670966148376465
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;kmem_cache_free;We can use it to understand what the RCU core is going to free;Similar;0.543519914150238
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Similar;0.6016905307769775
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;The size of slab memory shouldnt exceed 4Gb on 32-bit machines;it will fit into atomic_long_t we use for vmstats;Similar;0.5313878655433655
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;"SLAB doesnt actually use page allocator directly
";no change there;Similar;0.5605624318122864
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.546343207359314
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Similar;0.5822378396987915
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;All documentation files were explicitly excluded;explicitly excluded;Similar;0.7091902494430542
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;For non */uapi/* files;that summary was;Similar;0.5451377630233765
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Similar;0.5911054611206055
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5058639645576477
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.530228316783905
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.6823097467422485
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5495332479476929
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.5755821466445923
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.7186411619186401
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.6502604484558105
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5120387077331543
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5786314606666565
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5969372987747192
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;remove useless ctor parameter and reorder parameters;useless;Similar;0.6323625445365906
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;Cleanup zeroing allocations;Slab allocators;Similar;0.5048514604568481
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5321660041809082
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.7444263100624084
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;fix page order calculation on not 4KB page;-;Similar;0.5500737428665161
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6372003555297852
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.7386322021484375
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6157820224761963
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Remove unnecessary page_mapcount_reset() function call;unnecessary;Similar;0.5854617953300476
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;use struct folio instead of struct page;Where non-slab page can appear;Similar;0.5570328235626221
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Similar;0.5876985192298889
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Add mem_dump_obj() to print source of memory block;to print source of memory block;Similar;0.7769036889076233
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Similar;0.6455036401748657
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Similar;0.5445916652679443
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.7795999646186829
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;The information printed can depend on kernel configuration;depend on kernel configuration;Similar;0.6516190767288208
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Similar;0.6407501697540283
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.7638596296310425
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;have some use eventually for annotations in drivers/gpu;I might;Similar;0.5805795192718506
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.7339310050010681
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.6675053834915161
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Similar;0.5575461387634277
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;Similar;0.5110201835632324
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;patch ensures alignment on all arches and cache sizes;Still;Similar;0.6004195213317871
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;improve memory accounting;improve;Similar;0.5591043829917908
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;add three helpers, convert the appropriate places;these three patches;Similar;0.512901246547699
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.6891111135482788
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;configure and build (select SLOB allocator);SLOB allocator;Similar;0.7176449298858643
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.601969838142395
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.8133531808853149
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.8133531808853149
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;to be applied to the file;a file by file comparison of the scanner;Similar;0.6680160164833069
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Similar;0.5587351322174072
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Similar;0.534736156463623
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Similar;0.5633673071861267
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.5350850820541382
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.6342920064926147
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.5593600273132324
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.5783196687698364
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5726708769798279
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;allow future extension of the bulk alloc API;is done to;Similar;0.6137950420379639
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.6160571575164795
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.6428487300872803
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;resurrects approach first proposed in [1];To fix this issue;Similar;0.5131139159202576
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.6714527606964111
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6431662440299988
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5189110636711121
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.6981257200241089
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;function naming changes;requires some;Similar;0.6280326843261719
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.6808474063873291
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.7263535261154175
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.6197445392608643
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.6672653555870056
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.8084738254547119
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.6054255962371826
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;can be done in __kmem_cache_shutdown;What is done there;Similar;0.598007321357727
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5531477928161621
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;cleans up some bitrot in slob.c;Also;Similar;0.5586632490158081
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6972072720527649
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Fix gfp flags passed to lockdep;lockdep;Similar;0.6977277398109436
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.7076659202575684
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Adding this mask;fixes the bug;Similar;0.5070960521697998
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.6408045291900635
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.7560315728187561
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Remove kmemtrace ftrace plugin;tracing;Similar;0.5458831787109375
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6163808107376099
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5457645058631897
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.7644389867782593
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;use tracepoints;kmemtrace;Similar;0.5868505239486694
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6690511703491211
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;fix typo in mm/slob.c;build fix;Similar;0.682975172996521
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;fix lockup in slob_free()  ;lockup;Similar;0.5559966564178467
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.7459779381752014
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.7179553508758545
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;enable and use this tracer;To enable and use this tracer;Similar;0.5899193286895752
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;record page flag overlays explicitly;slob;Similar;0.5675400495529175
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;reduce external fragmentation by using three free lists;slob;Similar;0.5132316946983337
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5718880295753479
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5350486636161804
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5237659811973572
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5262209177017212
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5836561918258667
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;rework freelist handling;slob;Similar;0.5642381310462952
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;"align them to word size
";it is best in practice;Similar;0.557192325592041
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6843898296356201
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5141231417655945
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;SLOB to be used on SMP  ;allows;Similar;0.6002971529960632
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5546315908432007
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;handle SLAB_PANIC flag;slob;Similar;0.5192831754684448
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6158896684646606
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6181315183639526
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5441137552261353
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5933783054351807
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5535733103752136
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5604875087738037
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6861659288406372
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6859017610549927
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5251575708389282
Remove unnecessary page_mapcount_reset() function call;unnecessary;use struct folio instead of struct page;Where non-slab page can appear;Similar;0.710828423500061
Remove unnecessary page_mapcount_reset() function call;unnecessary;dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Similar;0.6150627136230469
Remove unnecessary page_mapcount_reset() function call;unnecessary;By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Similar;0.6400123238563538
Remove unnecessary page_mapcount_reset() function call;unnecessary;Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Similar;0.6767547130584717
Remove unnecessary page_mapcount_reset() function call;unnecessary;mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Similar;0.7238553166389465
Remove unnecessary page_mapcount_reset() function call;unnecessary;This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Similar;0.642655611038208
Remove unnecessary page_mapcount_reset() function call;unnecessary;kmem_cache_free;We can use it to understand what the RCU core is going to free;Similar;0.5186429619789124
Remove unnecessary page_mapcount_reset() function call;unnecessary;Add mem_dump_obj() to print source of memory block;to print source of memory block;Similar;0.7285028696060181
Remove unnecessary page_mapcount_reset() function call;unnecessary;There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Similar;0.533463180065155
Remove unnecessary page_mapcount_reset() function call;unnecessary;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.6306731700897217
Remove unnecessary page_mapcount_reset() function call;unnecessary;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.6558343172073364
Remove unnecessary page_mapcount_reset() function call;unnecessary;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.7387993931770325
Remove unnecessary page_mapcount_reset() function call;unnecessary;I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Similar;0.5362153053283691
Remove unnecessary page_mapcount_reset() function call;unnecessary;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.7718079090118408
Remove unnecessary page_mapcount_reset() function call;unnecessary;improve memory accounting;improve;Similar;0.5151215195655823
Remove unnecessary page_mapcount_reset() function call;unnecessary;Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Similar;0.5385403633117676
Remove unnecessary page_mapcount_reset() function call;unnecessary;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.5196447372436523
Remove unnecessary page_mapcount_reset() function call;unnecessary;configure and build (select SLOB allocator);SLOB allocator;Similar;0.5268806219100952
Remove unnecessary page_mapcount_reset() function call;unnecessary;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.7303411960601807
Remove unnecessary page_mapcount_reset() function call;unnecessary;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.7395687103271484
Remove unnecessary page_mapcount_reset() function call;unnecessary;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.7395687103271484
Remove unnecessary page_mapcount_reset() function call;unnecessary;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.7789391875267029
Remove unnecessary page_mapcount_reset() function call;unnecessary;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.8394695520401001
Remove unnecessary page_mapcount_reset() function call;unnecessary;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Similar;0.5586385726928711
Remove unnecessary page_mapcount_reset() function call;unnecessary;to be applied to the file;a file by file comparison of the scanner;Similar;0.5321320295333862
Remove unnecessary page_mapcount_reset() function call;unnecessary;For non */uapi/* files;that summary was;Similar;0.5480102300643921
Remove unnecessary page_mapcount_reset() function call;unnecessary;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.6648821830749512
Remove unnecessary page_mapcount_reset() function call;unnecessary;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.7498457431793213
Remove unnecessary page_mapcount_reset() function call;unnecessary;SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Similar;0.5743066072463989
Remove unnecessary page_mapcount_reset() function call;unnecessary;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.7662349939346313
Remove unnecessary page_mapcount_reset() function call;unnecessary;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.5714530348777771
Remove unnecessary page_mapcount_reset() function call;unnecessary;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.7211096286773682
Remove unnecessary page_mapcount_reset() function call;unnecessary;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.6640415191650391
Remove unnecessary page_mapcount_reset() function call;unnecessary;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5980552434921265
Remove unnecessary page_mapcount_reset() function call;unnecessary;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5961676239967346
Remove unnecessary page_mapcount_reset() function call;unnecessary;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.6096838116645813
Remove unnecessary page_mapcount_reset() function call;unnecessary;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.6141836643218994
Remove unnecessary page_mapcount_reset() function call;unnecessary;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.7052490711212158
Remove unnecessary page_mapcount_reset() function call;unnecessary;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5503100156784058
Remove unnecessary page_mapcount_reset() function call;unnecessary;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.7559296488761902
Remove unnecessary page_mapcount_reset() function call;unnecessary;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.7595599889755249
Remove unnecessary page_mapcount_reset() function call;unnecessary;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5381069183349609
Remove unnecessary page_mapcount_reset() function call;unnecessary;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5560479760169983
Remove unnecessary page_mapcount_reset() function call;unnecessary;function naming changes;requires some;Similar;0.6561761498451233
Remove unnecessary page_mapcount_reset() function call;unnecessary;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.749660074710846
Remove unnecessary page_mapcount_reset() function call;unnecessary;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.8120056390762329
Remove unnecessary page_mapcount_reset() function call;unnecessary;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5019422173500061
Remove unnecessary page_mapcount_reset() function call;unnecessary;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.6680924892425537
Remove unnecessary page_mapcount_reset() function call;unnecessary;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.6135553121566772
Remove unnecessary page_mapcount_reset() function call;unnecessary;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.6686257123947144
Remove unnecessary page_mapcount_reset() function call;unnecessary;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.7206623554229736
Remove unnecessary page_mapcount_reset() function call;unnecessary;can be done in __kmem_cache_shutdown;What is done there;Similar;0.7657841444015503
Remove unnecessary page_mapcount_reset() function call;unnecessary;Fix early boot kernel crash;Slob;Similar;0.5757113695144653
Remove unnecessary page_mapcount_reset() function call;unnecessary;Remove various small accessors;various small;Similar;0.5968242883682251
Remove unnecessary page_mapcount_reset() function call;unnecessary;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.5187550783157349
Remove unnecessary page_mapcount_reset() function call;unnecessary;cleans up some bitrot in slob.c;Also;Similar;0.6030118465423584
Remove unnecessary page_mapcount_reset() function call;unnecessary;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5981763601303101
Remove unnecessary page_mapcount_reset() function call;unnecessary;Fix gfp flags passed to lockdep;lockdep;Similar;0.6750757098197937
Remove unnecessary page_mapcount_reset() function call;unnecessary;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6104239225387573
Remove unnecessary page_mapcount_reset() function call;unnecessary;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.6368119120597839
Remove unnecessary page_mapcount_reset() function call;unnecessary;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.7089799046516418
Remove unnecessary page_mapcount_reset() function call;unnecessary;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5839022397994995
Remove unnecessary page_mapcount_reset() function call;unnecessary;Remove kmemtrace ftrace plugin;tracing;Similar;0.7975280284881592
Remove unnecessary page_mapcount_reset() function call;unnecessary;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.6536429524421692
Remove unnecessary page_mapcount_reset() function call;unnecessary;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.8113442659378052
Remove unnecessary page_mapcount_reset() function call;unnecessary;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5848220586776733
Remove unnecessary page_mapcount_reset() function call;unnecessary;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.6307417154312134
Remove unnecessary page_mapcount_reset() function call;unnecessary;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.541126012802124
Remove unnecessary page_mapcount_reset() function call;unnecessary;use tracepoints;kmemtrace;Similar;0.5297610759735107
Remove unnecessary page_mapcount_reset() function call;unnecessary;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.7687078714370728
Remove unnecessary page_mapcount_reset() function call;unnecessary;fix typo in mm/slob.c;build fix;Similar;0.6953386068344116
Remove unnecessary page_mapcount_reset() function call;unnecessary;fix lockup in slob_free()  ;lockup;Similar;0.6915779113769531
Remove unnecessary page_mapcount_reset() function call;unnecessary;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6320315003395081
Remove unnecessary page_mapcount_reset() function call;unnecessary;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.556193470954895
Remove unnecessary page_mapcount_reset() function call;unnecessary;Drop it  ;if you want;Similar;0.5331029295921326
Remove unnecessary page_mapcount_reset() function call;unnecessary;fix bogus ksize calculation fix  ;SLOB;Similar;0.6804959774017334
Remove unnecessary page_mapcount_reset() function call;unnecessary;fix bogus ksize calculation;bogus;Similar;0.6817349195480347
Remove unnecessary page_mapcount_reset() function call;unnecessary;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.6329190731048584
Remove unnecessary page_mapcount_reset() function call;unnecessary;Fix to return wrong pointer;Fix;Similar;0.7243492007255554
Remove unnecessary page_mapcount_reset() function call;unnecessary;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5920444130897522
Remove unnecessary page_mapcount_reset() function call;unnecessary;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.6238530874252319
Remove unnecessary page_mapcount_reset() function call;unnecessary;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5644693374633789
Remove unnecessary page_mapcount_reset() function call;unnecessary;fix memory corruption;memory corruption;Similar;0.6311323642730713
Remove unnecessary page_mapcount_reset() function call;unnecessary;remove useless ctor parameter and reorder parameters;useless;Similar;0.716652512550354
Remove unnecessary page_mapcount_reset() function call;unnecessary;Handle that separately in krealloc();separately;Similar;0.6191461682319641
Remove unnecessary page_mapcount_reset() function call;unnecessary;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.557366132736206
Remove unnecessary page_mapcount_reset() function call;unnecessary;reduce list scanning;slob;Similar;0.7572070360183716
Remove unnecessary page_mapcount_reset() function call;unnecessary;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.6003859639167786
Remove unnecessary page_mapcount_reset() function call;unnecessary;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5521325469017029
Remove unnecessary page_mapcount_reset() function call;unnecessary;Cleanup zeroing allocations;Slab allocators;Similar;0.6531805992126465
Remove unnecessary page_mapcount_reset() function call;unnecessary;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5530999302864075
Remove unnecessary page_mapcount_reset() function call;unnecessary;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6764677166938782
Remove unnecessary page_mapcount_reset() function call;unnecessary;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5508716106414795
Remove unnecessary page_mapcount_reset() function call;unnecessary;remove bigblock tracking;slob;Similar;0.7277389764785767
Remove unnecessary page_mapcount_reset() function call;unnecessary;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5318484902381897
Remove unnecessary page_mapcount_reset() function call;unnecessary;rework freelist handling;slob;Similar;0.5623037815093994
Remove unnecessary page_mapcount_reset() function call;unnecessary;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5724453330039978
Remove unnecessary page_mapcount_reset() function call;unnecessary;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6310327053070068
Remove unnecessary page_mapcount_reset() function call;unnecessary;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6630065441131592
Remove unnecessary page_mapcount_reset() function call;unnecessary;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.6255343556404114
Remove unnecessary page_mapcount_reset() function call;unnecessary;SLOB to be used on SMP  ;allows;Similar;0.5271353125572205
Remove unnecessary page_mapcount_reset() function call;unnecessary;fix page order calculation on not 4KB page;-;Similar;0.7066242098808289
Remove unnecessary page_mapcount_reset() function call;unnecessary;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.7296043634414673
Remove unnecessary page_mapcount_reset() function call;unnecessary;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.8182327747344971
Remove unnecessary page_mapcount_reset() function call;unnecessary;handle SLAB_PANIC flag;slob;Similar;0.657365620136261
Remove unnecessary page_mapcount_reset() function call;unnecessary;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5906442999839783
Remove unnecessary page_mapcount_reset() function call;unnecessary;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5753227472305298
Remove unnecessary page_mapcount_reset() function call;unnecessary;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.637139618396759
Remove unnecessary page_mapcount_reset() function call;unnecessary;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6705414056777954
Remove unnecessary page_mapcount_reset() function call;unnecessary;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5373952388763428
Remove unnecessary page_mapcount_reset() function call;unnecessary;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6940927505493164
Remove unnecessary page_mapcount_reset() function call;unnecessary;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6031051874160767
Remove unnecessary page_mapcount_reset() function call;unnecessary;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6890233755111694
Remove unnecessary page_mapcount_reset() function call;unnecessary;simplifies SLOB;at this point slob may be broken;Similar;0.5096560716629028
Remove unnecessary page_mapcount_reset() function call;unnecessary;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.616602897644043
Remove unnecessary page_mapcount_reset() function call;unnecessary;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7278150916099548
Remove unnecessary page_mapcount_reset() function call;unnecessary;fix;SLOB=y && SMP=y fix;Similar;0.5150957107543945
Remove unnecessary page_mapcount_reset() function call;unnecessary;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5847469568252563
Use unused field for units;instead;use struct folio instead of struct page;Where non-slab page can appear;Similar;0.5429322719573975
Use unused field for units;instead;dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Similar;0.5095234513282776
Use unused field for units;instead;By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Similar;0.5154058337211609
Use unused field for units;instead;mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Similar;0.5356309413909912
Use unused field for units;instead;kmem_cache_free;We can use it to understand what the RCU core is going to free;Similar;0.5351223945617676
Use unused field for units;instead;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.5307315587997437
Use unused field for units;instead;"SLAB doesnt actually use page allocator directly
";no change there;Similar;0.5044444799423218
Use unused field for units;instead;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.5086875557899475
Use unused field for units;instead;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.5566389560699463
Use unused field for units;instead;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.6004942655563354
Use unused field for units;instead;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.5472625494003296
Use unused field for units;instead;For non */uapi/* files;that summary was;Similar;0.6285617351531982
Use unused field for units;instead;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.6619081497192383
Use unused field for units;instead;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5560058355331421
Use unused field for units;instead;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.6045678853988647
Use unused field for units;instead;hiding potentially buggy callers;except temporarily;Similar;0.5534758567810059
Use unused field for units;instead;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Similar;0.6676284074783325
Use unused field for units;instead;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.5310698747634888
Use unused field for units;instead;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.5010632276535034
Use unused field for units;instead;unioned together;Conveniently;Similar;0.500849723815918
Use unused field for units;instead;function naming changes;requires some;Similar;0.5441492795944214
Use unused field for units;instead;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5437070727348328
Use unused field for units;instead;This affects RCU handling;somewhat;Similar;0.5264946818351746
Use unused field for units;instead;Remove various small accessors;various small;Similar;0.6609379053115845
Use unused field for units;instead;They are no longer needed;They have become so simple;Similar;0.6145323514938354
Use unused field for units;instead;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.5590553879737854
Use unused field for units;instead;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5052229166030884
Use unused field for units;instead;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5021783709526062
Use unused field for units;instead;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5883102416992188
Use unused field for units;instead;Remove kmemtrace ftrace plugin;tracing;Similar;0.6030455827713013
Use unused field for units;instead;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.6631819605827332
Use unused field for units;instead;use tracepoints;kmemtrace;Similar;0.5989857316017151
Use unused field for units;instead;Drop it  ;if you want;Similar;0.6808886528015137
Use unused field for units;instead;fix bogus ksize calculation fix  ;SLOB;Similar;0.5614199638366699
Use unused field for units;instead;fix bogus ksize calculation;bogus;Similar;0.5804538726806641
Use unused field for units;instead;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.589783787727356
Use unused field for units;instead;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5557875633239746
Use unused field for units;instead;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.507461667060852
Use unused field for units;instead;remove useless ctor parameter and reorder parameters;useless;Similar;0.5922541618347168
Use unused field for units;instead;Handle that separately in krealloc();separately;Similar;0.682750940322876
Use unused field for units;instead;reduce list scanning;slob;Similar;0.6648681163787842
Use unused field for units;instead;Cleanup zeroing allocations;Slab allocators;Similar;0.6446298956871033
Use unused field for units;instead;remove bigblock tracking;slob;Similar;0.5460172891616821
Use unused field for units;instead;rework freelist handling;slob;Similar;0.56500244140625
Use unused field for units;instead;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6297687292098999
Use unused field for units;instead;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5109936594963074
Use unused field for units;instead;SLOB to be used on SMP  ;allows;Similar;0.5373769998550415
Use unused field for units;instead;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5579404234886169
Use unused field for units;instead;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.5328594446182251
Use unused field for units;instead;simplifies SLOB;at this point slob may be broken;Similar;0.5599633455276489
Use unused field for units;instead;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5311902761459351
Use unused field for units;instead;fix;SLOB=y && SMP=y fix;Similar;0.5919474959373474
use struct folio instead of struct page;Where non-slab page can appear;dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Similar;0.6867844462394714
use struct folio instead of struct page;Where non-slab page can appear;By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Similar;0.8281378746032715
use struct folio instead of struct page;Where non-slab page can appear;Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Similar;0.594693660736084
use struct folio instead of struct page;Where non-slab page can appear;mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Similar;0.5832154750823975
use struct folio instead of struct page;Where non-slab page can appear;Add mem_dump_obj() to print source of memory block;to print source of memory block;Similar;0.6466687917709351
use struct folio instead of struct page;Where non-slab page can appear;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.5655828714370728
use struct folio instead of struct page;Where non-slab page can appear;The information printed can depend on kernel configuration;depend on kernel configuration;Similar;0.5731974840164185
use struct folio instead of struct page;Where non-slab page can appear;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.6366353631019592
use struct folio instead of struct page;Where non-slab page can appear;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.6525053977966309
use struct folio instead of struct page;Where non-slab page can appear;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.5584399700164795
use struct folio instead of struct page;Where non-slab page can appear;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.6117860674858093
use struct folio instead of struct page;Where non-slab page can appear;"SLAB doesnt actually use page allocator directly
";no change there;Similar;0.6089866757392883
use struct folio instead of struct page;Where non-slab page can appear;to find out the size of a potentially huge page;Its unnecessarily hard;Similar;0.5070595741271973
use struct folio instead of struct page;Where non-slab page can appear;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.5837981700897217
use struct folio instead of struct page;Where non-slab page can appear;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.7715612649917603
use struct folio instead of struct page;Where non-slab page can appear;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.6954492330551147
use struct folio instead of struct page;Where non-slab page can appear;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.6954492330551147
use struct folio instead of struct page;Where non-slab page can appear;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.5780870914459229
use struct folio instead of struct page;Where non-slab page can appear;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.5815213918685913
use struct folio instead of struct page;Where non-slab page can appear;to be applied to the file;a file by file comparison of the scanner;Similar;0.6110074520111084
use struct folio instead of struct page;Where non-slab page can appear;For non */uapi/* files;that summary was;Similar;0.6031081676483154
use struct folio instead of struct page;Where non-slab page can appear;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.6204253435134888
use struct folio instead of struct page;Where non-slab page can appear;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.6769329905509949
use struct folio instead of struct page;Where non-slab page can appear;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.5911204814910889
use struct folio instead of struct page;Where non-slab page can appear;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.6405920386314392
use struct folio instead of struct page;Where non-slab page can appear;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.665458083152771
use struct folio instead of struct page;Where non-slab page can appear;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5276498794555664
use struct folio instead of struct page;Where non-slab page can appear;resurrects approach first proposed in [1];To fix this issue;Similar;0.5051938891410828
use struct folio instead of struct page;Where non-slab page can appear;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5109749436378479
use struct folio instead of struct page;Where non-slab page can appear;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.5704125761985779
use struct folio instead of struct page;Where non-slab page can appear;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.5767168998718262
use struct folio instead of struct page;Where non-slab page can appear;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5470626950263977
use struct folio instead of struct page;Where non-slab page can appear;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.6511553525924683
use struct folio instead of struct page;Where non-slab page can appear;function naming changes;requires some;Similar;0.6299944519996643
use struct folio instead of struct page;Where non-slab page can appear;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.6714866161346436
use struct folio instead of struct page;Where non-slab page can appear;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.7002161145210266
use struct folio instead of struct page;Where non-slab page can appear;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5668395757675171
use struct folio instead of struct page;Where non-slab page can appear;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.7133013606071472
use struct folio instead of struct page;Where non-slab page can appear;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5511504411697388
use struct folio instead of struct page;Where non-slab page can appear;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6046611070632935
use struct folio instead of struct page;Where non-slab page can appear;Remove various small accessors;various small;Similar;0.5521254539489746
use struct folio instead of struct page;Where non-slab page can appear;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6039063930511475
use struct folio instead of struct page;Where non-slab page can appear;Fix gfp flags passed to lockdep;lockdep;Similar;0.5867031812667847
use struct folio instead of struct page;Where non-slab page can appear;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6107467412948608
use struct folio instead of struct page;Where non-slab page can appear;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5941674709320068
use struct folio instead of struct page;Where non-slab page can appear;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6237462759017944
use struct folio instead of struct page;Where non-slab page can appear;Remove kmemtrace ftrace plugin;tracing;Similar;0.5759395360946655
use struct folio instead of struct page;Where non-slab page can appear;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5539577007293701
use struct folio instead of struct page;Where non-slab page can appear;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5008278489112854
use struct folio instead of struct page;Where non-slab page can appear;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5461467504501343
use struct folio instead of struct page;Where non-slab page can appear;use tracepoints;kmemtrace;Similar;0.5327362418174744
use struct folio instead of struct page;Where non-slab page can appear;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5516557693481445
use struct folio instead of struct page;Where non-slab page can appear;fix typo in mm/slob.c;build fix;Similar;0.6465418338775635
use struct folio instead of struct page;Where non-slab page can appear;fix lockup in slob_free()  ;lockup;Similar;0.537680447101593
use struct folio instead of struct page;Where non-slab page can appear;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5215530395507812
use struct folio instead of struct page;Where non-slab page can appear;Drop it  ;if you want;Similar;0.5471276044845581
use struct folio instead of struct page;Where non-slab page can appear;fix bogus ksize calculation fix  ;SLOB;Similar;0.5301342010498047
use struct folio instead of struct page;Where non-slab page can appear;fix bogus ksize calculation;bogus;Similar;0.5434712171554565
use struct folio instead of struct page;Where non-slab page can appear;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5137042999267578
use struct folio instead of struct page;Where non-slab page can appear;Fix to return wrong pointer;Fix;Similar;0.5228103399276733
use struct folio instead of struct page;Where non-slab page can appear;reduce external fragmentation by using three free lists;slob;Similar;0.5261789560317993
use struct folio instead of struct page;Where non-slab page can appear;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5439237952232361
use struct folio instead of struct page;Where non-slab page can appear;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5344514846801758
use struct folio instead of struct page;Where non-slab page can appear;remove useless ctor parameter and reorder parameters;useless;Similar;0.529938817024231
use struct folio instead of struct page;Where non-slab page can appear;Handle that separately in krealloc();separately;Similar;0.6775449514389038
use struct folio instead of struct page;Where non-slab page can appear;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5318824052810669
use struct folio instead of struct page;Where non-slab page can appear;reduce list scanning;slob;Similar;0.6906823515892029
use struct folio instead of struct page;Where non-slab page can appear;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.6133351922035217
use struct folio instead of struct page;Where non-slab page can appear;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5013525485992432
use struct folio instead of struct page;Where non-slab page can appear;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.6121811866760254
use struct folio instead of struct page;Where non-slab page can appear;remove bigblock tracking;slob;Similar;0.576685905456543
use struct folio instead of struct page;Where non-slab page can appear;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.6569029688835144
use struct folio instead of struct page;Where non-slab page can appear;rework freelist handling;slob;Similar;0.5892064571380615
use struct folio instead of struct page;Where non-slab page can appear;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5668046474456787
use struct folio instead of struct page;Where non-slab page can appear;"align them to word size
";it is best in practice;Similar;0.6340492367744446
use struct folio instead of struct page;Where non-slab page can appear;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5909303426742554
use struct folio instead of struct page;Where non-slab page can appear;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5445461869239807
use struct folio instead of struct page;Where non-slab page can appear;SLOB to be used on SMP  ;allows;Similar;0.6009418964385986
use struct folio instead of struct page;Where non-slab page can appear;fix page order calculation on not 4KB page;-;Similar;0.7402642965316772
use struct folio instead of struct page;Where non-slab page can appear;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6356565952301025
use struct folio instead of struct page;Where non-slab page can appear;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6171423196792603
use struct folio instead of struct page;Where non-slab page can appear;handle SLAB_PANIC flag;slob;Similar;0.5404841303825378
use struct folio instead of struct page;Where non-slab page can appear;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.548835277557373
use struct folio instead of struct page;Where non-slab page can appear;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5641860365867615
use struct folio instead of struct page;Where non-slab page can appear;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5000635981559753
use struct folio instead of struct page;Where non-slab page can appear;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5790127515792847
use struct folio instead of struct page;Where non-slab page can appear;Redo a lot of comments;Also;Similar;0.5122313499450684
use struct folio instead of struct page;Where non-slab page can appear;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5390525460243225
use struct folio instead of struct page;Where non-slab page can appear;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5596809387207031
use struct folio instead of struct page;Where non-slab page can appear;simplifies SLOB;at this point slob may be broken;Similar;0.5143136978149414
use struct folio instead of struct page;Where non-slab page can appear;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5859247446060181
use struct folio instead of struct page;Where non-slab page can appear;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5921982526779175
use struct folio instead of struct page;Where non-slab page can appear;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6609982848167419
use struct folio instead of struct page;Where non-slab page can appear;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5151180028915405
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Similar;0.6684018969535828
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Similar;0.8277384042739868
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Similar;0.5979568958282471
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";kmem_cache_free;We can use it to understand what the RCU core is going to free;Similar;0.5534290075302124
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Similar;0.6058961153030396
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.6102157235145569
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";"SLAB doesnt actually use page allocator directly
";no change there;Similar;0.64646315574646
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.6713414192199707
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.6521058678627014
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Remove the unnecessary NULL pointer check;unnecessary;Similar;0.5741233825683594
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Similar;0.5532256364822388
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";All documentation files were explicitly excluded;explicitly excluded;Similar;0.595368504524231
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";For non */uapi/* files;that summary was;Similar;0.7251399755477905
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.5647467374801636
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5046294331550598
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.5473531484603882
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.6497511863708496
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Similar;0.5780676603317261
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.7214657664299011
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5611203908920288
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.6604769229888916
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";can be done in __kmem_cache_shutdown;What is done there;Similar;0.5382673740386963
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Remove various small accessors;various small;Similar;0.5078405141830444
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.6315010786056519
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.6953808069229126
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Remove kmemtrace ftrace plugin;tracing;Similar;0.5964651107788086
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.6005698442459106
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";fix typo in mm/slob.c;build fix;Similar;0.5022435784339905
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Drop it  ;if you want;Similar;0.5793331861495972
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.6210014820098877
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5116918087005615
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";remove useless ctor parameter and reorder parameters;useless;Similar;0.6817448139190674
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Handle that separately in krealloc();separately;Similar;0.6506889462471008
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";reduce list scanning;slob;Similar;0.6519663333892822
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Cleanup zeroing allocations;Slab allocators;Similar;0.5481374263763428
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.53741455078125
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";remove bigblock tracking;slob;Similar;0.5221447944641113
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.7791882753372192
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";fix page order calculation on not 4KB page;-;Similar;0.7710202932357788
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.630013644695282
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";handle SLAB_PANIC flag;slob;Similar;0.5247560739517212
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6342121362686157
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5889071226119995
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Slob does not need any special definitions;since we introduce a fallback case;Similar;0.6140351891517639
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5158550143241882
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6203519105911255
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5546852350234985
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5059608221054077
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Similar;0.5440039038658142
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Similar;0.5688091516494751
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Similar;0.5486704111099243
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Add mem_dump_obj() to print source of memory block;to print source of memory block;Similar;0.6336796283721924
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.580051064491272
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;The information printed can depend on kernel configuration;depend on kernel configuration;Similar;0.5009410977363586
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Similar;0.6184651851654053
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.5524171590805054
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.5511360168457031
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.5956764221191406
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.5766037106513977
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;"SLAB doesnt actually use page allocator directly
";no change there;Similar;0.640465259552002
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.591576874256134
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.8131656646728516
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.6490476727485657
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.6490476727485657
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.5584948062896729
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.5792443752288818
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;to be applied to the file;a file by file comparison of the scanner;Similar;0.570615291595459
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;For non */uapi/* files;that summary was;Similar;0.5645449161529541
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.5688358545303345
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.6152220964431763
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.623449444770813
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.6687698364257812
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.6420178413391113
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5292817950248718
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.5371376276016235
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.5989328026771545
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5507346391677856
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5843125581741333
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;function naming changes;requires some;Similar;0.5784538984298706
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.6303715705871582
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.6198477745056152
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5827692747116089
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5384656190872192
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.6234326362609863
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5045730471611023
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5266308188438416
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Remove various small accessors;various small;Similar;0.5247014164924622
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5100944638252258
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.544349193572998
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5804569721221924
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Remove kmemtrace ftrace plugin;tracing;Similar;0.5350737571716309
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5249553322792053
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;use tracepoints;kmemtrace;Similar;0.5143246650695801
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5043346881866455
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;fix typo in mm/slob.c;build fix;Similar;0.5748992562294006
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5788851380348206
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Drop it  ;if you want;Similar;0.5232610702514648
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5388727188110352
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;remove useless ctor parameter and reorder parameters;useless;Similar;0.5127618908882141
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Handle that separately in krealloc();separately;Similar;0.5596930980682373
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;reduce list scanning;slob;Similar;0.6274751424789429
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.543932318687439
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Cleanup zeroing allocations;Slab allocators;Similar;0.5040050745010376
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5965164303779602
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;remove bigblock tracking;slob;Similar;0.5702317953109741
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5515168905258179
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;rework freelist handling;slob;Similar;0.5007538199424744
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5500643849372864
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;"align them to word size
";it is best in practice;Similar;0.563876748085022
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5544165372848511
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;SLOB to be used on SMP  ;allows;Similar;0.590174674987793
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;fix page order calculation on not 4KB page;-;Similar;0.6665797233581543
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5785727500915527
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6128756999969482
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;handle SLAB_PANIC flag;slob;Similar;0.5494879484176636
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5369998216629028
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6375240683555603
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5457050800323486
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5547616481781006
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6268949508666992
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;simplifies SLOB;at this point slob may be broken;Similar;0.5111666917800903
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5627897381782532
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.655758261680603
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Similar;0.6144428253173828
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;kmem_cache_free;We can use it to understand what the RCU core is going to free;Similar;0.5802767276763916
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Add mem_dump_obj() to print source of memory block;to print source of memory block;Similar;0.5487417578697205
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Similar;0.575487494468689
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.5489680767059326
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.5448712110519409
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.617285966873169
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;"SLAB doesnt actually use page allocator directly
";no change there;Similar;0.5616416931152344
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.6613566875457764
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.5085064172744751
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.5085064172744751
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.6736389398574829
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.6398003697395325
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Similar;0.6177057027816772
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;All documentation files were explicitly excluded;explicitly excluded;Similar;0.5366336107254028
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;For non */uapi/* files;that summary was;Similar;0.6906110048294067
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.5409371256828308
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5444436073303223
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Similar;0.5384299159049988
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.6246964931488037
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.6326484084129333
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5190081000328064
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5040223002433777
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Similar;0.5523011684417725
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.7015863060951233
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.6414995193481445
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.6602506041526794
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5229190587997437
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5091447830200195
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5139473080635071
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.527259886264801
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5959445834159851
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.5812918543815613
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5280992984771729
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.711050271987915
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Remove kmemtrace ftrace plugin;tracing;Similar;0.653346836566925
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.577135443687439
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5993525981903076
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.541469395160675
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.5238507986068726
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5081227421760559
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.553399920463562
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;fix typo in mm/slob.c;build fix;Similar;0.540438175201416
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;fix lockup in slob_free()  ;lockup;Similar;0.5442805886268616
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5206493735313416
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Drop it  ;if you want;Similar;0.5275906920433044
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;fix bogus ksize calculation fix  ;SLOB;Similar;0.5369374752044678
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;fix bogus ksize calculation;bogus;Similar;0.5406486988067627
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5883901119232178
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5857547521591187
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;remove useless ctor parameter and reorder parameters;useless;Similar;0.6815977096557617
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Handle that separately in krealloc();separately;Similar;0.6314880847930908
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.514134407043457
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;reduce list scanning;slob;Similar;0.5912894010543823
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Cleanup zeroing allocations;Slab allocators;Similar;0.5657005906105042
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5138849020004272
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6353986263275146
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;remove bigblock tracking;slob;Similar;0.5390230417251587
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.7859430313110352
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5624222755432129
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;fix page order calculation on not 4KB page;-;Similar;0.7304446697235107
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5065924525260925
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6314363479614258
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;handle SLAB_PANIC flag;slob;Similar;0.5809987783432007
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6476122736930847
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6207447052001953
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.5634478330612183
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5448057651519775
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.7550264596939087
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6725025177001953
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Similar;0.5026918053627014
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;kmem_cache_free;We can use it to understand what the RCU core is going to free;Similar;0.5183326005935669
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Add mem_dump_obj() to print source of memory block;to print source of memory block;Similar;0.5526158809661865
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.6386033892631531
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Similar;0.5929677486419678
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.558819591999054
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.6284334659576416
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.5043423771858215
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.8202522993087769
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Similar;0.5031487345695496
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;Similar;0.5791143774986267
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.5960149765014648
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.5586742162704468
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.5586742162704468
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.7678171992301941
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.7464447021484375
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Similar;0.5229030251502991
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;All documentation files were explicitly excluded;explicitly excluded;Similar;0.5426372289657593
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;For non */uapi/* files;that summary was;Similar;0.5989855527877808
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.7783631682395935
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.684233546257019
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Similar;0.641372561454773
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.6865208745002747
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.5743881464004517
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.7327883243560791
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5499327182769775
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;We always end up returning some objects at the cost of another cmpxchg  ;Else;Similar;0.5360109210014343
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;hiding potentially buggy callers;except temporarily;Similar;0.5301044583320618
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5949962139129639
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Make dead caches discard free slabs immediately;slub;Similar;0.5820246934890747
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.6552164554595947
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Similar;0.532715916633606
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5920864343643188
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.7570770978927612
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5089215636253357
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.6425259113311768
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.7748516798019409
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;function naming changes;requires some;Similar;0.5819226503372192
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.5004386305809021
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5372679233551025
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5610688924789429
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;Similar;0.5160074234008789
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.6468349099159241
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5483918786048889
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;can be done in __kmem_cache_shutdown;What is done there;Similar;0.799201250076294
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;This affects RCU handling;somewhat;Similar;0.5403883457183838
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Remove various small accessors;various small;Similar;0.7439603805541992
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;They are no longer needed;They have become so simple;Similar;0.6892983317375183
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.6474775075912476
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Fix gfp flags passed to lockdep;lockdep;Similar;0.5566854476928711
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5153331160545349
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6158841848373413
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5943345427513123
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Remove kmemtrace ftrace plugin;tracing;Similar;0.7570401430130005
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.7812281250953674
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6840578317642212
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.6738417744636536
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;refactor code for future changes;Impact;Similar;0.5328459739685059
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;use tracepoints;kmemtrace;Similar;0.5030831098556519
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6020738482475281
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;fix typo in mm/slob.c;build fix;Similar;0.606089174747467
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;fix lockup in slob_free()  ;lockup;Similar;0.612078070640564
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Drop it  ;if you want;Similar;0.7074331045150757
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;fix bogus ksize calculation fix  ;SLOB;Similar;0.6057609915733337
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;fix bogus ksize calculation;bogus;Similar;0.6341883540153503
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.7774733304977417
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5232225060462952
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Fix to return wrong pointer;Fix;Similar;0.634727954864502
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;reduce external fragmentation by using three free lists;slob;Similar;0.5069599151611328
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.6088870763778687
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.6701899766921997
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;fix memory corruption;memory corruption;Similar;0.5780113935470581
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;remove useless ctor parameter and reorder parameters;useless;Similar;0.7792649269104004
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Handle that separately in krealloc();separately;Similar;0.5982683897018433
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5581476092338562
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;reduce list scanning;slob;Similar;0.8099866509437561
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5801151990890503
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Cleanup zeroing allocations;Slab allocators;Similar;0.7473866939544678
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5336312651634216
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6871528029441833
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;remove bigblock tracking;slob;Similar;0.7150959968566895
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;rework freelist handling;slob;Similar;0.5294241309165955
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6063270568847656
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5356690287590027
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6450344324111938
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.6155887842178345
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;SLOB to be used on SMP  ;allows;Similar;0.5691132545471191
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;fix page order calculation on not 4KB page;-;Similar;0.6140625476837158
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.614704966545105
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.7408998012542725
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;handle SLAB_PANIC flag;slob;Similar;0.6315222978591919
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.532528281211853
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5553227663040161
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6654032468795776
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5245410799980164
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.7049516439437866
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Redo a lot of comments;Also;Similar;0.5269838571548462
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5849112272262573
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5859577655792236
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;simplifies SLOB;at this point slob may be broken;Similar;0.5323647856712341
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6977389454841614
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;fix;SLOB=y && SMP=y fix;Similar;0.5317451357841492
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5078452229499817
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;kmem_cache_free;We can use it to understand what the RCU core is going to free;Similar;0.6963375806808472
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Add mem_dump_obj() to print source of memory block;to print source of memory block;Similar;0.5973494648933411
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Similar;0.5394837856292725
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.6108911037445068
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Similar;0.5983448624610901
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Similar;0.5160762071609497
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.5534330606460571
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.6463555097579956
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Similar;0.5768409967422485
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.5915147066116333
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;patch ensures alignment on all arches and cache sizes;Still;Similar;0.5277882814407349
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Similar;0.5733276605606079
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Similar;0.6222772598266602
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;configure and build (select SLOB allocator);SLOB allocator;Similar;0.560840904712677
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.604118287563324
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.6012617349624634
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.6012617349624634
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.5025418996810913
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.5678004026412964
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Similar;0.6149817705154419
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.5214446783065796
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5784547924995422
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.5554293990135193
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.5279096961021423
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.525231659412384
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5235141515731812
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;allow future extension of the bulk alloc API;is done to;Similar;0.5003266334533691
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.699074387550354
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5639615058898926
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Make dead caches discard free slabs immediately;slub;Similar;0.5788756608963013
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5948936343193054
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5574735403060913
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.6428554058074951
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.549439013004303
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5971416234970093
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5884057283401489
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.6140931248664856
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.7492614984512329
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5976527333259583
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.6212208271026611
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5900965332984924
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.6729249954223633
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5707765817642212
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5620908737182617
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.6164924502372742
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6022571325302124
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Fix early boot kernel crash;Slob;Similar;0.5480345487594604
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5133652687072754
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;cleans up some bitrot in slob.c;Also;Similar;0.5761346817016602
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5615012049674988
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Fix gfp flags passed to lockdep;lockdep;Similar;0.562219500541687
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5106306076049805
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5688232183456421
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5687406063079834
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Remove kmemtrace ftrace plugin;tracing;Similar;0.5669021606445312
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6083884239196777
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5890459418296814
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5572627186775208
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6534832715988159
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;fix typo in mm/slob.c;build fix;Similar;0.549217164516449
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;fix lockup in slob_free()  ;lockup;Similar;0.801626980304718
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6149126291275024
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.6784118413925171
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.695119321346283
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.6658048629760742
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;reduce external fragmentation by using three free lists;slob;Similar;0.5578688383102417
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5524591207504272
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5026025772094727
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5058496594429016
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5910748243331909
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.6650002598762512
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5353173017501831
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;rework freelist handling;slob;Similar;0.6445168852806091
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5724058151245117
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5437806248664856
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;fix page order calculation on not 4KB page;-;Similar;0.5391007661819458
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6484275460243225
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5896972417831421
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6431660652160645
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5677507519721985
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6298695802688599
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.6975618600845337
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6413705348968506
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6479743123054504
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6215790510177612
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6707847118377686
kmem_cache_free;We can use it to understand what the RCU core is going to free;this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;Similar;0.5762686729431152
kmem_cache_free;We can use it to understand what the RCU core is going to free;the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Similar;0.5074769258499146
kmem_cache_free;We can use it to understand what the RCU core is going to free;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.5532640218734741
kmem_cache_free;We can use it to understand what the RCU core is going to free;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.5124902129173279
kmem_cache_free;We can use it to understand what the RCU core is going to free;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.6113036870956421
kmem_cache_free;We can use it to understand what the RCU core is going to free;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.5529114007949829
kmem_cache_free;We can use it to understand what the RCU core is going to free;For non */uapi/* files;that summary was;Similar;0.6476237773895264
kmem_cache_free;We can use it to understand what the RCU core is going to free;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.5517160296440125
kmem_cache_free;We can use it to understand what the RCU core is going to free;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.5310978293418884
kmem_cache_free;We can use it to understand what the RCU core is going to free;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.6077333092689514
kmem_cache_free;We can use it to understand what the RCU core is going to free;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5566015243530273
kmem_cache_free;We can use it to understand what the RCU core is going to free;Make dead caches discard free slabs immediately;slub;Similar;0.5192052721977234
kmem_cache_free;We can use it to understand what the RCU core is going to free;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Similar;0.547078013420105
kmem_cache_free;We can use it to understand what the RCU core is going to free;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.5801167488098145
kmem_cache_free;We can use it to understand what the RCU core is going to free;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5567034482955933
kmem_cache_free;We can use it to understand what the RCU core is going to free;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5038027167320251
kmem_cache_free;We can use it to understand what the RCU core is going to free;using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";Similar;0.6025476455688477
kmem_cache_free;We can use it to understand what the RCU core is going to free;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5001081228256226
kmem_cache_free;We can use it to understand what the RCU core is going to free;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Similar;0.51853346824646
kmem_cache_free;We can use it to understand what the RCU core is going to free;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6254997253417969
kmem_cache_free;We can use it to understand what the RCU core is going to free;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.5498075485229492
kmem_cache_free;We can use it to understand what the RCU core is going to free;Remove various small accessors;various small;Similar;0.5336616039276123
kmem_cache_free;We can use it to understand what the RCU core is going to free;They are no longer needed;They have become so simple;Similar;0.5623518228530884
kmem_cache_free;We can use it to understand what the RCU core is going to free;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.6291106343269348
kmem_cache_free;We can use it to understand what the RCU core is going to free;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5154959559440613
kmem_cache_free;We can use it to understand what the RCU core is going to free;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.6295890808105469
kmem_cache_free;We can use it to understand what the RCU core is going to free;Remove kmemtrace ftrace plugin;tracing;Similar;0.5387973785400391
kmem_cache_free;We can use it to understand what the RCU core is going to free;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5896332859992981
kmem_cache_free;We can use it to understand what the RCU core is going to free;use tracepoints;kmemtrace;Similar;0.5161507725715637
kmem_cache_free;We can use it to understand what the RCU core is going to free;fix lockup in slob_free()  ;lockup;Similar;0.7634607553482056
kmem_cache_free;We can use it to understand what the RCU core is going to free;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5570195913314819
kmem_cache_free;We can use it to understand what the RCU core is going to free;Drop it  ;if you want;Similar;0.5976382493972778
kmem_cache_free;We can use it to understand what the RCU core is going to free;fix bogus ksize calculation fix  ;SLOB;Similar;0.5021330118179321
kmem_cache_free;We can use it to understand what the RCU core is going to free;fix bogus ksize calculation;bogus;Similar;0.518723726272583
kmem_cache_free;We can use it to understand what the RCU core is going to free;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5672117471694946
kmem_cache_free;We can use it to understand what the RCU core is going to free;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.6257871389389038
kmem_cache_free;We can use it to understand what the RCU core is going to free;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.6120216846466064
kmem_cache_free;We can use it to understand what the RCU core is going to free;reduce external fragmentation by using three free lists;slob;Similar;0.5233774185180664
kmem_cache_free;We can use it to understand what the RCU core is going to free;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5032829642295837
kmem_cache_free;We can use it to understand what the RCU core is going to free;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.6246088743209839
kmem_cache_free;We can use it to understand what the RCU core is going to free;remove useless ctor parameter and reorder parameters;useless;Similar;0.545256495475769
kmem_cache_free;We can use it to understand what the RCU core is going to free;Handle that separately in krealloc();separately;Similar;0.5539859533309937
kmem_cache_free;We can use it to understand what the RCU core is going to free;reduce list scanning;slob;Similar;0.5504146218299866
kmem_cache_free;We can use it to understand what the RCU core is going to free;Cleanup zeroing allocations;Slab allocators;Similar;0.6006625890731812
kmem_cache_free;We can use it to understand what the RCU core is going to free;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5150348544120789
kmem_cache_free;We can use it to understand what the RCU core is going to free;rework freelist handling;slob;Similar;0.6590595245361328
kmem_cache_free;We can use it to understand what the RCU core is going to free;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6008721590042114
kmem_cache_free;We can use it to understand what the RCU core is going to free;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.523583710193634
kmem_cache_free;We can use it to understand what the RCU core is going to free;fix page order calculation on not 4KB page;-;Similar;0.5563446283340454
kmem_cache_free;We can use it to understand what the RCU core is going to free;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5653397440910339
kmem_cache_free;We can use it to understand what the RCU core is going to free;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5864990949630737
kmem_cache_free;We can use it to understand what the RCU core is going to free;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5129930973052979
kmem_cache_free;We can use it to understand what the RCU core is going to free;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.659237802028656
kmem_cache_free;We can use it to understand what the RCU core is going to free;simplifies SLOB;at this point slob may be broken;Similar;0.5804731845855713
kmem_cache_free;We can use it to understand what the RCU core is going to free;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6081588268280029
kmem_cache_free;We can use it to understand what the RCU core is going to free;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6624901294708252
Add mem_dump_obj() to print source of memory block;to print source of memory block;There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Similar;0.6382734775543213
Add mem_dump_obj() to print source of memory block;to print source of memory block;"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Similar;0.5537770986557007
Add mem_dump_obj() to print source of memory block;to print source of memory block;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.7766838073730469
Add mem_dump_obj() to print source of memory block;to print source of memory block;The information printed can depend on kernel configuration;depend on kernel configuration;Similar;0.6397975087165833
Add mem_dump_obj() to print source of memory block;to print source of memory block;the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Similar;0.5450652837753296
Add mem_dump_obj() to print source of memory block;to print source of memory block;build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Similar;0.5864274501800537
Add mem_dump_obj() to print source of memory block;to print source of memory block;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.7401120066642761
Add mem_dump_obj() to print source of memory block;to print source of memory block;have some use eventually for annotations in drivers/gpu;I might;Similar;0.563807487487793
Add mem_dump_obj() to print source of memory block;to print source of memory block;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.7086617350578308
Add mem_dump_obj() to print source of memory block;to print source of memory block;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.6147326827049255
Add mem_dump_obj() to print source of memory block;to print source of memory block;I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Similar;0.525124192237854
Add mem_dump_obj() to print source of memory block;to print source of memory block;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.6031070351600647
Add mem_dump_obj() to print source of memory block;to print source of memory block;improve memory accounting;improve;Similar;0.5947228074073792
Add mem_dump_obj() to print source of memory block;to print source of memory block;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.594099760055542
Add mem_dump_obj() to print source of memory block;to print source of memory block;configure and build (select SLOB allocator);SLOB allocator;Similar;0.6677060723304749
Add mem_dump_obj() to print source of memory block;to print source of memory block;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.6338297128677368
Add mem_dump_obj() to print source of memory block;to print source of memory block;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.8057849407196045
Add mem_dump_obj() to print source of memory block;to print source of memory block;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.8057849407196045
Add mem_dump_obj() to print source of memory block;to print source of memory block;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.5567534565925598
Add mem_dump_obj() to print source of memory block;to print source of memory block;to be applied to the file;a file by file comparison of the scanner;Similar;0.6919450163841248
Add mem_dump_obj() to print source of memory block;to print source of memory block;the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Similar;0.5363699197769165
Add mem_dump_obj() to print source of memory block;to print source of memory block;Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Similar;0.6027198433876038
Add mem_dump_obj() to print source of memory block;to print source of memory block;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.5938705205917358
Add mem_dump_obj() to print source of memory block;to print source of memory block;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.6828147768974304
Add mem_dump_obj() to print source of memory block;to print source of memory block;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.695706844329834
Add mem_dump_obj() to print source of memory block;to print source of memory block;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.580198347568512
Add mem_dump_obj() to print source of memory block;to print source of memory block;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.5653636455535889
Add mem_dump_obj() to print source of memory block;to print source of memory block;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.6234575510025024
Add mem_dump_obj() to print source of memory block;to print source of memory block;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.5621482133865356
Add mem_dump_obj() to print source of memory block;to print source of memory block;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5816706418991089
Add mem_dump_obj() to print source of memory block;to print source of memory block;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.521815299987793
Add mem_dump_obj() to print source of memory block;to print source of memory block;resurrects approach first proposed in [1];To fix this issue;Similar;0.5061918497085571
Add mem_dump_obj() to print source of memory block;to print source of memory block;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.6893725991249084
Add mem_dump_obj() to print source of memory block;to print source of memory block;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5960474014282227
Add mem_dump_obj() to print source of memory block;to print source of memory block;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5852174758911133
Add mem_dump_obj() to print source of memory block;to print source of memory block;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5202085971832275
Add mem_dump_obj() to print source of memory block;to print source of memory block;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.5667824149131775
Add mem_dump_obj() to print source of memory block;to print source of memory block;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5004588961601257
Add mem_dump_obj() to print source of memory block;to print source of memory block;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.6562633514404297
Add mem_dump_obj() to print source of memory block;to print source of memory block;function naming changes;requires some;Similar;0.6224322319030762
Add mem_dump_obj() to print source of memory block;to print source of memory block;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.7339864373207092
Add mem_dump_obj() to print source of memory block;to print source of memory block;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.8035950660705566
Add mem_dump_obj() to print source of memory block;to print source of memory block;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.6022828221321106
Add mem_dump_obj() to print source of memory block;to print source of memory block;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5229710340499878
Add mem_dump_obj() to print source of memory block;to print source of memory block;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.6884207129478455
Add mem_dump_obj() to print source of memory block;to print source of memory block;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.7355273962020874
Add mem_dump_obj() to print source of memory block;to print source of memory block;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.6510845422744751
Add mem_dump_obj() to print source of memory block;to print source of memory block;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6593011617660522
Add mem_dump_obj() to print source of memory block;to print source of memory block;Fix early boot kernel crash;Slob;Similar;0.6273124814033508
Add mem_dump_obj() to print source of memory block;to print source of memory block;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5447332859039307
Add mem_dump_obj() to print source of memory block;to print source of memory block;cleans up some bitrot in slob.c;Also;Similar;0.6047627329826355
Add mem_dump_obj() to print source of memory block;to print source of memory block;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.7204272150993347
Add mem_dump_obj() to print source of memory block;to print source of memory block;Fix gfp flags passed to lockdep;lockdep;Similar;0.665560245513916
Add mem_dump_obj() to print source of memory block;to print source of memory block;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6932876706123352
Add mem_dump_obj() to print source of memory block;to print source of memory block;Adding this mask;fixes the bug;Similar;0.5048672556877136
Add mem_dump_obj() to print source of memory block;to print source of memory block;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.6785533428192139
Add mem_dump_obj() to print source of memory block;to print source of memory block;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.7144043445587158
Add mem_dump_obj() to print source of memory block;to print source of memory block;Remove kmemtrace ftrace plugin;tracing;Similar;0.6004518866539001
Add mem_dump_obj() to print source of memory block;to print source of memory block;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6733616590499878
Add mem_dump_obj() to print source of memory block;to print source of memory block;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5655384063720703
Add mem_dump_obj() to print source of memory block;to print source of memory block;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.6846857070922852
Add mem_dump_obj() to print source of memory block;to print source of memory block;use tracepoints;kmemtrace;Similar;0.5623038411140442
Add mem_dump_obj() to print source of memory block;to print source of memory block;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.7176769971847534
Add mem_dump_obj() to print source of memory block;to print source of memory block;fix typo in mm/slob.c;build fix;Similar;0.7125203609466553
Add mem_dump_obj() to print source of memory block;to print source of memory block;fix lockup in slob_free()  ;lockup;Similar;0.5703166723251343
Add mem_dump_obj() to print source of memory block;to print source of memory block;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.7061660289764404
Add mem_dump_obj() to print source of memory block;to print source of memory block;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6423105001449585
Add mem_dump_obj() to print source of memory block;to print source of memory block;enable and use this tracer;To enable and use this tracer;Similar;0.5551791191101074
Add mem_dump_obj() to print source of memory block;to print source of memory block;fix bogus ksize calculation fix  ;SLOB;Similar;0.5774857401847839
Add mem_dump_obj() to print source of memory block;to print source of memory block;fix bogus ksize calculation;bogus;Similar;0.5612739324569702
Add mem_dump_obj() to print source of memory block;to print source of memory block;record page flag overlays explicitly;slob;Similar;0.532265841960907
Add mem_dump_obj() to print source of memory block;to print source of memory block;Fix to return wrong pointer;Fix;Similar;0.5720994472503662
Add mem_dump_obj() to print source of memory block;to print source of memory block;fix memory corruption;memory corruption;Similar;0.5804576873779297
Add mem_dump_obj() to print source of memory block;to print source of memory block;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5336827635765076
Add mem_dump_obj() to print source of memory block;to print source of memory block;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5647042989730835
Add mem_dump_obj() to print source of memory block;to print source of memory block;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5802336931228638
Add mem_dump_obj() to print source of memory block;to print source of memory block;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5706640481948853
Add mem_dump_obj() to print source of memory block;to print source of memory block;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6053261756896973
Add mem_dump_obj() to print source of memory block;to print source of memory block;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5760510563850403
Add mem_dump_obj() to print source of memory block;to print source of memory block;remove bigblock tracking;slob;Similar;0.580026388168335
Add mem_dump_obj() to print source of memory block;to print source of memory block;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5748286843299866
Add mem_dump_obj() to print source of memory block;to print source of memory block;rework freelist handling;slob;Similar;0.5163344740867615
Add mem_dump_obj() to print source of memory block;to print source of memory block;"align them to word size
";it is best in practice;Similar;0.5478693246841431
Add mem_dump_obj() to print source of memory block;to print source of memory block;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6610420942306519
Add mem_dump_obj() to print source of memory block;to print source of memory block;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6150370836257935
Add mem_dump_obj() to print source of memory block;to print source of memory block;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5544456243515015
Add mem_dump_obj() to print source of memory block;to print source of memory block;SLOB to be used on SMP  ;allows;Similar;0.5860451459884644
Add mem_dump_obj() to print source of memory block;to print source of memory block;fix page order calculation on not 4KB page;-;Similar;0.5586894750595093
Add mem_dump_obj() to print source of memory block;to print source of memory block;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.603870153427124
Add mem_dump_obj() to print source of memory block;to print source of memory block;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5856212377548218
Add mem_dump_obj() to print source of memory block;to print source of memory block;handle SLAB_PANIC flag;slob;Similar;0.607559084892273
Add mem_dump_obj() to print source of memory block;to print source of memory block;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5524295568466187
Add mem_dump_obj() to print source of memory block;to print source of memory block;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.547694206237793
Add mem_dump_obj() to print source of memory block;to print source of memory block;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.7001829743385315
Add mem_dump_obj() to print source of memory block;to print source of memory block;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.566441535949707
Add mem_dump_obj() to print source of memory block;to print source of memory block;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5748581886291504
Add mem_dump_obj() to print source of memory block;to print source of memory block;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5894073247909546
Add mem_dump_obj() to print source of memory block;to print source of memory block;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6651692390441895
Add mem_dump_obj() to print source of memory block;to print source of memory block;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5815739631652832
Add mem_dump_obj() to print source of memory block;to print source of memory block;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7337374091148376
Add mem_dump_obj() to print source of memory block;to print source of memory block;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6139440536499023
Add mem_dump_obj() to print source of memory block;to print source of memory block;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.564894437789917
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Similar;0.7089349031448364
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.6687618494033813
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;The information printed can depend on kernel configuration;depend on kernel configuration;Similar;0.7560255527496338
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Similar;0.5261535048484802
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.603979229927063
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;have some use eventually for annotations in drivers/gpu;I might;Similar;0.6795414686203003
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.5931591391563416
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.5711413025856018
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Similar;0.5961326360702515
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;patch ensures alignment on all arches and cache sizes;Still;Similar;0.542352557182312
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;improve memory accounting;improve;Similar;0.5722461938858032
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;Similar;0.6124858856201172
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Similar;0.565641462802887
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;configure and build (select SLOB allocator);SLOB allocator;Similar;0.5625967979431152
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.5772697925567627
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.5772697925567627
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Similar;0.5255681276321411
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;to be applied to the file;a file by file comparison of the scanner;Similar;0.5179484486579895
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Similar;0.541488766670227
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Similar;0.6238545179367065
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.600616455078125
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5733229517936707
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5505527257919312
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.620142936706543
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5325257778167725
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5236124396324158
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.627570390701294
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.6402870416641235
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5169808268547058
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6506055593490601
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5045132637023926
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5068827867507935
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;function naming changes;requires some;Similar;0.512079119682312
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5401980876922607
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5421226024627686
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5113784670829773
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5800434947013855
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5791756510734558
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.6340850591659546
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5045679211616516
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Fix early boot kernel crash;Slob;Similar;0.6404725313186646
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.6554558277130127
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;cleans up some bitrot in slob.c;Also;Similar;0.6431124210357666
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6649890542030334
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Fix gfp flags passed to lockdep;lockdep;Similar;0.5800452828407288
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5307981371879578
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.587992787361145
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5374019145965576
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5678086280822754
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.6615813970565796
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5480879545211792
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;fix typo in mm/slob.c;build fix;Similar;0.6178423762321472
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;fix lockup in slob_free()  ;lockup;Similar;0.5212305188179016
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6104520559310913
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6878466606140137
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Fix to return wrong pointer;Fix;Similar;0.5608464479446411
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5467065572738647
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;fix memory corruption;memory corruption;Similar;0.6890928745269775
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5603842735290527
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5722947120666504
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5778169631958008
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5136644244194031
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6520506143569946
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6328808069229126
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5594837665557861
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5150876045227051
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5695939660072327
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5153505802154541
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6527913808822632
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5423754453659058
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5455641746520996
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5509775876998901
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.6520099639892578
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;resurrects approach first proposed in [1];To fix this issue;Similar;0.5221805572509766
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Similar;0.551912784576416
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.6179060935974121
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;unioned together;Conveniently;Similar;0.5514762997627258
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.6335334181785583
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";Similar;0.6058542728424072
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;Improve trace accuracy;by correctly tracing reported size;Similar;0.5267844200134277
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Similar;0.5658926367759705
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.6711547374725342
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;They are no longer needed;They have become so simple;Similar;0.5362610816955566
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;fix lockup in slob_free()  ;lockup;Similar;0.5079278349876404
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.6529985666275024
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;enable and use this tracer;To enable and use this tracer;Similar;0.5228331089019775
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;I find it more readable  ;personal opinion;Similar;0.5628229379653931
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5082414150238037
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;improved alignment handling;improved;Similar;0.5848017930984497
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;rework freelist handling;slob;Similar;0.6200048923492432
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5140413045883179
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.6217597723007202
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;simplifies SLOB;at this point slob may be broken;Similar;0.5294942259788513
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;fix;SLOB=y && SMP=y fix;Similar;0.5178999900817871
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Similar;0.5302444696426392
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;The information printed can depend on kernel configuration;depend on kernel configuration;Similar;0.6051949262619019
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Similar;0.5685970783233643
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;have some use eventually for annotations in drivers/gpu;I might;Similar;0.6238018870353699
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.5396097302436829
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Similar;0.6275322437286377
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;patch ensures alignment on all arches and cache sizes;Still;Similar;0.5102221369743347
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Similar;0.5121278166770935
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;Similar;0.6021542549133301
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Similar;0.5326778888702393
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;configure and build (select SLOB allocator);SLOB allocator;Similar;0.5111085176467896
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.5036307573318481
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.5036307573318481
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Similar;0.5164005756378174
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Similar;0.5630136132240295
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Similar;0.5652177333831787
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5560335516929626
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.5155379176139832
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5718544125556946
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.689391016960144
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;hiding potentially buggy callers;except temporarily;Similar;0.5318893194198608
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5672948956489563
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5907270908355713
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.6345301866531372
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5744285583496094
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6453732252120972
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5006629228591919
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5333734154701233
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5664259195327759
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5361131429672241
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5580955147743225
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;This will allow us to push more processing into common code later;improve readability;Similar;0.5613971948623657
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Fix early boot kernel crash;Slob;Similar;0.5432731509208679
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5142216682434082
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5228079557418823
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5284996628761292
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5412209033966064
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5583625435829163
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;refactor code for future changes;Impact;Similar;0.5100803375244141
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5117250084877014
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5765297412872314
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Fix to return wrong pointer;Fix;Similar;0.5627671480178833
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5130513906478882
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5217310190200806
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5397483110427856
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5130723714828491
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5428627729415894
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;Similar;0.5803924798965454
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5589020252227783
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5688472986221313
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5334095358848572
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5377967357635498
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5027590990066528
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5117132663726807
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5425450801849365
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5334150791168213
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;The information printed can depend on kernel configuration;depend on kernel configuration;Similar;0.7160589694976807
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Similar;0.5691231489181519
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.7510339021682739
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;have some use eventually for annotations in drivers/gpu;I might;Similar;0.569413423538208
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.7104259133338928
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.6747732162475586
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.5990214347839355
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;improve memory accounting;improve;Similar;0.5612962245941162
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Similar;0.501204252243042
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;Similar;0.5034952163696289
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.6622743606567383
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;configure and build (select SLOB allocator);SLOB allocator;Similar;0.6812954545021057
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.6012839078903198
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.7764564752578735
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.7764564752578735
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.5388932228088379
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;to be applied to the file;a file by file comparison of the scanner;Similar;0.6926993131637573
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;For non */uapi/* files;that summary was;Similar;0.5390006899833679
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.5223103761672974
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.5718693137168884
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.6788543462753296
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.600976824760437
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.6064829230308533
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.5581822395324707
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.6341211795806885
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;allow future extension of the bulk alloc API;is done to;Similar;0.5394164323806763
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.5099562406539917
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5604861378669739
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5412046909332275
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;resurrects approach first proposed in [1];To fix this issue;Similar;0.513443112373352
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5803713798522949
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.5028970837593079
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6068916916847229
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5311303734779358
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.5341372489929199
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;unioned together;Conveniently;Similar;0.549461841583252
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.6025382280349731
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.633147656917572
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;function naming changes;requires some;Similar;0.668394923210144
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5970290899276733
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.6613484025001526
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5588555335998535
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5867178440093994
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.6074585914611816
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.7157706022262573
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5583823919296265
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6685818433761597
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;This affects RCU handling;somewhat;Similar;0.6312791109085083
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Fix early boot kernel crash;Slob;Similar;0.5496988296508789
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Remove various small accessors;various small;Similar;0.574177622795105
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5186856985092163
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;cleans up some bitrot in slob.c;Also;Similar;0.6138250827789307
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6150541305541992
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Fix gfp flags passed to lockdep;lockdep;Similar;0.6828028559684753
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6574214100837708
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Adding this mask;fixes the bug;Similar;0.5285528302192688
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.6081493496894836
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.727520227432251
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Remove kmemtrace ftrace plugin;tracing;Similar;0.6419102549552917
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5586241483688354
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5992990732192993
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.6516208052635193
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;use tracepoints;kmemtrace;Similar;0.6269597411155701
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6986734867095947
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;fix typo in mm/slob.c;build fix;Similar;0.7258086204528809
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;fix lockup in slob_free()  ;lockup;Similar;0.6541125774383545
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6803911924362183
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6429154872894287
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;enable and use this tracer;To enable and use this tracer;Similar;0.6194782257080078
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;fix bogus ksize calculation fix  ;SLOB;Similar;0.5935317277908325
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5276322364807129
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;fix bogus ksize calculation;bogus;Similar;0.5888504981994629
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Fix to return wrong pointer;Fix;Similar;0.5823779106140137
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;reduce external fragmentation by using three free lists;slob;Similar;0.5230991840362549
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;fix memory corruption;memory corruption;Similar;0.5680539011955261
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Handle that separately in krealloc();separately;Similar;0.5244120359420776
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6191245317459106
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;reduce list scanning;slob;Similar;0.5404069423675537
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5117253661155701
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Cleanup zeroing allocations;Slab allocators;Similar;0.5306955575942993
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5294744968414307
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5415500402450562
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;remove bigblock tracking;slob;Similar;0.538246750831604
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5787973403930664
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;rework freelist handling;slob;Similar;0.6486562490463257
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6776276230812073
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6067395210266113
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5001246929168701
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;SLOB to be used on SMP  ;allows;Similar;0.705425500869751
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6496202349662781
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5403149724006653
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;handle SLAB_PANIC flag;slob;Similar;0.586609959602356
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5541250705718994
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5739554166793823
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.7173829674720764
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5563942790031433
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.639029860496521
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Redo a lot of comments;Also;Similar;0.5539928674697876
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6233115196228027
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5482001304626465
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;simplifies SLOB;at this point slob may be broken;Similar;0.5993540287017822
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6687581539154053
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;fix;SLOB=y && SMP=y fix;Similar;0.5653833746910095
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.657855212688446
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5200884342193604
The information printed can depend on kernel configuration;depend on kernel configuration;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.6082510948181152
The information printed can depend on kernel configuration;depend on kernel configuration;have some use eventually for annotations in drivers/gpu;I might;Similar;0.726136326789856
The information printed can depend on kernel configuration;depend on kernel configuration;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.5870898962020874
The information printed can depend on kernel configuration;depend on kernel configuration;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.6109962463378906
The information printed can depend on kernel configuration;depend on kernel configuration;I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Similar;0.5359039902687073
The information printed can depend on kernel configuration;depend on kernel configuration;patch ensures alignment on all arches and cache sizes;Still;Similar;0.5194461345672607
The information printed can depend on kernel configuration;depend on kernel configuration;improve memory accounting;improve;Similar;0.5888161659240723
The information printed can depend on kernel configuration;depend on kernel configuration;Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Similar;0.5078911781311035
The information printed can depend on kernel configuration;depend on kernel configuration;it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;Similar;0.6152772307395935
The information printed can depend on kernel configuration;depend on kernel configuration;"SLAB doesnt actually use page allocator directly
";no change there;Similar;0.5365533828735352
The information printed can depend on kernel configuration;depend on kernel configuration;to find out the size of a potentially huge page;Its unnecessarily hard;Similar;0.5132588744163513
The information printed can depend on kernel configuration;depend on kernel configuration;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.5720123648643494
The information printed can depend on kernel configuration;depend on kernel configuration;verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Similar;0.5138262510299683
The information printed can depend on kernel configuration;depend on kernel configuration;configure and build (select SLOB allocator);SLOB allocator;Similar;0.609490156173706
The information printed can depend on kernel configuration;depend on kernel configuration;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.6087349653244019
The information printed can depend on kernel configuration;depend on kernel configuration;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.6087349653244019
The information printed can depend on kernel configuration;depend on kernel configuration;to be applied to the file;a file by file comparison of the scanner;Similar;0.7070376873016357
The information printed can depend on kernel configuration;depend on kernel configuration;Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Similar;0.5248123407363892
The information printed can depend on kernel configuration;depend on kernel configuration;the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Similar;0.5361804962158203
The information printed can depend on kernel configuration;depend on kernel configuration;Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Similar;0.5108516216278076
The information printed can depend on kernel configuration;depend on kernel configuration;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.5271610617637634
The information printed can depend on kernel configuration;depend on kernel configuration;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.628553569316864
The information printed can depend on kernel configuration;depend on kernel configuration;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.5471860766410828
The information printed can depend on kernel configuration;depend on kernel configuration;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5439324378967285
The information printed can depend on kernel configuration;depend on kernel configuration;allow future extension of the bulk alloc API;is done to;Similar;0.5290509462356567
The information printed can depend on kernel configuration;depend on kernel configuration;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.5994454026222229
The information printed can depend on kernel configuration;depend on kernel configuration;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5607286095619202
The information printed can depend on kernel configuration;depend on kernel configuration;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5162680149078369
The information printed can depend on kernel configuration;depend on kernel configuration;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5511775016784668
The information printed can depend on kernel configuration;depend on kernel configuration;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6063587665557861
The information printed can depend on kernel configuration;depend on kernel configuration;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5151731967926025
The information printed can depend on kernel configuration;depend on kernel configuration;unioned together;Conveniently;Similar;0.5709573030471802
The information printed can depend on kernel configuration;depend on kernel configuration;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.6100086569786072
The information printed can depend on kernel configuration;depend on kernel configuration;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.6282823085784912
The information printed can depend on kernel configuration;depend on kernel configuration;function naming changes;requires some;Similar;0.64518141746521
The information printed can depend on kernel configuration;depend on kernel configuration;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5014826059341431
The information printed can depend on kernel configuration;depend on kernel configuration;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5855194330215454
The information printed can depend on kernel configuration;depend on kernel configuration;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.6351588368415833
The information printed can depend on kernel configuration;depend on kernel configuration;Improve trace accuracy;by correctly tracing reported size;Similar;0.5137025117874146
The information printed can depend on kernel configuration;depend on kernel configuration;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5527026653289795
The information printed can depend on kernel configuration;depend on kernel configuration;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5768294334411621
The information printed can depend on kernel configuration;depend on kernel configuration;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5032376050949097
The information printed can depend on kernel configuration;depend on kernel configuration;This will allow us to push more processing into common code later;improve readability;Similar;0.512001633644104
The information printed can depend on kernel configuration;depend on kernel configuration;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5131120085716248
The information printed can depend on kernel configuration;depend on kernel configuration;This affects RCU handling;somewhat;Similar;0.712997317314148
The information printed can depend on kernel configuration;depend on kernel configuration;Fix early boot kernel crash;Slob;Similar;0.6363319754600525
The information printed can depend on kernel configuration;depend on kernel configuration;cleans up some bitrot in slob.c;Also;Similar;0.5397794246673584
The information printed can depend on kernel configuration;depend on kernel configuration;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.7122583389282227
The information printed can depend on kernel configuration;depend on kernel configuration;Fix gfp flags passed to lockdep;lockdep;Similar;0.6097773313522339
The information printed can depend on kernel configuration;depend on kernel configuration;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.570233166217804
The information printed can depend on kernel configuration;depend on kernel configuration;Adding this mask;fixes the bug;Similar;0.520904541015625
The information printed can depend on kernel configuration;depend on kernel configuration;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5931804180145264
The information printed can depend on kernel configuration;depend on kernel configuration;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.607555627822876
The information printed can depend on kernel configuration;depend on kernel configuration;refactor code for future changes;Impact;Similar;0.5285906791687012
The information printed can depend on kernel configuration;depend on kernel configuration;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.6663364171981812
The information printed can depend on kernel configuration;depend on kernel configuration;use tracepoints;kmemtrace;Similar;0.5496537685394287
The information printed can depend on kernel configuration;depend on kernel configuration;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5344884395599365
The information printed can depend on kernel configuration;depend on kernel configuration;fix typo in mm/slob.c;build fix;Similar;0.6186978220939636
The information printed can depend on kernel configuration;depend on kernel configuration;fix lockup in slob_free()  ;lockup;Similar;0.5289738178253174
The information printed can depend on kernel configuration;depend on kernel configuration;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.548369824886322
The information printed can depend on kernel configuration;depend on kernel configuration;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5259624719619751
The information printed can depend on kernel configuration;depend on kernel configuration;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6224033832550049
The information printed can depend on kernel configuration;depend on kernel configuration;enable and use this tracer;To enable and use this tracer;Similar;0.5947732925415039
The information printed can depend on kernel configuration;depend on kernel configuration;fix bogus ksize calculation fix  ;SLOB;Similar;0.5319068431854248
The information printed can depend on kernel configuration;depend on kernel configuration;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5309019088745117
The information printed can depend on kernel configuration;depend on kernel configuration;fix bogus ksize calculation;bogus;Similar;0.5330274701118469
The information printed can depend on kernel configuration;depend on kernel configuration;record page flag overlays explicitly;slob;Similar;0.5362704992294312
The information printed can depend on kernel configuration;depend on kernel configuration;Fix to return wrong pointer;Fix;Similar;0.5273901224136353
The information printed can depend on kernel configuration;depend on kernel configuration;fix memory corruption;memory corruption;Similar;0.5950859785079956
The information printed can depend on kernel configuration;depend on kernel configuration;Handle that separately in krealloc();separately;Similar;0.5062776207923889
The information printed can depend on kernel configuration;depend on kernel configuration;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5100523829460144
The information printed can depend on kernel configuration;depend on kernel configuration;improved alignment handling;improved;Similar;0.5300388336181641
The information printed can depend on kernel configuration;depend on kernel configuration;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5777554512023926
The information printed can depend on kernel configuration;depend on kernel configuration;rework freelist handling;slob;Similar;0.6239399909973145
The information printed can depend on kernel configuration;depend on kernel configuration;"align them to word size
";it is best in practice;Similar;0.551250159740448
The information printed can depend on kernel configuration;depend on kernel configuration;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5157135128974915
The information printed can depend on kernel configuration;depend on kernel configuration;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6801626086235046
The information printed can depend on kernel configuration;depend on kernel configuration;SLOB to be used on SMP  ;allows;Similar;0.5776021480560303
The information printed can depend on kernel configuration;depend on kernel configuration;fix page order calculation on not 4KB page;-;Similar;0.5046048164367676
The information printed can depend on kernel configuration;depend on kernel configuration;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6169149279594421
The information printed can depend on kernel configuration;depend on kernel configuration;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5167081952095032
The information printed can depend on kernel configuration;depend on kernel configuration;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5655238628387451
The information printed can depend on kernel configuration;depend on kernel configuration;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5107899904251099
The information printed can depend on kernel configuration;depend on kernel configuration;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.522856593132019
The information printed can depend on kernel configuration;depend on kernel configuration;Redo a lot of comments;Also;Similar;0.5164732933044434
The information printed can depend on kernel configuration;depend on kernel configuration;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6053856611251831
The information printed can depend on kernel configuration;depend on kernel configuration;simplifies SLOB;at this point slob may be broken;Similar;0.5292250514030457
The information printed can depend on kernel configuration;depend on kernel configuration;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5119627118110657
The information printed can depend on kernel configuration;depend on kernel configuration;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5323516130447388
The information printed can depend on kernel configuration;depend on kernel configuration;fix;SLOB=y && SMP=y fix;Similar;0.5367377996444702
The information printed can depend on kernel configuration;depend on kernel configuration;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5870690941810608
The information printed can depend on kernel configuration;depend on kernel configuration;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.512697160243988
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.5768386125564575
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.5762895345687866
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;"SLAB doesnt actually use page allocator directly
";no change there;Similar;0.620146632194519
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;configure and build (select SLOB allocator);SLOB allocator;Similar;0.5054173469543457
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.622856855392456
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.5284069776535034
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Similar;0.6359544396400452
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;All documentation files were explicitly excluded;explicitly excluded;Similar;0.5994430780410767
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;For non */uapi/* files;that summary was;Similar;0.583781898021698
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5355892181396484
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Similar;0.6660743951797485
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.5544120073318481
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.5596548914909363
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5109914541244507
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Make dead caches discard free slabs immediately;slub;Similar;0.5477534532546997
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5932114124298096
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.6006240844726562
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.52413010597229
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.5915669202804565
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5648208260536194
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.5048322677612305
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5529215335845947
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5686264634132385
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5323168039321899
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.5207996368408203
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5250029563903809
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Remove kmemtrace ftrace plugin;tracing;Similar;0.5431974530220032
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5004680752754211
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.6410974264144897
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.5420843362808228
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;fix typo in mm/slob.c;build fix;Similar;0.5095486044883728
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;fix lockup in slob_free()  ;lockup;Similar;0.5219873189926147
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5237084627151489
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5229617357254028
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.6350188255310059
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;remove useless ctor parameter and reorder parameters;useless;Similar;0.5180747509002686
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5532341003417969
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Cleanup zeroing allocations;Slab allocators;Similar;0.570267915725708
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.6820501089096069
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6282726526260376
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6479032635688782
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6166330575942993
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5623339414596558
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;SLOB to be used on SMP  ;allows;Similar;0.5181182622909546
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;fix page order calculation on not 4KB page;-;Similar;0.5742920637130737
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5005638599395752
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5013084411621094
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5543873310089111
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.534110426902771
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5736402273178101
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5687066316604614
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5274365544319153
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6279979348182678
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Similar;0.5085777640342712
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;have some use eventually for annotations in drivers/gpu;I might;Similar;0.5478501915931702
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.5868198275566101
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Similar;0.5050216913223267
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;Similar;0.5589970946311951
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;patch ensures alignment on all arches and cache sizes;Still;Similar;0.6248297691345215
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Similar;0.5131106376647949
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;configure and build (select SLOB allocator);SLOB allocator;Similar;0.6242945194244385
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.5401207208633423
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.5401207208633423
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Similar;0.6091881990432739
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.5292442440986633
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;allow future extension of the bulk alloc API;is done to;Similar;0.5898600816726685
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.6436772346496582
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5591326951980591
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5165342688560486
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.582878828048706
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5055962800979614
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6017783284187317
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5549790859222412
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5631048083305359
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5348204374313354
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5536518096923828
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5566433668136597
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5571998357772827
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5579391717910767
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5568394064903259
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5777068138122559
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5684710741043091
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6517289876937866
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5201845169067383
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5520301461219788
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5094007253646851
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5994688272476196
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6063291430473328
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5935152769088745
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5363608598709106
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;Similar;0.5068387985229492
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5350937843322754
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5000676512718201
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5050763487815857
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5842410326004028
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5116134881973267
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Similar;0.7561713457107544
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.59019935131073
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Similar;0.5015915632247925
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.6436995267868042
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;improve memory accounting;improve;Similar;0.516527533531189
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.6770875453948975
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;configure and build (select SLOB allocator);SLOB allocator;Similar;0.7049573063850403
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.6443537473678589
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.749772310256958
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.749772310256958
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.5165578126907349
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;to be applied to the file;a file by file comparison of the scanner;Similar;0.6448572874069214
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.5119212865829468
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.5218459367752075
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.6764085292816162
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.6580295562744141
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.5169793367385864
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.5748035907745361
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5424633622169495
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5056983828544617
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5648901462554932
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.527961015701294
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5750083923339844
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.519416332244873
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5751221179962158
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.5582226514816284
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5681141018867493
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;function naming changes;requires some;Similar;0.5850952863693237
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.6174448132514954
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.6654425263404846
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.6268417835235596
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5467089414596558
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.7224432229995728
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.6580966711044312
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6881459951400757
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;This affects RCU handling;somewhat;Similar;0.5248472690582275
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Fix early boot kernel crash;Slob;Similar;0.5187943577766418
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5025737285614014
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;cleans up some bitrot in slob.c;Also;Similar;0.6483679413795471
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6030031442642212
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Fix gfp flags passed to lockdep;lockdep;Similar;0.6459221839904785
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6895939111709595
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Adding this mask;fixes the bug;Similar;0.5354292392730713
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5389178395271301
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.7190121412277222
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Remove kmemtrace ftrace plugin;tracing;Similar;0.6388657093048096
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5305109024047852
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6727824211120605
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.6762322187423706
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;use tracepoints;kmemtrace;Similar;0.6007316708564758
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6644267439842224
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;fix typo in mm/slob.c;build fix;Similar;0.7523441314697266
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;fix lockup in slob_free()  ;lockup;Similar;0.6189785003662109
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6451013088226318
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5961573123931885
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;enable and use this tracer;To enable and use this tracer;Similar;0.5316306352615356
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;fix bogus ksize calculation fix  ;SLOB;Similar;0.5747015476226807
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;fix bogus ksize calculation;bogus;Similar;0.562240719795227
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Fix to return wrong pointer;Fix;Similar;0.549491286277771
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;reduce external fragmentation by using three free lists;slob;Similar;0.5053764581680298
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;fix memory corruption;memory corruption;Similar;0.582857072353363
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Handle that separately in krealloc();separately;Similar;0.5877331495285034
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.7171281576156616
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;reduce list scanning;slob;Similar;0.547333300113678
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5023820400238037
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5999810695648193
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5024998188018799
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;remove bigblock tracking;slob;Similar;0.6057493686676025
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.6127786040306091
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;rework freelist handling;slob;Similar;0.5653685927391052
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5004253387451172
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.7388125658035278
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6373952627182007
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5607768297195435
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;SLOB to be used on SMP  ;allows;Similar;0.7494216561317444
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;fix page order calculation on not 4KB page;-;Similar;0.513985276222229
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5467941761016846
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5796554088592529
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;handle SLAB_PANIC flag;slob;Similar;0.6531350612640381
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6098633408546448
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6600480079650879
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5917871594429016
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6242092847824097
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5073790550231934
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;simplifies SLOB;at this point slob may be broken;Similar;0.6126545667648315
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5036316514015198
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6711786389350891
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6916510462760925
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5912977457046509
have some use eventually for annotations in drivers/gpu;I might;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.6401538848876953
have some use eventually for annotations in drivers/gpu;I might;I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Similar;0.5473560094833374
have some use eventually for annotations in drivers/gpu;I might;improve memory accounting;improve;Similar;0.6118314266204834
have some use eventually for annotations in drivers/gpu;I might;Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Similar;0.5048534870147705
have some use eventually for annotations in drivers/gpu;I might;it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;Similar;0.6077648997306824
have some use eventually for annotations in drivers/gpu;I might;to find out the size of a potentially huge page;Its unnecessarily hard;Similar;0.5302915573120117
have some use eventually for annotations in drivers/gpu;I might;verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Similar;0.5894179940223694
have some use eventually for annotations in drivers/gpu;I might;configure and build (select SLOB allocator);SLOB allocator;Similar;0.6185113787651062
have some use eventually for annotations in drivers/gpu;I might;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.5183759927749634
have some use eventually for annotations in drivers/gpu;I might;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.5183759927749634
have some use eventually for annotations in drivers/gpu;I might;to be applied to the file;a file by file comparison of the scanner;Similar;0.6355623006820679
have some use eventually for annotations in drivers/gpu;I might;Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Similar;0.5668295621871948
have some use eventually for annotations in drivers/gpu;I might;the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Similar;0.5055888295173645
have some use eventually for annotations in drivers/gpu;I might;Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Similar;0.6072343587875366
have some use eventually for annotations in drivers/gpu;I might;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.5336161851882935
have some use eventually for annotations in drivers/gpu;I might;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5654661655426025
have some use eventually for annotations in drivers/gpu;I might;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.510754406452179
have some use eventually for annotations in drivers/gpu;I might;allow future extension of the bulk alloc API;is done to;Similar;0.6479800939559937
have some use eventually for annotations in drivers/gpu;I might;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.6683546304702759
have some use eventually for annotations in drivers/gpu;I might;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5873774886131287
have some use eventually for annotations in drivers/gpu;I might;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5557413101196289
have some use eventually for annotations in drivers/gpu;I might;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6681063771247864
have some use eventually for annotations in drivers/gpu;I might;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5572875738143921
have some use eventually for annotations in drivers/gpu;I might;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5840739011764526
have some use eventually for annotations in drivers/gpu;I might;function naming changes;requires some;Similar;0.6049155592918396
have some use eventually for annotations in drivers/gpu;I might;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5023149251937866
have some use eventually for annotations in drivers/gpu;I might;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.557684063911438
have some use eventually for annotations in drivers/gpu;I might;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5327771902084351
have some use eventually for annotations in drivers/gpu;I might;Improve trace accuracy;by correctly tracing reported size;Similar;0.5105343461036682
have some use eventually for annotations in drivers/gpu;I might;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5207098722457886
have some use eventually for annotations in drivers/gpu;I might;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5370851159095764
have some use eventually for annotations in drivers/gpu;I might;This will allow us to push more processing into common code later;improve readability;Similar;0.7146117687225342
have some use eventually for annotations in drivers/gpu;I might;This affects RCU handling;somewhat;Similar;0.5220839977264404
have some use eventually for annotations in drivers/gpu;I might;Fix early boot kernel crash;Slob;Similar;0.599726140499115
have some use eventually for annotations in drivers/gpu;I might;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5867002010345459
have some use eventually for annotations in drivers/gpu;I might;cleans up some bitrot in slob.c;Also;Similar;0.5922102928161621
have some use eventually for annotations in drivers/gpu;I might;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6712888479232788
have some use eventually for annotations in drivers/gpu;I might;Fix gfp flags passed to lockdep;lockdep;Similar;0.5178020596504211
have some use eventually for annotations in drivers/gpu;I might;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5319823026657104
have some use eventually for annotations in drivers/gpu;I might;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5312933325767517
have some use eventually for annotations in drivers/gpu;I might;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5364651679992676
have some use eventually for annotations in drivers/gpu;I might;refactor code for future changes;Impact;Similar;0.6596487760543823
have some use eventually for annotations in drivers/gpu;I might;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.574652910232544
have some use eventually for annotations in drivers/gpu;I might;fix typo in mm/slob.c;build fix;Similar;0.567736029624939
have some use eventually for annotations in drivers/gpu;I might;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5679273009300232
have some use eventually for annotations in drivers/gpu;I might;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5504007339477539
have some use eventually for annotations in drivers/gpu;I might;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5812890529632568
have some use eventually for annotations in drivers/gpu;I might;enable and use this tracer;To enable and use this tracer;Similar;0.5403975248336792
have some use eventually for annotations in drivers/gpu;I might;fix memory corruption;memory corruption;Similar;0.5220267176628113
have some use eventually for annotations in drivers/gpu;I might;improved alignment handling;improved;Similar;0.5313147306442261
have some use eventually for annotations in drivers/gpu;I might;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5109855532646179
have some use eventually for annotations in drivers/gpu;I might;rework freelist handling;slob;Similar;0.5314725637435913
have some use eventually for annotations in drivers/gpu;I might;"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;Similar;0.5175684690475464
have some use eventually for annotations in drivers/gpu;I might;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6026607751846313
have some use eventually for annotations in drivers/gpu;I might;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5789129734039307
have some use eventually for annotations in drivers/gpu;I might;Redo a lot of comments;Also;Similar;0.5082873106002808
have some use eventually for annotations in drivers/gpu;I might;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5215733647346497
have some use eventually for annotations in drivers/gpu;I might;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5401244163513184
have some use eventually for annotations in drivers/gpu;I might;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5269159078598022
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Similar;0.5864704847335815
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Similar;0.5343087911605835
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.6979695558547974
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;improve memory accounting;improve;Similar;0.5361588001251221
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Similar;0.5567396879196167
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.6744102835655212
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;configure and build (select SLOB allocator);SLOB allocator;Similar;0.6598633527755737
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.6534066796302795
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.80347740650177
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.80347740650177
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.5928740501403809
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.6152850389480591
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Similar;0.5215358734130859
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;to be applied to the file;a file by file comparison of the scanner;Similar;0.6457521319389343
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;For non */uapi/* files;that summary was;Similar;0.5299326181411743
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.6169724464416504
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.6839969158172607
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.6635972261428833
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.6500111222267151
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.6311492919921875
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5911285281181335
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;allow future extension of the bulk alloc API;is done to;Similar;0.5250632762908936
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.5126060247421265
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5715400576591492
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;resurrects approach first proposed in [1];To fix this issue;Similar;0.5396265983581543
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.6432263851165771
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5013890862464905
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.5578336119651794
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6059434413909912
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5495179891586304
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5182945728302002
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.5607332587242126
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5087426900863647
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5156466960906982
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.6714245080947876
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;function naming changes;requires some;Similar;0.6751560568809509
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.7144445180892944
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.7417913675308228
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5733635425567627
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5323808193206787
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.6117368936538696
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.7601664066314697
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.6863694190979004
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;can be done in __kmem_cache_shutdown;What is done there;Similar;0.7188998460769653
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;This affects RCU handling;somewhat;Similar;0.5287302732467651
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Fix early boot kernel crash;Slob;Similar;0.544095516204834
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Remove various small accessors;various small;Similar;0.5335667729377747
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;cleans up some bitrot in slob.c;Also;Similar;0.5539549589157104
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6181536912918091
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Fix gfp flags passed to lockdep;lockdep;Similar;0.805382490158081
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.7065013647079468
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Adding this mask;fixes the bug;Similar;0.579120397567749
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.691870391368866
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.7661594152450562
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Remove kmemtrace ftrace plugin;tracing;Similar;0.6603553295135498
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5614563226699829
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.7142469882965088
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5689319372177124
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;refactor code for future changes;Impact;Similar;0.5771198272705078
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.646945595741272
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;use tracepoints;kmemtrace;Similar;0.6413564682006836
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.7697198390960693
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;fix typo in mm/slob.c;build fix;Similar;0.766179084777832
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;fix lockup in slob_free()  ;lockup;Similar;0.7363770008087158
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6497201919555664
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5982553958892822
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;enable and use this tracer;To enable and use this tracer;Similar;0.5904481410980225
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;fix bogus ksize calculation fix  ;SLOB;Similar;0.6872407793998718
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5155858993530273
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;fix bogus ksize calculation;bogus;Similar;0.6709449291229248
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Fix to return wrong pointer;Fix;Similar;0.6368422508239746
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5272600650787354
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;reduce external fragmentation by using three free lists;slob;Similar;0.5955275893211365
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5950205326080322
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;fix memory corruption;memory corruption;Similar;0.5965714454650879
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Handle that separately in krealloc();separately;Similar;0.5941215753555298
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6987847089767456
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;reduce list scanning;slob;Similar;0.6203708648681641
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5210381746292114
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Cleanup zeroing allocations;Slab allocators;Similar;0.5163949131965637
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5685197114944458
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5318650007247925
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.6008843183517456
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;remove bigblock tracking;slob;Similar;0.6719235777854919
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5664258003234863
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;rework freelist handling;slob;Similar;0.6922570466995239
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;"align them to word size
";it is best in practice;Similar;0.5491059422492981
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6951910853385925
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6661669015884399
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;SLOB to be used on SMP  ;allows;Similar;0.680500328540802
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;fix page order calculation on not 4KB page;-;Similar;0.6066341400146484
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6824091672897339
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6431031823158264
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;handle SLAB_PANIC flag;slob;Similar;0.6724621057510376
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6395611763000488
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6203163862228394
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5054396390914917
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6073397397994995
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6290218830108643
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Redo a lot of comments;Also;Similar;0.5568636059761047
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.694717526435852
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5511804819107056
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;simplifies SLOB;at this point slob may be broken;Similar;0.6254400610923767
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6313556432723999
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7012926340103149
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;fix;SLOB=y && SMP=y fix;Similar;0.5903943181037903
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6506261825561523
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.594740092754364
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;patch ensures alignment on all arches and cache sizes;Still;Similar;0.5524806976318359
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;add three helpers, convert the appropriate places;these three patches;Similar;0.5765105485916138
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;to find out the size of a potentially huge page;Its unnecessarily hard;Similar;0.6049572229385376
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.654337465763092
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;configure and build (select SLOB allocator);SLOB allocator;Similar;0.6018962860107422
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.5873583555221558
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.6697885990142822
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.6697885990142822
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;to be applied to the file;a file by file comparison of the scanner;Similar;0.6515700221061707
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Similar;0.609336256980896
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Similar;0.5054073333740234
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.546268105506897
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.5070410370826721
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;allow future extension of the bulk alloc API;is done to;Similar;0.6813600063323975
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;We always end up returning some objects at the cost of another cmpxchg  ;Else;Similar;0.5531291961669922
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.5847328901290894
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5213444232940674
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5208449363708496
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5441075563430786
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5465136766433716
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5793887972831726
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;unioned together;Conveniently;Similar;0.5520182847976685
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5004990696907043
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5030927062034607
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.6437809467315674
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;function naming changes;requires some;Similar;0.610889196395874
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5757575035095215
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.6069035530090332
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.6205712556838989
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.6221352815628052
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5538074374198914
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;This will allow us to push more processing into common code later;improve readability;Similar;0.6052559614181519
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;This affects RCU handling;somewhat;Similar;0.5041706562042236
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5948737859725952
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.646702229976654
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Fix gfp flags passed to lockdep;lockdep;Similar;0.6018490791320801
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5936952829360962
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5848806500434875
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5998188853263855
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;refactor code for future changes;Impact;Similar;0.6212903261184692
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5711426734924316
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5463184118270874
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;fix typo in mm/slob.c;build fix;Similar;0.6054090857505798
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6825824975967407
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5572350025177002
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;enable and use this tracer;To enable and use this tracer;Similar;0.5107030868530273
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;record page flag overlays explicitly;slob;Similar;0.5079420804977417
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;reduce external fragmentation by using three free lists;slob;Similar;0.5526676774024963
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5547041296958923
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5426594018936157
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.509914755821228
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5075092315673828
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5859915614128113
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;rework freelist handling;slob;Similar;0.5329301357269287
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;Similar;0.6100706458091736
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;"align them to word size
";it is best in practice;Similar;0.5828883647918701
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5388954281806946
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;SLOB to be used on SMP  ;allows;Similar;0.5946686267852783
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5112721920013428
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.516819953918457
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6094024777412415
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.533784031867981
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5778709053993225
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Redo a lot of comments;Also;Similar;0.624778151512146
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5305361747741699
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5864493250846863
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5180356502532959
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;SLOB saving nearly half a megabyte of RAM;Comparison for otherwise identical builds;Similar;0.508910596370697
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.7420414090156555
The size of slab memory shouldnt exceed 4Gb on 32-bit machines;it will fit into atomic_long_t we use for vmstats;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5240861177444458
The size of slab memory shouldnt exceed 4Gb on 32-bit machines;it will fit into atomic_long_t we use for vmstats;fix page order calculation on not 4KB page;-;Similar;0.5048587322235107
The size of slab memory shouldnt exceed 4Gb on 32-bit machines;it will fit into atomic_long_t we use for vmstats;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5280105471611023
The size of slab memory shouldnt exceed 4Gb on 32-bit machines;it will fit into atomic_long_t we use for vmstats;SLOB saving nearly half a megabyte of RAM;Comparison for otherwise identical builds;Similar;0.5919958353042603
The size of slab memory shouldnt exceed 4Gb on 32-bit machines;it will fit into atomic_long_t we use for vmstats;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5472126007080078
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Similar;0.5219042301177979
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.5688859224319458
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.5094856023788452
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.5094856023788452
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Similar;0.5853298902511597
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Similar;0.5948209762573242
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.5070934295654297
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5601409077644348
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5173321962356567
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.6383081078529358
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5641064047813416
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.585604727268219
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5673956871032715
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5929639339447021
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5208166837692261
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.5054985284805298
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5414719581604004
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5137925148010254
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5855749249458313
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5946500301361084
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5246044397354126
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.524535059928894
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5000412464141846
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5367358922958374
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5777418613433838
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Fix early boot kernel crash;Slob;Similar;0.5109485387802124
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.549328088760376
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Fix gfp flags passed to lockdep;lockdep;Similar;0.5449186563491821
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5290220975875854
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5503212213516235
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5585923194885254
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5521676540374756
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5496121644973755
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5368614196777344
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;fix lockup in slob_free()  ;lockup;Similar;0.5258558988571167
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5336134433746338
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6222578287124634
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.6119200587272644
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;reduce external fragmentation by using three free lists;slob;Similar;0.500706136226654
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;fix memory corruption;memory corruption;Similar;0.531548261642456
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5440465807914734
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5324699878692627
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5298833847045898
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5331519246101379
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5511160492897034
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5705154538154602
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5284173488616943
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5151485204696655
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5026494264602661
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6922686100006104
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5859202146530151
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5121695399284363
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;union of slab_list and lru;slab_list and lru are the same bits;Similar;0.5183466076850891
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;configure and build (select SLOB allocator);SLOB allocator;Similar;0.5233314633369446
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.655935525894165
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.5835020542144775
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.5835020542144775
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.7103431224822998
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.7140196561813354
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Similar;0.5504743456840515
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;For non */uapi/* files;that summary was;Similar;0.5661476850509644
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.7180550694465637
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.7246918082237244
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Similar;0.6047919988632202
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.7275115251541138
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.5910842418670654
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.7664573192596436
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5606304407119751
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5289520621299744
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Make dead caches discard free slabs immediately;slub;Similar;0.5227266550064087
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.6176520586013794
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.558685839176178
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.6489182114601135
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5289809703826904
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.7773572206497192
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.6793692111968994
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.8389064073562622
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;function naming changes;requires some;Similar;0.583513617515564
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.6442792415618896
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.6615111827850342
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.6538807153701782
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5230790972709656
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.6690889000892639
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;can be done in __kmem_cache_shutdown;What is done there;Similar;0.8336160182952881
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Remove various small accessors;various small;Similar;0.6716038584709167
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;They are no longer needed;They have become so simple;Similar;0.5468969941139221
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.5988619327545166
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;cleans up some bitrot in slob.c;Also;Similar;0.5976918935775757
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Fix gfp flags passed to lockdep;lockdep;Similar;0.6063966751098633
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5949273109436035
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6348462700843811
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5806233882904053
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Remove kmemtrace ftrace plugin;tracing;Similar;0.8030635118484497
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.7741802334785461
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.7202776670455933
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5109742283821106
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.6314481496810913
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6801099181175232
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;fix typo in mm/slob.c;build fix;Similar;0.6782541275024414
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;fix lockup in slob_free()  ;lockup;Similar;0.6662155389785767
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5835204124450684
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5095999240875244
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Drop it  ;if you want;Similar;0.6402416229248047
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;fix bogus ksize calculation fix  ;SLOB;Similar;0.615531325340271
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;fix bogus ksize calculation;bogus;Similar;0.625551700592041
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.7348951101303101
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Fix to return wrong pointer;Fix;Similar;0.623940110206604
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5688024759292603
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5758044123649597
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;fix memory corruption;memory corruption;Similar;0.538629412651062
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;remove useless ctor parameter and reorder parameters;useless;Similar;0.7217763662338257
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Handle that separately in krealloc();separately;Similar;0.625230073928833
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6011763215065002
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;reduce list scanning;slob;Similar;0.7596986293792725
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.6296316385269165
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Cleanup zeroing allocations;Slab allocators;Similar;0.7047279477119446
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5577526092529297
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6867361068725586
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;remove bigblock tracking;slob;Similar;0.7514429092407227
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;rework freelist handling;slob;Similar;0.5185610055923462
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5989995002746582
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6206365823745728
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5646848678588867
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5921647548675537
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;SLOB to be used on SMP  ;allows;Similar;0.5915561318397522
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;fix page order calculation on not 4KB page;-;Similar;0.6297270059585571
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.632003903388977
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.7969934940338135
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;handle SLAB_PANIC flag;slob;Similar;0.6266406774520874
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5059570074081421
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.6426980495452881
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6700180768966675
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5730346441268921
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.7886055707931519
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Redo a lot of comments;Also;Similar;0.5025118589401245
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5345001816749573
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6815412044525146
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;simplifies SLOB;at this point slob may be broken;Similar;0.5125906467437744
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5287503600120544
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7090489864349365
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5243314504623413
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5183461308479309
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;patch ensures alignment on all arches and cache sizes;Still;Similar;0.7122517824172974
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;allow future extension of the bulk alloc API;is done to;Similar;0.56465744972229
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.550186038017273
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5118185877799988
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5339450240135193
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";Similar;0.5182337760925293
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Similar;0.616097092628479
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5060250759124756
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;record page flag overlays explicitly;slob;Similar;0.5221070051193237
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.6136919260025024
patch ensures alignment on all arches and cache sizes;Still;Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Similar;0.5488350987434387
patch ensures alignment on all arches and cache sizes;Still;verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Similar;0.6033300161361694
patch ensures alignment on all arches and cache sizes;Still;configure and build (select SLOB allocator);SLOB allocator;Similar;0.5429549217224121
patch ensures alignment on all arches and cache sizes;Still;allow future extension of the bulk alloc API;is done to;Similar;0.539704442024231
patch ensures alignment on all arches and cache sizes;Still;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.5137025117874146
patch ensures alignment on all arches and cache sizes;Still;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5663790106773376
patch ensures alignment on all arches and cache sizes;Still;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6130525469779968
patch ensures alignment on all arches and cache sizes;Still;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.556513249874115
patch ensures alignment on all arches and cache sizes;Still;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.6071019172668457
patch ensures alignment on all arches and cache sizes;Still;If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;Similar;0.5192643404006958
patch ensures alignment on all arches and cache sizes;Still;using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";Similar;0.5022275447845459
patch ensures alignment on all arches and cache sizes;Still;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5178789496421814
patch ensures alignment on all arches and cache sizes;Still;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Similar;0.6065282821655273
patch ensures alignment on all arches and cache sizes;Still;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.6196713447570801
patch ensures alignment on all arches and cache sizes;Still;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5023542642593384
patch ensures alignment on all arches and cache sizes;Still;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5102961659431458
patch ensures alignment on all arches and cache sizes;Still;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6483209133148193
patch ensures alignment on all arches and cache sizes;Still;enable and use this tracer;To enable and use this tracer;Similar;0.5562154054641724
patch ensures alignment on all arches and cache sizes;Still;record page flag overlays explicitly;slob;Similar;0.5238083600997925
patch ensures alignment on all arches and cache sizes;Still;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5073140859603882
patch ensures alignment on all arches and cache sizes;Still;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5726636052131653
patch ensures alignment on all arches and cache sizes;Still;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.6353271007537842
patch ensures alignment on all arches and cache sizes;Still;improved alignment handling;improved;Similar;0.5293911695480347
patch ensures alignment on all arches and cache sizes;Still;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.6684693694114685
improve memory accounting;improve;Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Similar;0.5569745302200317
improve memory accounting;improve;it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;Similar;0.6396920680999756
improve memory accounting;improve;verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Similar;0.5438182353973389
improve memory accounting;improve;configure and build (select SLOB allocator);SLOB allocator;Similar;0.5109118223190308
improve memory accounting;improve;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.5438169240951538
improve memory accounting;improve;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.5438169240951538
improve memory accounting;improve;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.5109556913375854
improve memory accounting;improve;to be applied to the file;a file by file comparison of the scanner;Similar;0.6160564422607422
improve memory accounting;improve;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5134168863296509
improve memory accounting;improve;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.5294352769851685
improve memory accounting;improve;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5314444303512573
improve memory accounting;improve;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5244393348693848
improve memory accounting;improve;slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Similar;0.5090841054916382
improve memory accounting;improve;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6014817357063293
improve memory accounting;improve;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.6033025979995728
improve memory accounting;improve;unioned together;Conveniently;Similar;0.523531973361969
improve memory accounting;improve;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.6027392148971558
improve memory accounting;improve;function naming changes;requires some;Similar;0.590400218963623
improve memory accounting;improve;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5643401741981506
improve memory accounting;improve;using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";Similar;0.5390878915786743
improve memory accounting;improve;Improve trace accuracy;by correctly tracing reported size;Similar;0.7891196012496948
improve memory accounting;improve;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5373406410217285
improve memory accounting;improve;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5090836882591248
improve memory accounting;improve;This will allow us to push more processing into common code later;improve readability;Similar;0.5956070423126221
improve memory accounting;improve;can be done in __kmem_cache_shutdown;What is done there;Similar;0.525594174861908
improve memory accounting;improve;This affects RCU handling;somewhat;Similar;0.6010094881057739
improve memory accounting;improve;Fix early boot kernel crash;Slob;Similar;0.5128346681594849
improve memory accounting;improve;cleans up some bitrot in slob.c;Also;Similar;0.5599101185798645
improve memory accounting;improve;Adding this mask;fixes the bug;Similar;0.5849398374557495
improve memory accounting;improve;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5040995478630066
improve memory accounting;improve;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.606395959854126
improve memory accounting;improve;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5611461400985718
improve memory accounting;improve;refactor code for future changes;Impact;Similar;0.542819619178772
improve memory accounting;improve;use tracepoints;kmemtrace;Similar;0.591238796710968
improve memory accounting;improve;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5306844711303711
improve memory accounting;improve;fix typo in mm/slob.c;build fix;Similar;0.5872241258621216
improve memory accounting;improve;fix lockup in slob_free()  ;lockup;Similar;0.5533086657524109
improve memory accounting;improve;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5321342349052429
improve memory accounting;improve;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.6343695521354675
improve memory accounting;improve;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5467455387115479
improve memory accounting;improve;enable and use this tracer;To enable and use this tracer;Similar;0.6949025392532349
improve memory accounting;improve;I find it more readable  ;personal opinion;Similar;0.6433922052383423
improve memory accounting;improve;fix bogus ksize calculation fix  ;SLOB;Similar;0.5840082168579102
improve memory accounting;improve;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.531872034072876
improve memory accounting;improve;fix bogus ksize calculation;bogus;Similar;0.5941137075424194
improve memory accounting;improve;Fix to return wrong pointer;Fix;Similar;0.5413974523544312
improve memory accounting;improve;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5952287912368774
improve memory accounting;improve;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5471305847167969
improve memory accounting;improve;fix memory corruption;memory corruption;Similar;0.7690743207931519
improve memory accounting;improve;reduce list scanning;slob;Similar;0.5239852666854858
improve memory accounting;improve;improved alignment handling;improved;Similar;0.7323771715164185
improve memory accounting;improve;rework freelist handling;slob;Similar;0.5721961259841919
improve memory accounting;improve;"align them to word size
";it is best in practice;Similar;0.5641966462135315
improve memory accounting;improve;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5661742687225342
improve memory accounting;improve;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5305208563804626
improve memory accounting;improve;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5623055100440979
improve memory accounting;improve;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5028231143951416
improve memory accounting;improve;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5502990484237671
improve memory accounting;improve;Redo a lot of comments;Also;Similar;0.5292637944221497
improve memory accounting;improve;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.510849118232727
improve memory accounting;improve;simplifies SLOB;at this point slob may be broken;Similar;0.6481710076332092
improve memory accounting;improve;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.52936851978302
improve memory accounting;improve;fix;SLOB=y && SMP=y fix;Similar;0.5783184170722961
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;Similar;0.5175845623016357
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Similar;0.7135986685752869
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;configure and build (select SLOB allocator);SLOB allocator;Similar;0.5304465293884277
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.5158603191375732
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.5158603191375732
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.5026040077209473
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.540108323097229
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;allow future extension of the bulk alloc API;is done to;Similar;0.5551620125770569
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.5229364633560181
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5488684177398682
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.6393516063690186
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5892666578292847
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5848026871681213
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.652758002281189
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5504239201545715
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5941956639289856
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.589461624622345
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5600741505622864
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5041379928588867
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5082734227180481
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";Similar;0.674095630645752
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Improve trace accuracy;by correctly tracing reported size;Similar;0.5471371412277222
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5583412647247314
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;This will allow us to push more processing into common code later;improve readability;Similar;0.5722402334213257
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5209189057350159
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;cleans up some bitrot in slob.c;Also;Similar;0.521705150604248
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5682207345962524
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5183735489845276
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5310071706771851
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;refactor code for future changes;Impact;Similar;0.5552809238433838
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5992530584335327
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;fix typo in mm/slob.c;build fix;Similar;0.5159665942192078
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;fix lockup in slob_free()  ;lockup;Similar;0.6514860987663269
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5274063348770142
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5439953207969666
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;enable and use this tracer;To enable and use this tracer;Similar;0.5230619311332703
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;I find it more readable  ;personal opinion;Similar;0.5297442078590393
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;fix bogus ksize calculation fix  ;SLOB;Similar;0.5742480158805847
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;fix bogus ksize calculation;bogus;Similar;0.5582123398780823
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Fix to return wrong pointer;Fix;Similar;0.5779784917831421
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5935862064361572
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;reduce external fragmentation by using three free lists;slob;Similar;0.5214393138885498
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.6815261840820312
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5334570407867432
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;fix memory corruption;memory corruption;Similar;0.5644643306732178
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5772497653961182
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5815309286117554
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;improved alignment handling;improved;Similar;0.5084630250930786
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5021653175354004
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;rework freelist handling;slob;Similar;0.6501544713973999
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5816408395767212
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5399957299232483
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;fix page order calculation on not 4KB page;-;Similar;0.555949330329895
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6302260160446167
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6953192949295044
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.6034292578697205
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6338698863983154
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;simplifies SLOB;at this point slob may be broken;Similar;0.5532859563827515
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;"SLAB doesnt actually use page allocator directly
";no change there;Similar;0.5182428359985352
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.509697437286377
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.5839040279388428
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;to be applied to the file;a file by file comparison of the scanner;Similar;0.505765438079834
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5730700492858887
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;hiding potentially buggy callers;except temporarily;Similar;0.6737865805625916
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.562904953956604
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.6451561450958252
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Similar;0.5142000317573547
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5384180545806885
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5007840991020203
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5091477632522583
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;function naming changes;requires some;Similar;0.5076888799667358
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;Similar;0.5455130934715271
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5495350360870361
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;Improve trace accuracy;by correctly tracing reported size;Similar;0.6173741221427917
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;This will allow us to push more processing into common code later;improve readability;Similar;0.5420268774032593
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5046466588973999
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;This affects RCU handling;somewhat;Similar;0.6271686553955078
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;Fix early boot kernel crash;Slob;Similar;0.5330792665481567
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;Remove various small accessors;various small;Similar;0.5733586549758911
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;Adding this mask;fixes the bug;Similar;0.504198431968689
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5067695379257202
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.5840029716491699
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;refactor code for future changes;Impact;Similar;0.537420392036438
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.6263851523399353
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;enable and use this tracer;To enable and use this tracer;Similar;0.5207559466362
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;I find it more readable  ;personal opinion;Similar;0.5100568532943726
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5330968499183655
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;fix bogus ksize calculation;bogus;Similar;0.5292314291000366
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;Fix to return wrong pointer;Fix;Similar;0.6274138689041138
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5462343096733093
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;fix memory corruption;memory corruption;Similar;0.7410479187965393
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;reduce list scanning;slob;Similar;0.5190249085426331
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.557191014289856
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;improved alignment handling;improved;Similar;0.5341427326202393
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;can be slightly faster too;skip almost-full freelist pages completely;Similar;0.5170795917510986
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;"Taking a spinlock in a destructor is a bit risky
";since the slab allocators may run the destructors anytime they decide a slab is no longer needed;Similar;0.6013844013214111
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6365687847137451
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5158467292785645
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5511820316314697
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5119905471801758
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;simplifies SLOB;at this point slob may be broken;Similar;0.5597469806671143
"SLAB doesnt actually use page allocator directly
";no change there;to find out the size of a potentially huge page;Its unnecessarily hard;Similar;0.5253869295120239
"SLAB doesnt actually use page allocator directly
";no change there;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.5722668766975403
"SLAB doesnt actually use page allocator directly
";no change there;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.561293363571167
"SLAB doesnt actually use page allocator directly
";no change there;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.5253525972366333
"SLAB doesnt actually use page allocator directly
";no change there;For non */uapi/* files;that summary was;Similar;0.5616946220397949
"SLAB doesnt actually use page allocator directly
";no change there;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.5119144916534424
"SLAB doesnt actually use page allocator directly
";no change there;SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Similar;0.5043767690658569
"SLAB doesnt actually use page allocator directly
";no change there;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.5576622486114502
"SLAB doesnt actually use page allocator directly
";no change there;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.528388261795044
"SLAB doesnt actually use page allocator directly
";no change there;slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Similar;0.5156955122947693
"SLAB doesnt actually use page allocator directly
";no change there;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Similar;0.6555171608924866
"SLAB doesnt actually use page allocator directly
";no change there;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.5123466849327087
"SLAB doesnt actually use page allocator directly
";no change there;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5605525970458984
"SLAB doesnt actually use page allocator directly
";no change there;This affects RCU handling;somewhat;Similar;0.5888500213623047
"SLAB doesnt actually use page allocator directly
";no change there;Remove various small accessors;various small;Similar;0.5776883363723755
"SLAB doesnt actually use page allocator directly
";no change there;They are no longer needed;They have become so simple;Similar;0.5782688856124878
"SLAB doesnt actually use page allocator directly
";no change there;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.5454291105270386
"SLAB doesnt actually use page allocator directly
";no change there;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5104919075965881
"SLAB doesnt actually use page allocator directly
";no change there;fix bogus ksize calculation fix  ;SLOB;Similar;0.504801869392395
"SLAB doesnt actually use page allocator directly
";no change there;fix bogus ksize calculation;bogus;Similar;0.5256025195121765
"SLAB doesnt actually use page allocator directly
";no change there;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5648615956306458
"SLAB doesnt actually use page allocator directly
";no change there;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5517045259475708
"SLAB doesnt actually use page allocator directly
";no change there;remove useless ctor parameter and reorder parameters;useless;Similar;0.5266386270523071
"SLAB doesnt actually use page allocator directly
";no change there;Handle that separately in krealloc();separately;Similar;0.5616000890731812
"SLAB doesnt actually use page allocator directly
";no change there;reduce list scanning;slob;Similar;0.5693403482437134
"SLAB doesnt actually use page allocator directly
";no change there;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.518811047077179
"SLAB doesnt actually use page allocator directly
";no change there;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5573400259017944
"SLAB doesnt actually use page allocator directly
";no change there;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.657812237739563
"SLAB doesnt actually use page allocator directly
";no change there;SLOB to be used on SMP  ;allows;Similar;0.5063577890396118
"SLAB doesnt actually use page allocator directly
";no change there;fix page order calculation on not 4KB page;-;Similar;0.6692115068435669
"SLAB doesnt actually use page allocator directly
";no change there;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5233858823776245
"SLAB doesnt actually use page allocator directly
";no change there;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.5673797130584717
"SLAB doesnt actually use page allocator directly
";no change there;fix;SLOB=y && SMP=y fix;Similar;0.5001431703567505
add three helpers, convert the appropriate places;these three patches;configure and build (select SLOB allocator);SLOB allocator;Similar;0.6139287948608398
add three helpers, convert the appropriate places;these three patches;allow future extension of the bulk alloc API;is done to;Similar;0.5258827209472656
add three helpers, convert the appropriate places;these three patches;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.5371902585029602
add three helpers, convert the appropriate places;these three patches;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5149102210998535
add three helpers, convert the appropriate places;these three patches;Fix gfp flags passed to lockdep;lockdep;Similar;0.5065479874610901
add three helpers, convert the appropriate places;these three patches;refactor code for future changes;Impact;Similar;0.5206945538520813
add three helpers, convert the appropriate places;these three patches;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5150430798530579
add three helpers, convert the appropriate places;these three patches;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5101977586746216
add three helpers, convert the appropriate places;these three patches;reduce external fragmentation by using three free lists;slob;Similar;0.6833734512329102
add three helpers, convert the appropriate places;these three patches;Redo a lot of comments;Also;Similar;0.5128697156906128
to find out the size of a potentially huge page;Its unnecessarily hard;to be applied to the file;a file by file comparison of the scanner;Similar;0.5834217071533203
to find out the size of a potentially huge page;Its unnecessarily hard;We always end up returning some objects at the cost of another cmpxchg  ;Else;Similar;0.5287126302719116
to find out the size of a potentially huge page;Its unnecessarily hard;slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Similar;0.5273897647857666
to find out the size of a potentially huge page;Its unnecessarily hard;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.6102773547172546
to find out the size of a potentially huge page;Its unnecessarily hard;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5294381380081177
to find out the size of a potentially huge page;Its unnecessarily hard;a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;Similar;0.5022616982460022
to find out the size of a potentially huge page;Its unnecessarily hard;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.6437774300575256
to find out the size of a potentially huge page;Its unnecessarily hard;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5060232877731323
to find out the size of a potentially huge page;Its unnecessarily hard;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5024421811103821
to find out the size of a potentially huge page;Its unnecessarily hard;enable and use this tracer;To enable and use this tracer;Similar;0.5048897862434387
to find out the size of a potentially huge page;Its unnecessarily hard;I find it more readable  ;personal opinion;Similar;0.5865417122840881
to find out the size of a potentially huge page;Its unnecessarily hard;record page flag overlays explicitly;slob;Similar;0.5317703485488892
to find out the size of a potentially huge page;Its unnecessarily hard;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.505936861038208
to find out the size of a potentially huge page;Its unnecessarily hard;can be slightly faster too;skip almost-full freelist pages completely;Similar;0.5496137142181396
to find out the size of a potentially huge page;Its unnecessarily hard;"align them to word size
";it is best in practice;Similar;0.653587818145752
to find out the size of a potentially huge page;Its unnecessarily hard;Redo a lot of comments;Also;Similar;0.593216598033905
to find out the size of a potentially huge page;Its unnecessarily hard;SLOB saving nearly half a megabyte of RAM;Comparison for otherwise identical builds;Similar;0.5514950752258301
to find out the size of a potentially huge page;Its unnecessarily hard;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5584721565246582
union of slab_list and lru;slab_list and lru are the same bits;configure and build (select SLOB allocator);SLOB allocator;Similar;0.6486133337020874
union of slab_list and lru;slab_list and lru are the same bits;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.668303370475769
union of slab_list and lru;slab_list and lru are the same bits;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.6902379989624023
union of slab_list and lru;slab_list and lru are the same bits;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.6902379989624023
union of slab_list and lru;slab_list and lru are the same bits;to be applied to the file;a file by file comparison of the scanner;Similar;0.6569240093231201
union of slab_list and lru;slab_list and lru are the same bits;For non */uapi/* files;that summary was;Similar;0.5188177824020386
union of slab_list and lru;slab_list and lru are the same bits;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.5653286576271057
union of slab_list and lru;slab_list and lru are the same bits;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5352643132209778
union of slab_list and lru;slab_list and lru are the same bits;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.6183757781982422
union of slab_list and lru;slab_list and lru are the same bits;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.554597020149231
union of slab_list and lru;slab_list and lru are the same bits;resurrects approach first proposed in [1];To fix this issue;Similar;0.5326939821243286
union of slab_list and lru;slab_list and lru are the same bits;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5074819326400757
union of slab_list and lru;slab_list and lru are the same bits;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5023791790008545
union of slab_list and lru;slab_list and lru are the same bits;unioned together;Conveniently;Similar;0.7226439714431763
union of slab_list and lru;slab_list and lru are the same bits;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.612415075302124
union of slab_list and lru;slab_list and lru are the same bits;function naming changes;requires some;Similar;0.6576933860778809
union of slab_list and lru;slab_list and lru are the same bits;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5760366320610046
union of slab_list and lru;slab_list and lru are the same bits;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.6147105693817139
union of slab_list and lru;slab_list and lru are the same bits;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.6200526356697083
union of slab_list and lru;slab_list and lru are the same bits;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.628847599029541
union of slab_list and lru;slab_list and lru are the same bits;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5518832206726074
union of slab_list and lru;slab_list and lru are the same bits;This affects RCU handling;somewhat;Similar;0.5864318013191223
union of slab_list and lru;slab_list and lru are the same bits;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5971245765686035
union of slab_list and lru;slab_list and lru are the same bits;Fix gfp flags passed to lockdep;lockdep;Similar;0.6722812652587891
union of slab_list and lru;slab_list and lru are the same bits;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6894924640655518
union of slab_list and lru;slab_list and lru are the same bits;Adding this mask;fixes the bug;Similar;0.5424546599388123
union of slab_list and lru;slab_list and lru are the same bits;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5377360582351685
union of slab_list and lru;slab_list and lru are the same bits;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6527141332626343
union of slab_list and lru;slab_list and lru are the same bits;Remove kmemtrace ftrace plugin;tracing;Similar;0.5745961666107178
union of slab_list and lru;slab_list and lru are the same bits;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.540324330329895
union of slab_list and lru;slab_list and lru are the same bits;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5135548114776611
union of slab_list and lru;slab_list and lru are the same bits;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.6353628039360046
union of slab_list and lru;slab_list and lru are the same bits;use tracepoints;kmemtrace;Similar;0.6118428707122803
union of slab_list and lru;slab_list and lru are the same bits;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6240700483322144
union of slab_list and lru;slab_list and lru are the same bits;fix typo in mm/slob.c;build fix;Similar;0.6982826590538025
union of slab_list and lru;slab_list and lru are the same bits;fix lockup in slob_free()  ;lockup;Similar;0.5161669850349426
union of slab_list and lru;slab_list and lru are the same bits;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6797393560409546
union of slab_list and lru;slab_list and lru are the same bits;enable and use this tracer;To enable and use this tracer;Similar;0.553234875202179
union of slab_list and lru;slab_list and lru are the same bits;fix bogus ksize calculation fix  ;SLOB;Similar;0.5876713991165161
union of slab_list and lru;slab_list and lru are the same bits;fix bogus ksize calculation;bogus;Similar;0.5660285949707031
union of slab_list and lru;slab_list and lru are the same bits;Handle that separately in krealloc();separately;Similar;0.5651552677154541
union of slab_list and lru;slab_list and lru are the same bits;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5864435434341431
union of slab_list and lru;slab_list and lru are the same bits;reduce list scanning;slob;Similar;0.5150020718574524
union of slab_list and lru;slab_list and lru are the same bits;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5956984758377075
union of slab_list and lru;slab_list and lru are the same bits;remove bigblock tracking;slob;Similar;0.5806306600570679
union of slab_list and lru;slab_list and lru are the same bits;rework freelist handling;slob;Similar;0.5644046068191528
union of slab_list and lru;slab_list and lru are the same bits;"align them to word size
";it is best in practice;Similar;0.6146942973136902
union of slab_list and lru;slab_list and lru are the same bits;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6400703191757202
union of slab_list and lru;slab_list and lru are the same bits;SLOB to be used on SMP  ;allows;Similar;0.6801915168762207
union of slab_list and lru;slab_list and lru are the same bits;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6009564399719238
union of slab_list and lru;slab_list and lru are the same bits;handle SLAB_PANIC flag;slob;Similar;0.6476365327835083
union of slab_list and lru;slab_list and lru are the same bits;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6463268995285034
union of slab_list and lru;slab_list and lru are the same bits;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6831709146499634
union of slab_list and lru;slab_list and lru are the same bits;Redo a lot of comments;Also;Similar;0.5298099517822266
union of slab_list and lru;slab_list and lru are the same bits;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.513411283493042
union of slab_list and lru;slab_list and lru are the same bits;simplifies SLOB;at this point slob may be broken;Similar;0.6074331998825073
union of slab_list and lru;slab_list and lru are the same bits;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5775989294052124
union of slab_list and lru;slab_list and lru are the same bits;fix;SLOB=y && SMP=y fix;Similar;0.6007424592971802
union of slab_list and lru;slab_list and lru are the same bits;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.7811172604560852
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;configure and build (select SLOB allocator);SLOB allocator;Similar;0.5915660262107849
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;to be applied to the file;a file by file comparison of the scanner;Similar;0.5109252333641052
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Similar;0.6759796142578125
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.6555116176605225
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.6632112860679626
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5208359956741333
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5768589973449707
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5275596380233765
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5408496856689453
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6796495914459229
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5297057032585144
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5234494209289551
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5425260066986084
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5486181974411011
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5468199253082275
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5208275318145752
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";Similar;0.5972146987915039
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5002305507659912
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Improve trace accuracy;by correctly tracing reported size;Similar;0.5120548009872437
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5560469031333923
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Similar;0.547082781791687
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;This will allow us to push more processing into common code later;improve readability;Similar;0.5636942386627197
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Fix early boot kernel crash;Slob;Similar;0.6261470913887024
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.703431248664856
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;cleans up some bitrot in slob.c;Also;Similar;0.6826143264770508
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Fix gfp flags passed to lockdep;lockdep;Similar;0.5130823850631714
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5322335958480835
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;refactor code for future changes;Impact;Similar;0.5168991088867188
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.591486930847168
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;fix typo in mm/slob.c;build fix;Similar;0.5350165367126465
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;fix lockup in slob_free()  ;lockup;Similar;0.63988196849823
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6561306715011597
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;enable and use this tracer;To enable and use this tracer;Similar;0.5146443843841553
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5048301219940186
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Fix to return wrong pointer;Fix;Similar;0.5172895193099976
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.6450310945510864
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;fix memory corruption;memory corruption;Similar;0.541450023651123
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5013066530227661
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5012825727462769
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;improved alignment handling;improved;Similar;0.5125809907913208
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;rework freelist handling;slob;Similar;0.5846467018127441
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5620943307876587
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5115240812301636
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6179077625274658
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6913148164749146
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5640997886657715
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5064412355422974
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.7315507531166077
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5420551300048828
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.528461217880249
configure and build (select SLOB allocator);SLOB allocator;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.5268123149871826
configure and build (select SLOB allocator);SLOB allocator;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.6811431050300598
configure and build (select SLOB allocator);SLOB allocator;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.6811431050300598
configure and build (select SLOB allocator);SLOB allocator;to be applied to the file;a file by file comparison of the scanner;Similar;0.6648275852203369
configure and build (select SLOB allocator);SLOB allocator;Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Similar;0.5346364974975586
configure and build (select SLOB allocator);SLOB allocator;the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Similar;0.5908632874488831
configure and build (select SLOB allocator);SLOB allocator;Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Similar;0.6157852411270142
configure and build (select SLOB allocator);SLOB allocator;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.6350544691085815
configure and build (select SLOB allocator);SLOB allocator;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5827858448028564
configure and build (select SLOB allocator);SLOB allocator;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.5223286151885986
configure and build (select SLOB allocator);SLOB allocator;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.6293489933013916
configure and build (select SLOB allocator);SLOB allocator;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5292041301727295
configure and build (select SLOB allocator);SLOB allocator;allow future extension of the bulk alloc API;is done to;Similar;0.6220930814743042
configure and build (select SLOB allocator);SLOB allocator;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.6031150817871094
configure and build (select SLOB allocator);SLOB allocator;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5884612798690796
configure and build (select SLOB allocator);SLOB allocator;resurrects approach first proposed in [1];To fix this issue;Similar;0.6296694278717041
configure and build (select SLOB allocator);SLOB allocator;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5588136911392212
configure and build (select SLOB allocator);SLOB allocator;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5379820466041565
configure and build (select SLOB allocator);SLOB allocator;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6991609930992126
configure and build (select SLOB allocator);SLOB allocator;unioned together;Conveniently;Similar;0.5965180397033691
configure and build (select SLOB allocator);SLOB allocator;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5305205583572388
configure and build (select SLOB allocator);SLOB allocator;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.6596624851226807
configure and build (select SLOB allocator);SLOB allocator;function naming changes;requires some;Similar;0.5997036695480347
configure and build (select SLOB allocator);SLOB allocator;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.5479301810264587
configure and build (select SLOB allocator);SLOB allocator;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5632802248001099
configure and build (select SLOB allocator);SLOB allocator;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.6523897647857666
configure and build (select SLOB allocator);SLOB allocator;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5759495496749878
configure and build (select SLOB allocator);SLOB allocator;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.7098777294158936
configure and build (select SLOB allocator);SLOB allocator;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.6680186986923218
configure and build (select SLOB allocator);SLOB allocator;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Similar;0.526237964630127
configure and build (select SLOB allocator);SLOB allocator;This will allow us to push more processing into common code later;improve readability;Similar;0.5269031524658203
configure and build (select SLOB allocator);SLOB allocator;can be done in __kmem_cache_shutdown;What is done there;Similar;0.505813717842102
configure and build (select SLOB allocator);SLOB allocator;Fix early boot kernel crash;Slob;Similar;0.6092147827148438
configure and build (select SLOB allocator);SLOB allocator;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5009596943855286
configure and build (select SLOB allocator);SLOB allocator;cleans up some bitrot in slob.c;Also;Similar;0.6611675024032593
configure and build (select SLOB allocator);SLOB allocator;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6118650436401367
configure and build (select SLOB allocator);SLOB allocator;Fix gfp flags passed to lockdep;lockdep;Similar;0.6857932209968567
configure and build (select SLOB allocator);SLOB allocator;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.7434679865837097
configure and build (select SLOB allocator);SLOB allocator;Adding this mask;fixes the bug;Similar;0.5602813363075256
configure and build (select SLOB allocator);SLOB allocator;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.569746732711792
configure and build (select SLOB allocator);SLOB allocator;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6735976934432983
configure and build (select SLOB allocator);SLOB allocator;Remove kmemtrace ftrace plugin;tracing;Similar;0.5512946844100952
configure and build (select SLOB allocator);SLOB allocator;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5772038698196411
configure and build (select SLOB allocator);SLOB allocator;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5017713308334351
configure and build (select SLOB allocator);SLOB allocator;refactor code for future changes;Impact;Similar;0.580309271812439
configure and build (select SLOB allocator);SLOB allocator;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.6310551762580872
configure and build (select SLOB allocator);SLOB allocator;use tracepoints;kmemtrace;Similar;0.600719690322876
configure and build (select SLOB allocator);SLOB allocator;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.739021897315979
configure and build (select SLOB allocator);SLOB allocator;fix typo in mm/slob.c;build fix;Similar;0.7165273427963257
configure and build (select SLOB allocator);SLOB allocator;fix lockup in slob_free()  ;lockup;Similar;0.6139823198318481
configure and build (select SLOB allocator);SLOB allocator;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.7211351990699768
configure and build (select SLOB allocator);SLOB allocator;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6945236921310425
configure and build (select SLOB allocator);SLOB allocator;enable and use this tracer;To enable and use this tracer;Similar;0.6274281740188599
configure and build (select SLOB allocator);SLOB allocator;fix bogus ksize calculation fix  ;SLOB;Similar;0.6326037645339966
configure and build (select SLOB allocator);SLOB allocator;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5191830396652222
configure and build (select SLOB allocator);SLOB allocator;fix bogus ksize calculation;bogus;Similar;0.5927190184593201
configure and build (select SLOB allocator);SLOB allocator;Fix to return wrong pointer;Fix;Similar;0.5279918909072876
configure and build (select SLOB allocator);SLOB allocator;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6593188047409058
configure and build (select SLOB allocator);SLOB allocator;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5373467206954956
configure and build (select SLOB allocator);SLOB allocator;remove bigblock tracking;slob;Similar;0.5127293467521667
configure and build (select SLOB allocator);SLOB allocator;rework freelist handling;slob;Similar;0.6320794820785522
configure and build (select SLOB allocator);SLOB allocator;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6709755659103394
configure and build (select SLOB allocator);SLOB allocator;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6050053834915161
configure and build (select SLOB allocator);SLOB allocator;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5630944967269897
configure and build (select SLOB allocator);SLOB allocator;SLOB to be used on SMP  ;allows;Similar;0.6873977184295654
configure and build (select SLOB allocator);SLOB allocator;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5902540683746338
configure and build (select SLOB allocator);SLOB allocator;handle SLAB_PANIC flag;slob;Similar;0.511208176612854
configure and build (select SLOB allocator);SLOB allocator;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6563273668289185
configure and build (select SLOB allocator);SLOB allocator;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5707933902740479
configure and build (select SLOB allocator);SLOB allocator;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5667897462844849
configure and build (select SLOB allocator);SLOB allocator;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6878450512886047
configure and build (select SLOB allocator);SLOB allocator;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.6010829210281372
configure and build (select SLOB allocator);SLOB allocator;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.600722074508667
configure and build (select SLOB allocator);SLOB allocator;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5012238025665283
configure and build (select SLOB allocator);SLOB allocator;simplifies SLOB;at this point slob may be broken;Similar;0.5869659781455994
configure and build (select SLOB allocator);SLOB allocator;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5637948513031006
configure and build (select SLOB allocator);SLOB allocator;fix;SLOB=y && SMP=y fix;Similar;0.5439224243164062
configure and build (select SLOB allocator);SLOB allocator;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.665316104888916
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.6707460880279541
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.6707460880279541
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.6077195405960083
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.611028790473938
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Similar;0.5736491680145264
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;to be applied to the file;a file by file comparison of the scanner;Similar;0.5332392454147339
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;For non */uapi/* files;that summary was;Similar;0.5883711576461792
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.565618634223938
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.657731294631958
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Similar;0.5298523902893066
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.7198964953422546
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.6470879316329956
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.6494024991989136
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.5059247612953186
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5637661218643188
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5696499347686768
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5168236494064331
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5189359188079834
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.6968460083007812
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5108109712600708
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5765407085418701
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.6684633493423462
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5877701044082642
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5309357643127441
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;function naming changes;requires some;Similar;0.6364619731903076
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.6960031986236572
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.6667706370353699
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.6246781349182129
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5756689310073853
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5413830280303955
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.6213735938072205
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5991561412811279
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6252622604370117
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Remove various small accessors;various small;Similar;0.5000820159912109
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5940508842468262
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Fix gfp flags passed to lockdep;lockdep;Similar;0.6226543188095093
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6148585677146912
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5327023267745972
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6435028314590454
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5990985035896301
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Remove kmemtrace ftrace plugin;tracing;Similar;0.6708921194076538
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5798497200012207
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6295544505119324
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5422500371932983
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.5360395908355713
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5591417551040649
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6662288904190063
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;fix typo in mm/slob.c;build fix;Similar;0.6144253015518188
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;fix lockup in slob_free()  ;lockup;Similar;0.5557250380516052
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6795953512191772
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5059680938720703
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;fix bogus ksize calculation fix  ;SLOB;Similar;0.576208770275116
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;fix bogus ksize calculation;bogus;Similar;0.5705171823501587
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5700938105583191
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Fix to return wrong pointer;Fix;Similar;0.5700591802597046
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.552453875541687
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5120525360107422
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;remove useless ctor parameter and reorder parameters;useless;Similar;0.6003181338310242
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Handle that separately in krealloc();separately;Similar;0.6218564510345459
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.539402961730957
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;reduce list scanning;slob;Similar;0.6526892185211182
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5379951596260071
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Cleanup zeroing allocations;Slab allocators;Similar;0.5487707853317261
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.568366289138794
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5682156682014465
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.6103200912475586
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;remove bigblock tracking;slob;Similar;0.6219015121459961
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5228081941604614
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;rework freelist handling;slob;Similar;0.5195928812026978
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6415998339653015
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6572425365447998
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5421837568283081
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5259938836097717
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;SLOB to be used on SMP  ;allows;Similar;0.5529043078422546
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;fix page order calculation on not 4KB page;-;Similar;0.6704926490783691
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6883998513221741
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.7366145849227905
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;handle SLAB_PANIC flag;slob;Similar;0.5590133666992188
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5640064477920532
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5902630686759949
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6598867774009705
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5100053548812866
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6368227005004883
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5496617555618286
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.7121062278747559
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6568436622619629
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6690064072608948
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6223280429840088
Add a return parameter to slob_page_alloc();to signal that the list was modified;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;1.000000238418579
Add a return parameter to slob_page_alloc();to signal that the list was modified;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.50041264295578
Add a return parameter to slob_page_alloc();to signal that the list was modified;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.5642217397689819
Add a return parameter to slob_page_alloc();to signal that the list was modified;to be applied to the file;a file by file comparison of the scanner;Similar;0.7039384841918945
Add a return parameter to slob_page_alloc();to signal that the list was modified;the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Similar;0.5021131038665771
Add a return parameter to slob_page_alloc();to signal that the list was modified;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.5228046774864197
Add a return parameter to slob_page_alloc();to signal that the list was modified;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.7163746356964111
Add a return parameter to slob_page_alloc();to signal that the list was modified;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.6445112228393555
Add a return parameter to slob_page_alloc();to signal that the list was modified;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.6547980904579163
Add a return parameter to slob_page_alloc();to signal that the list was modified;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.6045841574668884
Add a return parameter to slob_page_alloc();to signal that the list was modified;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.7261785864830017
Add a return parameter to slob_page_alloc();to signal that the list was modified;allow future extension of the bulk alloc API;is done to;Similar;0.597191333770752
Add a return parameter to slob_page_alloc();to signal that the list was modified;We always end up returning some objects at the cost of another cmpxchg  ;Else;Similar;0.5635759830474854
Add a return parameter to slob_page_alloc();to signal that the list was modified;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.5900484323501587
Add a return parameter to slob_page_alloc();to signal that the list was modified;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.6210423707962036
Add a return parameter to slob_page_alloc();to signal that the list was modified;resurrects approach first proposed in [1];To fix this issue;Similar;0.623359739780426
Add a return parameter to slob_page_alloc();to signal that the list was modified;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.61058509349823
Add a return parameter to slob_page_alloc();to signal that the list was modified;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6790286302566528
Add a return parameter to slob_page_alloc();to signal that the list was modified;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5138911604881287
Add a return parameter to slob_page_alloc();to signal that the list was modified;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5126157999038696
Add a return parameter to slob_page_alloc();to signal that the list was modified;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.7346421480178833
Add a return parameter to slob_page_alloc();to signal that the list was modified;function naming changes;requires some;Similar;0.6882790923118591
Add a return parameter to slob_page_alloc();to signal that the list was modified;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.5024211406707764
Add a return parameter to slob_page_alloc();to signal that the list was modified;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.7834230661392212
Add a return parameter to slob_page_alloc();to signal that the list was modified;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.8003391027450562
Add a return parameter to slob_page_alloc();to signal that the list was modified;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.6454377174377441
Add a return parameter to slob_page_alloc();to signal that the list was modified;Improve trace accuracy;by correctly tracing reported size;Similar;0.500524640083313
Add a return parameter to slob_page_alloc();to signal that the list was modified;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.7396184802055359
Add a return parameter to slob_page_alloc();to signal that the list was modified;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.8592509031295776
Add a return parameter to slob_page_alloc();to signal that the list was modified;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.6500966548919678
Add a return parameter to slob_page_alloc();to signal that the list was modified;This will allow us to push more processing into common code later;improve readability;Similar;0.5072833299636841
Add a return parameter to slob_page_alloc();to signal that the list was modified;can be done in __kmem_cache_shutdown;What is done there;Similar;0.623148500919342
Add a return parameter to slob_page_alloc();to signal that the list was modified;This affects RCU handling;somewhat;Similar;0.504331648349762
Add a return parameter to slob_page_alloc();to signal that the list was modified;Fix early boot kernel crash;Slob;Similar;0.5247019529342651
Add a return parameter to slob_page_alloc();to signal that the list was modified;cleans up some bitrot in slob.c;Also;Similar;0.5469684600830078
Add a return parameter to slob_page_alloc();to signal that the list was modified;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6747435331344604
Add a return parameter to slob_page_alloc();to signal that the list was modified;Fix gfp flags passed to lockdep;lockdep;Similar;0.7081780433654785
Add a return parameter to slob_page_alloc();to signal that the list was modified;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.7555752396583557
Add a return parameter to slob_page_alloc();to signal that the list was modified;Adding this mask;fixes the bug;Similar;0.5877610445022583
Add a return parameter to slob_page_alloc();to signal that the list was modified;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.6783036589622498
Add a return parameter to slob_page_alloc();to signal that the list was modified;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.8490380048751831
Add a return parameter to slob_page_alloc();to signal that the list was modified;Remove kmemtrace ftrace plugin;tracing;Similar;0.6047312021255493
Add a return parameter to slob_page_alloc();to signal that the list was modified;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5011853575706482
Add a return parameter to slob_page_alloc();to signal that the list was modified;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6619674563407898
Add a return parameter to slob_page_alloc();to signal that the list was modified;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.576615035533905
Add a return parameter to slob_page_alloc();to signal that the list was modified;refactor code for future changes;Impact;Similar;0.6182957887649536
Add a return parameter to slob_page_alloc();to signal that the list was modified;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.699397087097168
Add a return parameter to slob_page_alloc();to signal that the list was modified;use tracepoints;kmemtrace;Similar;0.6734787225723267
Add a return parameter to slob_page_alloc();to signal that the list was modified;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.7502491474151611
Add a return parameter to slob_page_alloc();to signal that the list was modified;fix typo in mm/slob.c;build fix;Similar;0.7475081086158752
Add a return parameter to slob_page_alloc();to signal that the list was modified;fix lockup in slob_free()  ;lockup;Similar;0.6355692148208618
Add a return parameter to slob_page_alloc();to signal that the list was modified;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.7506104707717896
Add a return parameter to slob_page_alloc();to signal that the list was modified;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6399459838867188
Add a return parameter to slob_page_alloc();to signal that the list was modified;enable and use this tracer;To enable and use this tracer;Similar;0.6515660285949707
Add a return parameter to slob_page_alloc();to signal that the list was modified;fix bogus ksize calculation fix  ;SLOB;Similar;0.6074475049972534
Add a return parameter to slob_page_alloc();to signal that the list was modified;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5299488306045532
Add a return parameter to slob_page_alloc();to signal that the list was modified;fix bogus ksize calculation;bogus;Similar;0.5913935303688049
Add a return parameter to slob_page_alloc();to signal that the list was modified;record page flag overlays explicitly;slob;Similar;0.5428271889686584
Add a return parameter to slob_page_alloc();to signal that the list was modified;Fix to return wrong pointer;Fix;Similar;0.6396950483322144
Add a return parameter to slob_page_alloc();to signal that the list was modified;reduce external fragmentation by using three free lists;slob;Similar;0.5284719467163086
Add a return parameter to slob_page_alloc();to signal that the list was modified;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5125788450241089
Add a return parameter to slob_page_alloc();to signal that the list was modified;fix memory corruption;memory corruption;Similar;0.5027524828910828
Add a return parameter to slob_page_alloc();to signal that the list was modified;Handle that separately in krealloc();separately;Similar;0.5880396366119385
Add a return parameter to slob_page_alloc();to signal that the list was modified;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6795361042022705
Add a return parameter to slob_page_alloc();to signal that the list was modified;reduce list scanning;slob;Similar;0.5340101718902588
Add a return parameter to slob_page_alloc();to signal that the list was modified;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.6072414517402649
Add a return parameter to slob_page_alloc();to signal that the list was modified;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5441619157791138
Add a return parameter to slob_page_alloc();to signal that the list was modified;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.6772886514663696
Add a return parameter to slob_page_alloc();to signal that the list was modified;remove bigblock tracking;slob;Similar;0.5914146900177002
Add a return parameter to slob_page_alloc();to signal that the list was modified;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.6056356430053711
Add a return parameter to slob_page_alloc();to signal that the list was modified;rework freelist handling;slob;Similar;0.6324640512466431
Add a return parameter to slob_page_alloc();to signal that the list was modified;"align them to word size
";it is best in practice;Similar;0.610041618347168
Add a return parameter to slob_page_alloc();to signal that the list was modified;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.7120730876922607
Add a return parameter to slob_page_alloc();to signal that the list was modified;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6056467294692993
Add a return parameter to slob_page_alloc();to signal that the list was modified;SLOB to be used on SMP  ;allows;Similar;0.6869815587997437
Add a return parameter to slob_page_alloc();to signal that the list was modified;fix page order calculation on not 4KB page;-;Similar;0.5839492678642273
Add a return parameter to slob_page_alloc();to signal that the list was modified;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6395737528800964
Add a return parameter to slob_page_alloc();to signal that the list was modified;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6025968790054321
Add a return parameter to slob_page_alloc();to signal that the list was modified;handle SLAB_PANIC flag;slob;Similar;0.6309922933578491
Add a return parameter to slob_page_alloc();to signal that the list was modified;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.690975546836853
Add a return parameter to slob_page_alloc();to signal that the list was modified;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.587226390838623
Add a return parameter to slob_page_alloc();to signal that the list was modified;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6770105361938477
Add a return parameter to slob_page_alloc();to signal that the list was modified;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6090400815010071
Add a return parameter to slob_page_alloc();to signal that the list was modified;Redo a lot of comments;Also;Similar;0.54097580909729
Add a return parameter to slob_page_alloc();to signal that the list was modified;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6227401494979858
Add a return parameter to slob_page_alloc();to signal that the list was modified;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6150684952735901
Add a return parameter to slob_page_alloc();to signal that the list was modified;simplifies SLOB;at this point slob may be broken;Similar;0.5926529169082642
Add a return parameter to slob_page_alloc();to signal that the list was modified;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5543317794799805
Add a return parameter to slob_page_alloc();to signal that the list was modified;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6847695112228394
Add a return parameter to slob_page_alloc();to signal that the list was modified;fix;SLOB=y && SMP=y fix;Similar;0.5440207719802856
Add a return parameter to slob_page_alloc();to signal that the list was modified;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.7378389835357666
Add a return parameter to slob_page_alloc();to signal that the list was modified;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5512997508049011
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;remove an unnecessary check for __GFP_ZERO;unnecessary;Similar;0.50041264295578
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.5642217397689819
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;to be applied to the file;a file by file comparison of the scanner;Similar;0.7039384841918945
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Similar;0.5021131038665771
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.5228046774864197
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.7163746356964111
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.6445112228393555
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.6547980904579163
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.6045841574668884
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.7261785864830017
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;allow future extension of the bulk alloc API;is done to;Similar;0.597191333770752
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;We always end up returning some objects at the cost of another cmpxchg  ;Else;Similar;0.5635759830474854
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.5900484323501587
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.6210423707962036
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;resurrects approach first proposed in [1];To fix this issue;Similar;0.623359739780426
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.61058509349823
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6790286302566528
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5138911604881287
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5126157999038696
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.7346421480178833
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;function naming changes;requires some;Similar;0.6882790923118591
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.5024211406707764
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.7834230661392212
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.8003391027450562
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.6454377174377441
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Improve trace accuracy;by correctly tracing reported size;Similar;0.500524640083313
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.7396184802055359
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.8592509031295776
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.6500966548919678
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;This will allow us to push more processing into common code later;improve readability;Similar;0.5072833299636841
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;can be done in __kmem_cache_shutdown;What is done there;Similar;0.623148500919342
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;This affects RCU handling;somewhat;Similar;0.504331648349762
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Fix early boot kernel crash;Slob;Similar;0.5247019529342651
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;cleans up some bitrot in slob.c;Also;Similar;0.5469684600830078
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6747435331344604
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Fix gfp flags passed to lockdep;lockdep;Similar;0.7081780433654785
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.7555752396583557
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Adding this mask;fixes the bug;Similar;0.5877610445022583
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.6783036589622498
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.8490380048751831
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Remove kmemtrace ftrace plugin;tracing;Similar;0.6047312021255493
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5011853575706482
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6619674563407898
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.576615035533905
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;refactor code for future changes;Impact;Similar;0.6182957887649536
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.699397087097168
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;use tracepoints;kmemtrace;Similar;0.6734787225723267
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.7502491474151611
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;fix typo in mm/slob.c;build fix;Similar;0.7475081086158752
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;fix lockup in slob_free()  ;lockup;Similar;0.6355692148208618
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.7506104707717896
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6399459838867188
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;enable and use this tracer;To enable and use this tracer;Similar;0.6515660285949707
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;fix bogus ksize calculation fix  ;SLOB;Similar;0.6074475049972534
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5299488306045532
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;fix bogus ksize calculation;bogus;Similar;0.5913935303688049
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;record page flag overlays explicitly;slob;Similar;0.5428271889686584
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Fix to return wrong pointer;Fix;Similar;0.6396950483322144
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;reduce external fragmentation by using three free lists;slob;Similar;0.5284719467163086
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5125788450241089
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;fix memory corruption;memory corruption;Similar;0.5027524828910828
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Handle that separately in krealloc();separately;Similar;0.5880396366119385
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6795361042022705
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;reduce list scanning;slob;Similar;0.5340101718902588
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.6072414517402649
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5441619157791138
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.6772886514663696
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;remove bigblock tracking;slob;Similar;0.5914146900177002
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.6056356430053711
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;rework freelist handling;slob;Similar;0.6324640512466431
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;"align them to word size
";it is best in practice;Similar;0.610041618347168
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.7120730876922607
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6056467294692993
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;SLOB to be used on SMP  ;allows;Similar;0.6869815587997437
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;fix page order calculation on not 4KB page;-;Similar;0.5839492678642273
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6395737528800964
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6025968790054321
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;handle SLAB_PANIC flag;slob;Similar;0.6309922933578491
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.690975546836853
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.587226390838623
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6770105361938477
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6090400815010071
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Redo a lot of comments;Also;Similar;0.54097580909729
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6227401494979858
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6150684952735901
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;simplifies SLOB;at this point slob may be broken;Similar;0.5926529169082642
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5543317794799805
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6847695112228394
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;fix;SLOB=y && SMP=y fix;Similar;0.5440207719802856
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.7378389835357666
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5512997508049011
remove an unnecessary check for __GFP_ZERO;unnecessary;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.8974013328552246
remove an unnecessary check for __GFP_ZERO;unnecessary;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Similar;0.5128580927848816
remove an unnecessary check for __GFP_ZERO;unnecessary;All documentation files were explicitly excluded;explicitly excluded;Similar;0.5746960639953613
remove an unnecessary check for __GFP_ZERO;unnecessary;For non */uapi/* files;that summary was;Similar;0.6310036182403564
remove an unnecessary check for __GFP_ZERO;unnecessary;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.7683179378509521
remove an unnecessary check for __GFP_ZERO;unnecessary;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5934939980506897
remove an unnecessary check for __GFP_ZERO;unnecessary;SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Similar;0.6603608727455139
remove an unnecessary check for __GFP_ZERO;unnecessary;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.6535376310348511
remove an unnecessary check for __GFP_ZERO;unnecessary;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.7475936412811279
remove an unnecessary check for __GFP_ZERO;unnecessary;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5363187193870544
remove an unnecessary check for __GFP_ZERO;unnecessary;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5633777379989624
remove an unnecessary check for __GFP_ZERO;unnecessary;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.509846568107605
remove an unnecessary check for __GFP_ZERO;unnecessary;slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Similar;0.5190033316612244
remove an unnecessary check for __GFP_ZERO;unnecessary;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Similar;0.6578526496887207
remove an unnecessary check for __GFP_ZERO;unnecessary;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5006596446037292
remove an unnecessary check for __GFP_ZERO;unnecessary;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.686974048614502
remove an unnecessary check for __GFP_ZERO;unnecessary;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5333385467529297
remove an unnecessary check for __GFP_ZERO;unnecessary;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.6408412456512451
remove an unnecessary check for __GFP_ZERO;unnecessary;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.7600481510162354
remove an unnecessary check for __GFP_ZERO;unnecessary;function naming changes;requires some;Similar;0.5037582516670227
remove an unnecessary check for __GFP_ZERO;unnecessary;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.5096169114112854
remove an unnecessary check for __GFP_ZERO;unnecessary;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5263828039169312
remove an unnecessary check for __GFP_ZERO;unnecessary;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5222327709197998
remove an unnecessary check for __GFP_ZERO;unnecessary;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.592211127281189
remove an unnecessary check for __GFP_ZERO;unnecessary;Improve trace accuracy;by correctly tracing reported size;Similar;0.5288683176040649
remove an unnecessary check for __GFP_ZERO;unnecessary;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5027115345001221
remove an unnecessary check for __GFP_ZERO;unnecessary;can be done in __kmem_cache_shutdown;What is done there;Similar;0.7086449861526489
remove an unnecessary check for __GFP_ZERO;unnecessary;Remove various small accessors;various small;Similar;0.7018819451332092
remove an unnecessary check for __GFP_ZERO;unnecessary;They are no longer needed;They have become so simple;Similar;0.6909642219543457
remove an unnecessary check for __GFP_ZERO;unnecessary;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.7260681986808777
remove an unnecessary check for __GFP_ZERO;unnecessary;Fix gfp flags passed to lockdep;lockdep;Similar;0.5478019714355469
remove an unnecessary check for __GFP_ZERO;unnecessary;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5365457534790039
remove an unnecessary check for __GFP_ZERO;unnecessary;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5719244480133057
remove an unnecessary check for __GFP_ZERO;unnecessary;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.6675572395324707
remove an unnecessary check for __GFP_ZERO;unnecessary;Remove kmemtrace ftrace plugin;tracing;Similar;0.7234839200973511
remove an unnecessary check for __GFP_ZERO;unnecessary;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.7307645082473755
remove an unnecessary check for __GFP_ZERO;unnecessary;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6801496744155884
remove an unnecessary check for __GFP_ZERO;unnecessary;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5744917988777161
remove an unnecessary check for __GFP_ZERO;unnecessary;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.6657941341400146
remove an unnecessary check for __GFP_ZERO;unnecessary;use tracepoints;kmemtrace;Similar;0.5457373857498169
remove an unnecessary check for __GFP_ZERO;unnecessary;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6126872301101685
remove an unnecessary check for __GFP_ZERO;unnecessary;fix typo in mm/slob.c;build fix;Similar;0.5789296627044678
remove an unnecessary check for __GFP_ZERO;unnecessary;fix lockup in slob_free()  ;lockup;Similar;0.6329864263534546
remove an unnecessary check for __GFP_ZERO;unnecessary;Drop it  ;if you want;Similar;0.697813868522644
remove an unnecessary check for __GFP_ZERO;unnecessary;fix bogus ksize calculation fix  ;SLOB;Similar;0.7207964658737183
remove an unnecessary check for __GFP_ZERO;unnecessary;fix bogus ksize calculation;bogus;Similar;0.7491651773452759
remove an unnecessary check for __GFP_ZERO;unnecessary;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.7279804944992065
remove an unnecessary check for __GFP_ZERO;unnecessary;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.6051010489463806
remove an unnecessary check for __GFP_ZERO;unnecessary;Fix to return wrong pointer;Fix;Similar;0.6881599426269531
remove an unnecessary check for __GFP_ZERO;unnecessary;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5438058972358704
remove an unnecessary check for __GFP_ZERO;unnecessary;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5921183824539185
remove an unnecessary check for __GFP_ZERO;unnecessary;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.6931140422821045
remove an unnecessary check for __GFP_ZERO;unnecessary;fix memory corruption;memory corruption;Similar;0.589945912361145
remove an unnecessary check for __GFP_ZERO;unnecessary;remove useless ctor parameter and reorder parameters;useless;Similar;0.8305707573890686
remove an unnecessary check for __GFP_ZERO;unnecessary;Handle that separately in krealloc();separately;Similar;0.6655770540237427
remove an unnecessary check for __GFP_ZERO;unnecessary;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5339587926864624
remove an unnecessary check for __GFP_ZERO;unnecessary;reduce list scanning;slob;Similar;0.823667049407959
remove an unnecessary check for __GFP_ZERO;unnecessary;Cleanup zeroing allocations;Slab allocators;Similar;0.8183853626251221
remove an unnecessary check for __GFP_ZERO;unnecessary;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.6195696592330933
remove an unnecessary check for __GFP_ZERO;unnecessary;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6815654039382935
remove an unnecessary check for __GFP_ZERO;unnecessary;remove bigblock tracking;slob;Similar;0.7080926895141602
remove an unnecessary check for __GFP_ZERO;unnecessary;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6631468534469604
remove an unnecessary check for __GFP_ZERO;unnecessary;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5143671035766602
remove an unnecessary check for __GFP_ZERO;unnecessary;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5943173170089722
remove an unnecessary check for __GFP_ZERO;unnecessary;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5720975399017334
remove an unnecessary check for __GFP_ZERO;unnecessary;SLOB to be used on SMP  ;allows;Similar;0.5000594258308411
remove an unnecessary check for __GFP_ZERO;unnecessary;fix page order calculation on not 4KB page;-;Similar;0.6751394867897034
remove an unnecessary check for __GFP_ZERO;unnecessary;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6115116477012634
remove an unnecessary check for __GFP_ZERO;unnecessary;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.791232705116272
remove an unnecessary check for __GFP_ZERO;unnecessary;handle SLAB_PANIC flag;slob;Similar;0.6179497241973877
remove an unnecessary check for __GFP_ZERO;unnecessary;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5989490747451782
remove an unnecessary check for __GFP_ZERO;unnecessary;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6831283569335938
remove an unnecessary check for __GFP_ZERO;unnecessary;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5156317949295044
remove an unnecessary check for __GFP_ZERO;unnecessary;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5961012840270996
remove an unnecessary check for __GFP_ZERO;unnecessary;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.5769836902618408
remove an unnecessary check for __GFP_ZERO;unnecessary;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5559982061386108
remove an unnecessary check for __GFP_ZERO;unnecessary;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5547109842300415
remove an unnecessary check for __GFP_ZERO;unnecessary;simplifies SLOB;at this point slob may be broken;Similar;0.5752663612365723
remove an unnecessary check for __GFP_ZERO;unnecessary;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5605607032775879
remove an unnecessary check for __GFP_ZERO;unnecessary;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.717842698097229
remove an unnecessary check for __GFP_ZERO;unnecessary;fix;SLOB=y && SMP=y fix;Similar;0.5931389331817627
Remove the unnecessary NULL pointer check;unnecessary;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Similar;0.5339720845222473
Remove the unnecessary NULL pointer check;unnecessary;All documentation files were explicitly excluded;explicitly excluded;Similar;0.5103761553764343
Remove the unnecessary NULL pointer check;unnecessary;For non */uapi/* files;that summary was;Similar;0.5725460052490234
Remove the unnecessary NULL pointer check;unnecessary;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.7260054349899292
Remove the unnecessary NULL pointer check;unnecessary;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.6739793419837952
Remove the unnecessary NULL pointer check;unnecessary;SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Similar;0.6164946556091309
Remove the unnecessary NULL pointer check;unnecessary;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.6578749418258667
Remove the unnecessary NULL pointer check;unnecessary;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.5168241858482361
Remove the unnecessary NULL pointer check;unnecessary;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.7394582629203796
Remove the unnecessary NULL pointer check;unnecessary;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5962507724761963
Remove the unnecessary NULL pointer check;unnecessary;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.594342827796936
Remove the unnecessary NULL pointer check;unnecessary;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5971946716308594
Remove the unnecessary NULL pointer check;unnecessary;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5479724407196045
Remove the unnecessary NULL pointer check;unnecessary;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Similar;0.5795172452926636
Remove the unnecessary NULL pointer check;unnecessary;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.6740585565567017
Remove the unnecessary NULL pointer check;unnecessary;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5725389719009399
Remove the unnecessary NULL pointer check;unnecessary;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.6981891393661499
Remove the unnecessary NULL pointer check;unnecessary;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.7417685985565186
Remove the unnecessary NULL pointer check;unnecessary;function naming changes;requires some;Similar;0.5679200887680054
Remove the unnecessary NULL pointer check;unnecessary;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5454699397087097
Remove the unnecessary NULL pointer check;unnecessary;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.573798418045044
Remove the unnecessary NULL pointer check;unnecessary;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.6789097189903259
Remove the unnecessary NULL pointer check;unnecessary;Improve trace accuracy;by correctly tracing reported size;Similar;0.5500319004058838
Remove the unnecessary NULL pointer check;unnecessary;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5627977848052979
Remove the unnecessary NULL pointer check;unnecessary;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6851198077201843
Remove the unnecessary NULL pointer check;unnecessary;This affects RCU handling;somewhat;Similar;0.517155647277832
Remove the unnecessary NULL pointer check;unnecessary;Fix early boot kernel crash;Slob;Similar;0.5265980362892151
Remove the unnecessary NULL pointer check;unnecessary;Remove various small accessors;various small;Similar;0.6894997358322144
Remove the unnecessary NULL pointer check;unnecessary;They are no longer needed;They have become so simple;Similar;0.6348831653594971
Remove the unnecessary NULL pointer check;unnecessary;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.6320329904556274
Remove the unnecessary NULL pointer check;unnecessary;cleans up some bitrot in slob.c;Also;Similar;0.5191415548324585
Remove the unnecessary NULL pointer check;unnecessary;Fix gfp flags passed to lockdep;lockdep;Similar;0.5695712566375732
Remove the unnecessary NULL pointer check;unnecessary;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5294700860977173
Remove the unnecessary NULL pointer check;unnecessary;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6006217002868652
Remove the unnecessary NULL pointer check;unnecessary;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.6012541651725769
Remove the unnecessary NULL pointer check;unnecessary;Remove kmemtrace ftrace plugin;tracing;Similar;0.7331014275550842
Remove the unnecessary NULL pointer check;unnecessary;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.6800308227539062
Remove the unnecessary NULL pointer check;unnecessary;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.7155568599700928
Remove the unnecessary NULL pointer check;unnecessary;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5302466154098511
Remove the unnecessary NULL pointer check;unnecessary;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.6935755014419556
Remove the unnecessary NULL pointer check;unnecessary;use tracepoints;kmemtrace;Similar;0.5498484373092651
Remove the unnecessary NULL pointer check;unnecessary;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6553299427032471
Remove the unnecessary NULL pointer check;unnecessary;fix typo in mm/slob.c;build fix;Similar;0.5855263471603394
Remove the unnecessary NULL pointer check;unnecessary;fix lockup in slob_free()  ;lockup;Similar;0.6665470600128174
Remove the unnecessary NULL pointer check;unnecessary;enable and use this tracer;To enable and use this tracer;Similar;0.5124858617782593
Remove the unnecessary NULL pointer check;unnecessary;Drop it  ;if you want;Similar;0.6386237740516663
Remove the unnecessary NULL pointer check;unnecessary;fix bogus ksize calculation fix  ;SLOB;Similar;0.7248209714889526
Remove the unnecessary NULL pointer check;unnecessary;fix bogus ksize calculation;bogus;Similar;0.7447758913040161
Remove the unnecessary NULL pointer check;unnecessary;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.6926271915435791
Remove the unnecessary NULL pointer check;unnecessary;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5914560556411743
Remove the unnecessary NULL pointer check;unnecessary;Fix to return wrong pointer;Fix;Similar;0.7862646579742432
Remove the unnecessary NULL pointer check;unnecessary;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5691502690315247
Remove the unnecessary NULL pointer check;unnecessary;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5810002684593201
Remove the unnecessary NULL pointer check;unnecessary;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.6562849283218384
Remove the unnecessary NULL pointer check;unnecessary;fix memory corruption;memory corruption;Similar;0.6723047494888306
Remove the unnecessary NULL pointer check;unnecessary;remove useless ctor parameter and reorder parameters;useless;Similar;0.8053261041641235
Remove the unnecessary NULL pointer check;unnecessary;Handle that separately in krealloc();separately;Similar;0.6110317707061768
Remove the unnecessary NULL pointer check;unnecessary;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5487412214279175
Remove the unnecessary NULL pointer check;unnecessary;reduce list scanning;slob;Similar;0.7878323197364807
Remove the unnecessary NULL pointer check;unnecessary;Cleanup zeroing allocations;Slab allocators;Similar;0.7823262810707092
Remove the unnecessary NULL pointer check;unnecessary;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5685111880302429
Remove the unnecessary NULL pointer check;unnecessary;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.7095568776130676
Remove the unnecessary NULL pointer check;unnecessary;remove bigblock tracking;slob;Similar;0.7042820453643799
Remove the unnecessary NULL pointer check;unnecessary;rework freelist handling;slob;Similar;0.5646607875823975
Remove the unnecessary NULL pointer check;unnecessary;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6061257123947144
Remove the unnecessary NULL pointer check;unnecessary;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5234380960464478
Remove the unnecessary NULL pointer check;unnecessary;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.647987425327301
Remove the unnecessary NULL pointer check;unnecessary;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.6000534296035767
Remove the unnecessary NULL pointer check;unnecessary;SLOB to be used on SMP  ;allows;Similar;0.5182269811630249
Remove the unnecessary NULL pointer check;unnecessary;fix page order calculation on not 4KB page;-;Similar;0.6510617136955261
Remove the unnecessary NULL pointer check;unnecessary;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6709120869636536
Remove the unnecessary NULL pointer check;unnecessary;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.7717999219894409
Remove the unnecessary NULL pointer check;unnecessary;handle SLAB_PANIC flag;slob;Similar;0.6207945942878723
Remove the unnecessary NULL pointer check;unnecessary;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.628173828125
Remove the unnecessary NULL pointer check;unnecessary;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5149664878845215
Remove the unnecessary NULL pointer check;unnecessary;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6633340120315552
Remove the unnecessary NULL pointer check;unnecessary;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6226305961608887
Remove the unnecessary NULL pointer check;unnecessary;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5097250938415527
Remove the unnecessary NULL pointer check;unnecessary;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6126230955123901
Remove the unnecessary NULL pointer check;unnecessary;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.5039135217666626
Remove the unnecessary NULL pointer check;unnecessary;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5922037363052368
Remove the unnecessary NULL pointer check;unnecessary;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5913273096084595
Remove the unnecessary NULL pointer check;unnecessary;simplifies SLOB;at this point slob may be broken;Similar;0.5884262323379517
Remove the unnecessary NULL pointer check;unnecessary;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5383414626121521
Remove the unnecessary NULL pointer check;unnecessary;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6914058923721313
Remove the unnecessary NULL pointer check;unnecessary;fix;SLOB=y && SMP=y fix;Similar;0.5940742492675781
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;All documentation files were explicitly excluded;explicitly excluded;Similar;0.5749573111534119
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;For non */uapi/* files;that summary was;Similar;0.5789141654968262
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Kate also obtained a third independent scan of the 4.13 code base from FOSSology, and compared selected files where the other two scanners disagreed against that SPDX file;to see if there was new insights;Similar;0.5226749181747437
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Similar;0.5479569435119629
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.5322365760803223
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.6165775060653687
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Similar;0.5227512121200562
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.5048051476478577
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5908828973770142
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5786003470420837
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5765344500541687
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5575382709503174
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5557454824447632
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.652936577796936
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.6554710865020752
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.5732202529907227
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.6052242517471313
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5316447019577026
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.6698563098907471
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5457199215888977
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5007926225662231
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5650945901870728
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Fix gfp flags passed to lockdep;lockdep;Similar;0.5464808940887451
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5716004967689514
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5040371417999268
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Remove kmemtrace ftrace plugin;tracing;Similar;0.5767817497253418
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5332289934158325
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5756922960281372
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.5241073369979858
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5715569853782654
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5272660851478577
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;fix lockup in slob_free()  ;lockup;Similar;0.5106299519538879
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5059528350830078
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5757164359092712
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5437551140785217
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5106277465820312
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;remove useless ctor parameter and reorder parameters;useless;Similar;0.5686821937561035
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5061043500900269
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.552665114402771
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5875662565231323
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6331759691238403
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5909829139709473
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5135657787322998
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;fix page order calculation on not 4KB page;-;Similar;0.5427011847496033
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.543998122215271
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5369933843612671
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5148993730545044
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6457840800285339
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5059772729873657
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6130221486091614
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.650434672832489
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5950329303741455
to be applied to the file;a file by file comparison of the scanner;Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Similar;0.5288208723068237
to be applied to the file;a file by file comparison of the scanner;the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Similar;0.6618026494979858
to be applied to the file;a file by file comparison of the scanner;For non */uapi/* files;that summary was;Similar;0.5266318321228027
to be applied to the file;a file by file comparison of the scanner;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.5267206430435181
to be applied to the file;a file by file comparison of the scanner;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.5257068276405334
to be applied to the file;a file by file comparison of the scanner;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.631553053855896
to be applied to the file;a file by file comparison of the scanner;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.61397784948349
to be applied to the file;a file by file comparison of the scanner;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.5200743079185486
to be applied to the file;a file by file comparison of the scanner;allow future extension of the bulk alloc API;is done to;Similar;0.5470052361488342
to be applied to the file;a file by file comparison of the scanner;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5085703134536743
to be applied to the file;a file by file comparison of the scanner;resurrects approach first proposed in [1];To fix this issue;Similar;0.651323676109314
to be applied to the file;a file by file comparison of the scanner;slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Similar;0.5375008583068848
to be applied to the file;a file by file comparison of the scanner;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6068357229232788
to be applied to the file;a file by file comparison of the scanner;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5962598323822021
to be applied to the file;a file by file comparison of the scanner;unioned together;Conveniently;Similar;0.7008150815963745
to be applied to the file;a file by file comparison of the scanner;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.6473886966705322
to be applied to the file;a file by file comparison of the scanner;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.7151471972465515
to be applied to the file;a file by file comparison of the scanner;function naming changes;requires some;Similar;0.7046183347702026
to be applied to the file;a file by file comparison of the scanner;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5448756217956543
to be applied to the file;a file by file comparison of the scanner;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.660793662071228
to be applied to the file;a file by file comparison of the scanner;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5721619129180908
to be applied to the file;a file by file comparison of the scanner;Improve trace accuracy;by correctly tracing reported size;Similar;0.5755761861801147
to be applied to the file;a file by file comparison of the scanner;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.615235447883606
to be applied to the file;a file by file comparison of the scanner;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.6398366689682007
to be applied to the file;a file by file comparison of the scanner;This will allow us to push more processing into common code later;improve readability;Similar;0.5175759196281433
to be applied to the file;a file by file comparison of the scanner;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5538415908813477
to be applied to the file;a file by file comparison of the scanner;This affects RCU handling;somewhat;Similar;0.6532486081123352
to be applied to the file;a file by file comparison of the scanner;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.5125082731246948
to be applied to the file;a file by file comparison of the scanner;Fix early boot kernel crash;Slob;Similar;0.5114814639091492
to be applied to the file;a file by file comparison of the scanner;Remove various small accessors;various small;Similar;0.5112513899803162
to be applied to the file;a file by file comparison of the scanner;cleans up some bitrot in slob.c;Also;Similar;0.5267959237098694
to be applied to the file;a file by file comparison of the scanner;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6053215861320496
to be applied to the file;a file by file comparison of the scanner;Fix gfp flags passed to lockdep;lockdep;Similar;0.652214527130127
to be applied to the file;a file by file comparison of the scanner;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.671485185623169
to be applied to the file;a file by file comparison of the scanner;Adding this mask;fixes the bug;Similar;0.5993578433990479
to be applied to the file;a file by file comparison of the scanner;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.6019370555877686
to be applied to the file;a file by file comparison of the scanner;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6884725093841553
to be applied to the file;a file by file comparison of the scanner;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5075256824493408
to be applied to the file;a file by file comparison of the scanner;refactor code for future changes;Impact;Similar;0.5967425107955933
to be applied to the file;a file by file comparison of the scanner;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5897146463394165
to be applied to the file;a file by file comparison of the scanner;use tracepoints;kmemtrace;Similar;0.6720758080482483
to be applied to the file;a file by file comparison of the scanner;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5845130681991577
to be applied to the file;a file by file comparison of the scanner;fix typo in mm/slob.c;build fix;Similar;0.6816953420639038
to be applied to the file;a file by file comparison of the scanner;fix lockup in slob_free()  ;lockup;Similar;0.5183554887771606
to be applied to the file;a file by file comparison of the scanner;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5957568883895874
to be applied to the file;a file by file comparison of the scanner;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5763059258460999
to be applied to the file;a file by file comparison of the scanner;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5595180988311768
to be applied to the file;a file by file comparison of the scanner;enable and use this tracer;To enable and use this tracer;Similar;0.7267080545425415
to be applied to the file;a file by file comparison of the scanner;I find it more readable  ;personal opinion;Similar;0.5736578702926636
to be applied to the file;a file by file comparison of the scanner;fix bogus ksize calculation fix  ;SLOB;Similar;0.5730804204940796
to be applied to the file;a file by file comparison of the scanner;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.6118974685668945
to be applied to the file;a file by file comparison of the scanner;fix bogus ksize calculation;bogus;Similar;0.5665148496627808
to be applied to the file;a file by file comparison of the scanner;record page flag overlays explicitly;slob;Similar;0.6453625559806824
to be applied to the file;a file by file comparison of the scanner;Fix to return wrong pointer;Fix;Similar;0.5077593326568604
to be applied to the file;a file by file comparison of the scanner;fix memory corruption;memory corruption;Similar;0.5397577285766602
to be applied to the file;a file by file comparison of the scanner;Handle that separately in krealloc();separately;Similar;0.5395018458366394
to be applied to the file;a file by file comparison of the scanner;reduce list scanning;slob;Similar;0.5412724018096924
to be applied to the file;a file by file comparison of the scanner;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5419880151748657
to be applied to the file;a file by file comparison of the scanner;improved alignment handling;improved;Similar;0.5708552598953247
to be applied to the file;a file by file comparison of the scanner;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5529135465621948
to be applied to the file;a file by file comparison of the scanner;remove bigblock tracking;slob;Similar;0.5364532470703125
to be applied to the file;a file by file comparison of the scanner;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5920822024345398
to be applied to the file;a file by file comparison of the scanner;rework freelist handling;slob;Similar;0.6691150665283203
to be applied to the file;a file by file comparison of the scanner;"align them to word size
";it is best in practice;Similar;0.7070972919464111
to be applied to the file;a file by file comparison of the scanner;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5358774065971375
to be applied to the file;a file by file comparison of the scanner;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6619366407394409
to be applied to the file;a file by file comparison of the scanner;SLOB to be used on SMP  ;allows;Similar;0.674010157585144
to be applied to the file;a file by file comparison of the scanner;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5205840468406677
to be applied to the file;a file by file comparison of the scanner;handle SLAB_PANIC flag;slob;Similar;0.530839204788208
to be applied to the file;a file by file comparison of the scanner;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6614569425582886
to be applied to the file;a file by file comparison of the scanner;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5551610589027405
to be applied to the file;a file by file comparison of the scanner;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6222601532936096
to be applied to the file;a file by file comparison of the scanner;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5071659088134766
to be applied to the file;a file by file comparison of the scanner;Redo a lot of comments;Also;Similar;0.6060069799423218
to be applied to the file;a file by file comparison of the scanner;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.612758994102478
to be applied to the file;a file by file comparison of the scanner;simplifies SLOB;at this point slob may be broken;Similar;0.6009213924407959
to be applied to the file;a file by file comparison of the scanner;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5325527787208557
to be applied to the file;a file by file comparison of the scanner;fix;SLOB=y && SMP=y fix;Similar;0.6620909571647644
to be applied to the file;a file by file comparison of the scanner;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6725977659225464
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Similar;0.5619889497756958
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);allow future extension of the bulk alloc API;is done to;Similar;0.5565686225891113
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.576669454574585
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.527954638004303
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5376263856887817
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5139753818511963
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5161654949188232
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5171982049942017
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Fix gfp flags passed to lockdep;lockdep;Similar;0.5336635708808899
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.543940544128418
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.6060328483581543
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5148283243179321
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.511337399482727
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5151784420013428
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5033708810806274
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5829017162322998
All documentation files were explicitly excluded;explicitly excluded;For non */uapi/* files;that summary was;Similar;0.5955321788787842
All documentation files were explicitly excluded;explicitly excluded;SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Similar;0.5647518634796143
All documentation files were explicitly excluded;explicitly excluded;Make dead caches discard free slabs immediately;slub;Similar;0.5365808010101318
All documentation files were explicitly excluded;explicitly excluded;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Similar;0.500971257686615
All documentation files were explicitly excluded;explicitly excluded;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.6342830061912537
All documentation files were explicitly excluded;explicitly excluded;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.5867919921875
All documentation files were explicitly excluded;explicitly excluded;They are no longer needed;They have become so simple;Similar;0.5330123901367188
All documentation files were explicitly excluded;explicitly excluded;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.6805858612060547
All documentation files were explicitly excluded;explicitly excluded;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.6327570676803589
All documentation files were explicitly excluded;explicitly excluded;Remove kmemtrace ftrace plugin;tracing;Similar;0.5209167003631592
All documentation files were explicitly excluded;explicitly excluded;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5358521938323975
All documentation files were explicitly excluded;explicitly excluded;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.6425607800483704
All documentation files were explicitly excluded;explicitly excluded;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5730528235435486
All documentation files were explicitly excluded;explicitly excluded;remove useless ctor parameter and reorder parameters;useless;Similar;0.6973730325698853
All documentation files were explicitly excluded;explicitly excluded;reduce list scanning;slob;Similar;0.5214868187904358
All documentation files were explicitly excluded;explicitly excluded;Cleanup zeroing allocations;Slab allocators;Similar;0.605339765548706
All documentation files were explicitly excluded;explicitly excluded;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5654071569442749
All documentation files were explicitly excluded;explicitly excluded;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5056269764900208
All documentation files were explicitly excluded;explicitly excluded;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6287888288497925
All documentation files were explicitly excluded;explicitly excluded;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5131295919418335
All documentation files were explicitly excluded;explicitly excluded;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.541224479675293
All documentation files were explicitly excluded;explicitly excluded;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5181694030761719
All documentation files were explicitly excluded;explicitly excluded;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.6456131935119629
All documentation files were explicitly excluded;explicitly excluded;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5278466939926147
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Similar;0.5160764455795288
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.5427862405776978
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.5160665512084961
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5632002949714661
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;unioned together;Conveniently;Similar;0.510644793510437
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.6473450660705566
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.5589475631713867
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5245004892349243
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Similar;0.548919677734375
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Fix early boot kernel crash;Slob;Similar;0.5136673450469971
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5402852296829224
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Fix gfp flags passed to lockdep;lockdep;Similar;0.5689430832862854
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5124326944351196
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.519679844379425
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.517746090888977
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5071843862533569
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5033454298973083
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;enable and use this tracer;To enable and use this tracer;Similar;0.510015070438385
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;record page flag overlays explicitly;slob;Similar;0.6308825612068176
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.537246584892273
For non */uapi/* files;that summary was;reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Similar;0.6474288702011108
For non */uapi/* files;that summary was;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5466210842132568
For non */uapi/* files;that summary was;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.6353719830513
For non */uapi/* files;that summary was;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5260815620422363
For non */uapi/* files;that summary was;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Similar;0.5973393321037292
For non */uapi/* files;that summary was;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.6082271337509155
For non */uapi/* files;that summary was;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.6185179948806763
For non */uapi/* files;that summary was;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6158641576766968
For non */uapi/* files;that summary was;Remove various small accessors;various small;Similar;0.6343571543693542
For non */uapi/* files;that summary was;They are no longer needed;They have become so simple;Similar;0.5420849323272705
For non */uapi/* files;that summary was;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.5930423736572266
For non */uapi/* files;that summary was;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5276331901550293
For non */uapi/* files;that summary was;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.6606708765029907
For non */uapi/* files;that summary was;Remove kmemtrace ftrace plugin;tracing;Similar;0.6108402013778687
For non */uapi/* files;that summary was;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.6426236629486084
For non */uapi/* files;that summary was;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5007439255714417
For non */uapi/* files;that summary was;use tracepoints;kmemtrace;Similar;0.5089272856712341
For non */uapi/* files;that summary was;fix typo in mm/slob.c;build fix;Similar;0.5537587404251099
For non */uapi/* files;that summary was;fix lockup in slob_free()  ;lockup;Similar;0.5601414442062378
For non */uapi/* files;that summary was;Drop it  ;if you want;Similar;0.6627412438392639
For non */uapi/* files;that summary was;fix bogus ksize calculation;bogus;Similar;0.5133868455886841
For non */uapi/* files;that summary was;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.6096739768981934
For non */uapi/* files;that summary was;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.6014354825019836
For non */uapi/* files;that summary was;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5132389068603516
For non */uapi/* files;that summary was;remove useless ctor parameter and reorder parameters;useless;Similar;0.6489133834838867
For non */uapi/* files;that summary was;Handle that separately in krealloc();separately;Similar;0.7143223285675049
For non */uapi/* files;that summary was;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5092947483062744
For non */uapi/* files;that summary was;reduce list scanning;slob;Similar;0.6664782762527466
For non */uapi/* files;that summary was;Cleanup zeroing allocations;Slab allocators;Similar;0.6424546241760254
For non */uapi/* files;that summary was;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5064516067504883
For non */uapi/* files;that summary was;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5885167717933655
For non */uapi/* files;that summary was;rework freelist handling;slob;Similar;0.5511808395385742
For non */uapi/* files;that summary was;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6887398958206177
For non */uapi/* files;that summary was;SLOB to be used on SMP  ;allows;Similar;0.533855676651001
For non */uapi/* files;that summary was;fix page order calculation on not 4KB page;-;Similar;0.6441317796707153
For non */uapi/* files;that summary was;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.574576735496521
For non */uapi/* files;that summary was;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5682214498519897
For non */uapi/* files;that summary was;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5144115686416626
For non */uapi/* files;that summary was;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5859895944595337
For non */uapi/* files;that summary was;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.5834856033325195
For non */uapi/* files;that summary was;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6298536062240601
For non */uapi/* files;that summary was;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6366672515869141
For non */uapi/* files;that summary was;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5135111808776855
Kate also obtained a third independent scan of the 4.13 code base from FOSSology, and compared selected files where the other two scanners disagreed against that SPDX file;to see if there was new insights;Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Similar;0.5898628830909729
Kate also obtained a third independent scan of the 4.13 code base from FOSSology, and compared selected files where the other two scanners disagreed against that SPDX file;to see if there was new insights;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.529729425907135
Kate also obtained a third independent scan of the 4.13 code base from FOSSology, and compared selected files where the other two scanners disagreed against that SPDX file;to see if there was new insights;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5163270235061646
Kate also obtained a third independent scan of the 4.13 code base from FOSSology, and compared selected files where the other two scanners disagreed against that SPDX file;to see if there was new insights;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5032609701156616
Kate also obtained a third independent scan of the 4.13 code base from FOSSology, and compared selected files where the other two scanners disagreed against that SPDX file;to see if there was new insights;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5195083618164062
Kate also obtained a third independent scan of the 4.13 code base from FOSSology, and compared selected files where the other two scanners disagreed against that SPDX file;to see if there was new insights;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5265729427337646
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.854971170425415
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5149235725402832
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.6961278915405273
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.526931881904602
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5367583632469177
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5534667372703552
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5694854259490967
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5031651258468628
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5222242474555969
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5455184578895569
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5876830816268921
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5666621923446655
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.557712972164154
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Fix early boot kernel crash;Slob;Similar;0.5325916409492493
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.6862059235572815
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;cleans up some bitrot in slob.c;Also;Similar;0.6040445566177368
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6175843477249146
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Fix gfp flags passed to lockdep;lockdep;Similar;0.5457384586334229
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5806411504745483
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5754748582839966
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5144701600074768
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5722106695175171
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6972291469573975
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5912355184555054
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.505798876285553
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5480542778968811
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5198344588279724
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5131577253341675
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5082758069038391
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.5101531744003296
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.5886052846908569
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5236196517944336
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5339577198028564
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5606504678726196
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5233482122421265
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5092711448669434
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5458700060844421
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5148874521255493
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.54282146692276
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5076568126678467
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5666443109512329
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Fix early boot kernel crash;Slob;Similar;0.5629189014434814
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.6616191864013672
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;cleans up some bitrot in slob.c;Also;Similar;0.5969113111495972
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5880188941955566
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Fix gfp flags passed to lockdep;lockdep;Similar;0.546926736831665
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5518007278442383
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5243661403656006
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5406672954559326
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5437520742416382
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5556185245513916
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6915855407714844
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.6038094758987427
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5291299223899841
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5314722657203674
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5075787305831909
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.551639199256897
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.6072088479995728
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Similar;0.6695268154144287
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.5962187051773071
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.525495171546936
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.7749050855636597
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5303802490234375
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;resurrects approach first proposed in [1];To fix this issue;Similar;0.5139678716659546
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Similar;0.6742427349090576
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5857261419296265
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.6467577219009399
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.6312998533248901
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.7139275074005127
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;unioned together;Conveniently;Similar;0.5377583503723145
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;function naming changes;requires some;Similar;0.6461432576179504
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5378979444503784
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5463603734970093
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Improve trace accuracy;by correctly tracing reported size;Similar;0.5291104316711426
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;can be done in __kmem_cache_shutdown;What is done there;Similar;0.7376055717468262
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;This affects RCU handling;somewhat;Similar;0.638709545135498
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Remove various small accessors;various small;Similar;0.8317462205886841
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;They are no longer needed;They have become so simple;Similar;0.7631567716598511
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.6185008883476257
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Fix gfp flags passed to lockdep;lockdep;Similar;0.6100633144378662
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5544222593307495
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Adding this mask;fixes the bug;Similar;0.5291063785552979
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5195082426071167
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6010792255401611
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5084384679794312
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5927619934082031
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Remove kmemtrace ftrace plugin;tracing;Similar;0.7699634432792664
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.8668506145477295
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5587747693061829
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;refactor code for future changes;Impact;Similar;0.5035912990570068
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;use tracepoints;kmemtrace;Similar;0.5663416385650635
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.561126708984375
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;fix typo in mm/slob.c;build fix;Similar;0.6185775995254517
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;fix lockup in slob_free()  ;lockup;Similar;0.5902696847915649
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5594015121459961
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;enable and use this tracer;To enable and use this tracer;Similar;0.5088744759559631
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;I find it more readable  ;personal opinion;Similar;0.5265426635742188
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Drop it  ;if you want;Similar;0.804094672203064
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;fix bogus ksize calculation fix  ;SLOB;Similar;0.6793144345283508
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5501687526702881
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;fix bogus ksize calculation;bogus;Similar;0.7189890146255493
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.7885946035385132
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5186785459518433
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Fix to return wrong pointer;Fix;Similar;0.6082656383514404
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;reduce external fragmentation by using three free lists;slob;Similar;0.5105624198913574
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.6079776883125305
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.752700924873352
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;fix memory corruption;memory corruption;Similar;0.5344007611274719
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;remove useless ctor parameter and reorder parameters;useless;Similar;0.7635419368743896
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Handle that separately in krealloc();separately;Similar;0.7156859636306763
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5322943329811096
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;reduce list scanning;slob;Similar;0.9132909774780273
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Cleanup zeroing allocations;Slab allocators;Similar;0.7954673767089844
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;remove bigblock tracking;slob;Similar;0.7491610050201416
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;rework freelist handling;slob;Similar;0.5895060896873474
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5972825884819031
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;"align them to word size
";it is best in practice;Similar;0.5063695907592773
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5538104772567749
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.547754168510437
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;SLOB to be used on SMP  ;allows;Similar;0.6453796029090881
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;fix page order calculation on not 4KB page;-;Similar;0.624319314956665
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5831769704818726
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.715296745300293
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;handle SLAB_PANIC flag;slob;Similar;0.5973138809204102
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5762412548065186
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5266756415367126
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5228579640388489
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5312212109565735
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5875656604766846
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Redo a lot of comments;Also;Similar;0.5599042177200317
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5337589979171753
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;simplifies SLOB;at this point slob may be broken;Similar;0.6564541459083557
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6520282030105591
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;fix;SLOB=y && SMP=y fix;Similar;0.6834818124771118
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5294634103775024
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Similar;0.5027058124542236
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.600773811340332
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Similar;0.6253572702407837
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.7044610381126404
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.7177588939666748
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;allow future extension of the bulk alloc API;is done to;Similar;0.5124883651733398
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.5339276790618896
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;hiding potentially buggy callers;except temporarily;Similar;0.504043459892273
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5544956922531128
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5700540542602539
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.6371101140975952
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;resurrects approach first proposed in [1];To fix this issue;Similar;0.5687771439552307
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5833379030227661
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5726853609085083
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.52530437707901
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.6630693674087524
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6077377200126648
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5368537902832031
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5995328426361084
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.6434422731399536
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5104301571846008
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5792385339736938
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;function naming changes;requires some;Similar;0.708432137966156
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.6653611660003662
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.7130295038223267
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5626460909843445
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.6562231779098511
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.555078387260437
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.6526030898094177
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.6286500096321106
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;This will allow us to push more processing into common code later;improve readability;Similar;0.5584580898284912
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;can be done in __kmem_cache_shutdown;What is done there;Similar;0.7291111946105957
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;This affects RCU handling;somewhat;Similar;0.5847041606903076
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Fix early boot kernel crash;Slob;Similar;0.5243121385574341
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Remove various small accessors;various small;Similar;0.6311782598495483
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;cleans up some bitrot in slob.c;Also;Similar;0.6235383749008179
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6347687244415283
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Fix gfp flags passed to lockdep;lockdep;Similar;0.7140703797340393
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6687031984329224
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Adding this mask;fixes the bug;Similar;0.5666829347610474
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5761412382125854
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.7568875551223755
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5532817244529724
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Remove kmemtrace ftrace plugin;tracing;Similar;0.7109355926513672
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.6544516086578369
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6466145515441895
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5019214153289795
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.5327537059783936
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;refactor code for future changes;Impact;Similar;0.587385356426239
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5398563146591187
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;use tracepoints;kmemtrace;Similar;0.5638071298599243
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6566543579101562
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;fix typo in mm/slob.c;build fix;Similar;0.6593664288520813
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;fix lockup in slob_free()  ;lockup;Similar;0.6383090019226074
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6604974865913391
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6294543743133545
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;enable and use this tracer;To enable and use this tracer;Similar;0.5600922107696533
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Drop it  ;if you want;Similar;0.5621122717857361
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;fix bogus ksize calculation fix  ;SLOB;Similar;0.5980974435806274
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5525332689285278
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;fix bogus ksize calculation;bogus;Similar;0.6137322187423706
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.6306846737861633
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Fix to return wrong pointer;Fix;Similar;0.6552574634552002
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;reduce external fragmentation by using three free lists;slob;Similar;0.5034133195877075
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5503019690513611
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5391479730606079
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;fix memory corruption;memory corruption;Similar;0.5659538507461548
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;remove useless ctor parameter and reorder parameters;useless;Similar;0.5966547727584839
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Handle that separately in krealloc();separately;Similar;0.6890881657600403
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6203597187995911
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;reduce list scanning;slob;Similar;0.6726326942443848
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.6394685506820679
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Cleanup zeroing allocations;Slab allocators;Similar;0.5587576627731323
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6021145582199097
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5232264995574951
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;remove bigblock tracking;slob;Similar;0.6376492977142334
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5785885453224182
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;rework freelist handling;slob;Similar;0.6170607805252075
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5857594609260559
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6811179518699646
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6229895949363708
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;SLOB to be used on SMP  ;allows;Similar;0.6228970289230347
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;fix page order calculation on not 4KB page;-;Similar;0.5952622294425964
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6931444406509399
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6266704797744751
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;handle SLAB_PANIC flag;slob;Similar;0.526823103427887
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6055059432983398
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5871737599372864
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5042035579681396
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5934823751449585
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5798143148422241
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6753724813461304
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Redo a lot of comments;Also;Similar;0.505334734916687
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6031079292297363
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6499731540679932
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;simplifies SLOB;at this point slob may be broken;Similar;0.5529552102088928
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.528215765953064
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7100319862365723
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;fix;SLOB=y && SMP=y fix;Similar;0.5057346224784851
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5673972368240356
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5158355236053467
"reported chasing a bug
";their assumption that SLAB_DESTROY_BY_RCU provided an existence guarantee;hiding potentially buggy callers;except temporarily;Similar;0.5946202278137207
"reported chasing a bug
";their assumption that SLAB_DESTROY_BY_RCU provided an existence guarantee;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5960708856582642
"reported chasing a bug
";their assumption that SLAB_DESTROY_BY_RCU provided an existence guarantee;Improve trace accuracy;by correctly tracing reported size;Similar;0.5340497493743896
"reported chasing a bug
";their assumption that SLAB_DESTROY_BY_RCU provided an existence guarantee;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.501227617263794
"reported chasing a bug
";their assumption that SLAB_DESTROY_BY_RCU provided an existence guarantee;use tracepoints;kmemtrace;Similar;0.5222259163856506
"reported chasing a bug
";their assumption that SLAB_DESTROY_BY_RCU provided an existence guarantee;enable and use this tracer;To enable and use this tracer;Similar;0.5533224940299988
"reported chasing a bug
";their assumption that SLAB_DESTROY_BY_RCU provided an existence guarantee;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6688166856765747
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Similar;0.6903823018074036
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.5536261796951294
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.6045030355453491
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Make dead caches discard free slabs immediately;slub;Similar;0.6244279742240906
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5905836820602417
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5687102675437927
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5034629106521606
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.6095030903816223
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.6647528409957886
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.6190727949142456
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.5346429944038391
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;Similar;0.5115781426429749
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5048032999038696
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5707194805145264
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.5698992013931274
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Remove kmemtrace ftrace plugin;tracing;Similar;0.5364643931388855
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6401715278625488
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5651253461837769
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.7603754997253418
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5601485967636108
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5017188787460327
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;remove useless ctor parameter and reorder parameters;useless;Similar;0.6405352354049683
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;reduce list scanning;slob;Similar;0.5076169371604919
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.583777904510498
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Cleanup zeroing allocations;Slab allocators;Similar;0.5742539167404175
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.623440146446228
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6901932954788208
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;remove bigblock tracking;slob;Similar;0.5722752809524536
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5406108498573303
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.7362749576568604
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.620372474193573
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;handle SLAB_PANIC flag;slob;Similar;0.5148627758026123
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5614246129989624
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6052179336547852
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5502679944038391
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6197774410247803
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5841915607452393
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.7003133296966553
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5268986225128174
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5411778092384338
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.6329612731933594
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5908939838409424
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.6317168474197388
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.6372686624526978
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.7363445162773132
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;function naming changes;requires some;Similar;0.5977530479431152
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.739937424659729
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.7562132477760315
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5580652952194214
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5584734082221985
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5937336087226868
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.6551538705825806
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;can be done in __kmem_cache_shutdown;What is done there;Similar;0.7196677923202515
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Fix early boot kernel crash;Slob;Similar;0.5170962810516357
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5810821056365967
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Fix gfp flags passed to lockdep;lockdep;Similar;0.603744387626648
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5666561126708984
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Adding this mask;fixes the bug;Similar;0.5121325850486755
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5068092346191406
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5866026878356934
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5215897560119629
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Remove kmemtrace ftrace plugin;tracing;Similar;0.7491255402565002
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.634178638458252
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.8136236667633057
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.6437488794326782
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5200896263122559
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6841446757316589
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;fix typo in mm/slob.c;build fix;Similar;0.6196120977401733
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6394640803337097
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Drop it  ;if you want;Similar;0.5073683261871338
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;fix bogus ksize calculation fix  ;SLOB;Similar;0.5708510875701904
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;fix bogus ksize calculation;bogus;Similar;0.5645551681518555
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.554817795753479
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Fix to return wrong pointer;Fix;Similar;0.5831683278083801
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;remove useless ctor parameter and reorder parameters;useless;Similar;0.666167140007019
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Handle that separately in krealloc();separately;Similar;0.5239267349243164
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;reduce list scanning;slob;Similar;0.628897488117218
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5657760500907898
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Cleanup zeroing allocations;Slab allocators;Similar;0.5851383209228516
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.553297221660614
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6544957160949707
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5165843963623047
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;remove bigblock tracking;slob;Similar;0.7436137199401855
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6473826169967651
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.612838864326477
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.7706257104873657
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;SLOB to be used on SMP  ;allows;Similar;0.5207643508911133
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;fix page order calculation on not 4KB page;-;Similar;0.5167064666748047
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5530313849449158
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.821791410446167
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;handle SLAB_PANIC flag;slob;Similar;0.7714790105819702
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.6384912729263306
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5088513493537903
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6795074343681335
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.662264883518219
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5742090940475464
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6979203224182129
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5569188594818115
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7061631679534912
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5183390378952026
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.508334219455719
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Similar;0.614316463470459
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.6076922416687012
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;We always end up returning some objects at the cost of another cmpxchg  ;Else;Similar;0.5449764728546143
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5501554012298584
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;resurrects approach first proposed in [1];To fix this issue;Similar;0.7448348999023438
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5651800632476807
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6486903429031372
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.6276087760925293
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;unioned together;Conveniently;Similar;0.619331419467926
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.6268258094787598
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.6359041929244995
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;function naming changes;requires some;Similar;0.5767119526863098
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.6893887519836426
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5287637114524841
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5954208970069885
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Improve trace accuracy;by correctly tracing reported size;Similar;0.54812091588974
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.6005157828330994
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.6832597255706787
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.598811686038971
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Similar;0.5553758144378662
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;This will allow us to push more processing into common code later;improve readability;Similar;0.5171658992767334
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6177167296409607
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;This affects RCU handling;somewhat;Similar;0.520411491394043
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.6068928837776184
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Fix early boot kernel crash;Slob;Similar;0.5455516576766968
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Remove various small accessors;various small;Similar;0.5519580841064453
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;cleans up some bitrot in slob.c;Also;Similar;0.5360046625137329
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5010703206062317
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Fix gfp flags passed to lockdep;lockdep;Similar;0.6027302742004395
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.657159686088562
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Adding this mask;fixes the bug;Similar;0.5983074903488159
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5636306405067444
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.7255163192749023
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.6296752691268921
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5365651845932007
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Remove kmemtrace ftrace plugin;tracing;Similar;0.560078501701355
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5658104419708252
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5010845065116882
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5944732427597046
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;refactor code for future changes;Impact;Similar;0.5588224530220032
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;use tracepoints;kmemtrace;Similar;0.681869387626648
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6133134365081787
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;fix typo in mm/slob.c;build fix;Similar;0.6339815855026245
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;fix lockup in slob_free()  ;lockup;Similar;0.6665226221084595
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.531731903553009
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5698727369308472
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;enable and use this tracer;To enable and use this tracer;Similar;0.6963076591491699
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Drop it  ;if you want;Similar;0.54925537109375
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;fix bogus ksize calculation fix  ;SLOB;Similar;0.6294604539871216
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.6544381976127625
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;fix bogus ksize calculation;bogus;Similar;0.6397691965103149
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.6210479736328125
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Fix to return wrong pointer;Fix;Similar;0.595017671585083
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.532547116279602
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;fix memory corruption;memory corruption;Similar;0.5128458738327026
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Handle that separately in krealloc();separately;Similar;0.5722814798355103
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5392839312553406
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;reduce list scanning;slob;Similar;0.5024574995040894
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.6119914054870605
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5431259870529175
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;remove bigblock tracking;slob;Similar;0.5514585375785828
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;rework freelist handling;slob;Similar;0.6582156419754028
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;"align them to word size
";it is best in practice;Similar;0.5593230128288269
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5832006335258484
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5600804686546326
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;SLOB to be used on SMP  ;allows;Similar;0.6141860485076904
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;fix page order calculation on not 4KB page;-;Similar;0.5322741270065308
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5603617429733276
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.520972490310669
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;handle SLAB_PANIC flag;slob;Similar;0.5398761034011841
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6830511093139648
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5912848114967346
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5476875305175781
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;simplifies SLOB;at this point slob may be broken;Similar;0.5840464234352112
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6366326808929443
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;fix;SLOB=y && SMP=y fix;Similar;0.6407036185264587
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5150701403617859
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Adjust API to return type int instead of previously type bool;previously type bool;Similar;0.5912683010101318
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5033177137374878
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;resurrects approach first proposed in [1];To fix this issue;Similar;0.5696994066238403
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5267060995101929
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Similar;0.5857926607131958
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Similar;0.6319360136985779
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5768193006515503
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.6249651908874512
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5868895053863525
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5679295063018799
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.805903434753418
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;function naming changes;requires some;Similar;0.6064428091049194
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5521895885467529
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.6095432043075562
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5100804567337036
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5227539539337158
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5135207176208496
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5433517694473267
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5487527847290039
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;can be done in __kmem_cache_shutdown;What is done there;Similar;0.7456822395324707
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;This affects RCU handling;somewhat;Similar;0.5506969094276428
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Remove various small accessors;various small;Similar;0.7303742170333862
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;They are no longer needed;They have become so simple;Similar;0.6612844467163086
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.6241625547409058
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Fix gfp flags passed to lockdep;lockdep;Similar;0.5291348695755005
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.611218273639679
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Adding this mask;fixes the bug;Similar;0.6339989900588989
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6362950801849365
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5496037602424622
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.614778459072113
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Remove kmemtrace ftrace plugin;tracing;Similar;0.7734609842300415
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.8209226131439209
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6064316034317017
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.5398198366165161
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;use tracepoints;kmemtrace;Similar;0.5618069171905518
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5863447189331055
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;fix typo in mm/slob.c;build fix;Similar;0.6057062149047852
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;fix lockup in slob_free()  ;lockup;Similar;0.6254674196243286
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5393176078796387
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5030546188354492
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;enable and use this tracer;To enable and use this tracer;Similar;0.5203584432601929
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Drop it  ;if you want;Similar;0.796398401260376
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;fix bogus ksize calculation fix  ;SLOB;Similar;0.6688966155052185
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5120657682418823
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;fix bogus ksize calculation;bogus;Similar;0.6994096040725708
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.7091729640960693
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.571359395980835
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Fix to return wrong pointer;Fix;Similar;0.6330381631851196
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5407314300537109
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.6293669939041138
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;fix memory corruption;memory corruption;Similar;0.5133572220802307
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;remove useless ctor parameter and reorder parameters;useless;Similar;0.7287633419036865
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Handle that separately in krealloc();separately;Similar;0.7262412309646606
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5233258008956909
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;reduce list scanning;slob;Similar;0.7759580612182617
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5263655781745911
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Cleanup zeroing allocations;Slab allocators;Similar;0.7106128931045532
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5078167915344238
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5591527223587036
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5145597457885742
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;remove bigblock tracking;slob;Similar;0.7768382430076599
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;rework freelist handling;slob;Similar;0.5321104526519775
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6232221722602844
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;"align them to word size
";it is best in practice;Similar;0.5126029253005981
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6103559732437134
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.561285674571991
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5125089883804321
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;SLOB to be used on SMP  ;allows;Similar;0.6075280904769897
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;fix page order calculation on not 4KB page;-;Similar;0.6503301858901978
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5779178142547607
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.8013726472854614
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;handle SLAB_PANIC flag;slob;Similar;0.720707893371582
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6415386199951172
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.517969012260437
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5384538173675537
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6186918020248413
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6014580726623535
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6134624481201172
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5794443488121033
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;simplifies SLOB;at this point slob may be broken;Similar;0.619053840637207
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7118191123008728
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;fix;SLOB=y && SMP=y fix;Similar;0.6628404855728149
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5301263332366943
Adjust API to return type int instead of previously type bool;previously type bool;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.6011348962783813
Adjust API to return type int instead of previously type bool;previously type bool;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5532478094100952
Adjust API to return type int instead of previously type bool;previously type bool;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.5427165031433105
Adjust API to return type int instead of previously type bool;previously type bool;resurrects approach first proposed in [1];To fix this issue;Similar;0.5801883935928345
Adjust API to return type int instead of previously type bool;previously type bool;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5737684965133667
Adjust API to return type int instead of previously type bool;previously type bool;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.5401479601860046
Adjust API to return type int instead of previously type bool;previously type bool;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5939159989356995
Adjust API to return type int instead of previously type bool;previously type bool;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5196197032928467
Adjust API to return type int instead of previously type bool;previously type bool;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.50655597448349
Adjust API to return type int instead of previously type bool;previously type bool;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5460055470466614
Adjust API to return type int instead of previously type bool;previously type bool;function naming changes;requires some;Similar;0.6322258114814758
Adjust API to return type int instead of previously type bool;previously type bool;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.6777040362358093
Adjust API to return type int instead of previously type bool;previously type bool;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.6257174015045166
Adjust API to return type int instead of previously type bool;previously type bool;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.576957106590271
Adjust API to return type int instead of previously type bool;previously type bool;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5436165928840637
Adjust API to return type int instead of previously type bool;previously type bool;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5857492685317993
Adjust API to return type int instead of previously type bool;previously type bool;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.6687290668487549
Adjust API to return type int instead of previously type bool;previously type bool;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5619114637374878
Adjust API to return type int instead of previously type bool;previously type bool;This will allow us to push more processing into common code later;improve readability;Similar;0.5520484447479248
Adjust API to return type int instead of previously type bool;previously type bool;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5230122208595276
Adjust API to return type int instead of previously type bool;previously type bool;Fix early boot kernel crash;Slob;Similar;0.5206259489059448
Adjust API to return type int instead of previously type bool;previously type bool;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5941847562789917
Adjust API to return type int instead of previously type bool;previously type bool;Fix gfp flags passed to lockdep;lockdep;Similar;0.5873331427574158
Adjust API to return type int instead of previously type bool;previously type bool;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6428729295730591
Adjust API to return type int instead of previously type bool;previously type bool;Adding this mask;fixes the bug;Similar;0.5001561641693115
Adjust API to return type int instead of previously type bool;previously type bool;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5706226825714111
Adjust API to return type int instead of previously type bool;previously type bool;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6662360429763794
Adjust API to return type int instead of previously type bool;previously type bool;Remove kmemtrace ftrace plugin;tracing;Similar;0.5648149251937866
Adjust API to return type int instead of previously type bool;previously type bool;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5397599935531616
Adjust API to return type int instead of previously type bool;previously type bool;refactor code for future changes;Impact;Similar;0.5630608797073364
Adjust API to return type int instead of previously type bool;previously type bool;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5192389488220215
Adjust API to return type int instead of previously type bool;previously type bool;use tracepoints;kmemtrace;Similar;0.5440328121185303
Adjust API to return type int instead of previously type bool;previously type bool;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5718350410461426
Adjust API to return type int instead of previously type bool;previously type bool;fix typo in mm/slob.c;build fix;Similar;0.5924173593521118
Adjust API to return type int instead of previously type bool;previously type bool;fix lockup in slob_free()  ;lockup;Similar;0.5523041486740112
Adjust API to return type int instead of previously type bool;previously type bool;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5912232398986816
Adjust API to return type int instead of previously type bool;previously type bool;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5217782258987427
Adjust API to return type int instead of previously type bool;previously type bool;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5963282585144043
Adjust API to return type int instead of previously type bool;previously type bool;enable and use this tracer;To enable and use this tracer;Similar;0.5208330154418945
Adjust API to return type int instead of previously type bool;previously type bool;fix bogus ksize calculation fix  ;SLOB;Similar;0.5415633916854858
Adjust API to return type int instead of previously type bool;previously type bool;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5248803496360779
Adjust API to return type int instead of previously type bool;previously type bool;fix bogus ksize calculation;bogus;Similar;0.5384904146194458
Adjust API to return type int instead of previously type bool;previously type bool;Fix to return wrong pointer;Fix;Similar;0.6508007645606995
Adjust API to return type int instead of previously type bool;previously type bool;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5143676996231079
Adjust API to return type int instead of previously type bool;previously type bool;Handle that separately in krealloc();separately;Similar;0.6038868427276611
Adjust API to return type int instead of previously type bool;previously type bool;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5534448027610779
Adjust API to return type int instead of previously type bool;previously type bool;reduce list scanning;slob;Similar;0.5294171571731567
Adjust API to return type int instead of previously type bool;previously type bool;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5298950672149658
Adjust API to return type int instead of previously type bool;previously type bool;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5945384502410889
Adjust API to return type int instead of previously type bool;previously type bool;rework freelist handling;slob;Similar;0.583895742893219
Adjust API to return type int instead of previously type bool;previously type bool;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5483525991439819
Adjust API to return type int instead of previously type bool;previously type bool;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5342426300048828
Adjust API to return type int instead of previously type bool;previously type bool;SLOB to be used on SMP  ;allows;Similar;0.5115073919296265
Adjust API to return type int instead of previously type bool;previously type bool;fix page order calculation on not 4KB page;-;Similar;0.5750542879104614
Adjust API to return type int instead of previously type bool;previously type bool;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6873138546943665
Adjust API to return type int instead of previously type bool;previously type bool;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5591529011726379
Adjust API to return type int instead of previously type bool;previously type bool;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5587788820266724
Adjust API to return type int instead of previously type bool;previously type bool;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6277163624763489
Adjust API to return type int instead of previously type bool;previously type bool;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5718178749084473
Adjust API to return type int instead of previously type bool;previously type bool;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5796058177947998
Adjust API to return type int instead of previously type bool;previously type bool;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5618366003036499
Adjust API to return type int instead of previously type bool;previously type bool;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5447677373886108
Adjust API to return type int instead of previously type bool;previously type bool;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5478003621101379
Adjust API to return type int instead of previously type bool;previously type bool;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5270367860794067
Adjust API to return type int instead of previously type bool;previously type bool;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6041374206542969
Adjust API to return type int instead of previously type bool;previously type bool;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5219129323959351
allow future extension of the bulk alloc API;is done to;"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Similar;0.6207020878791809
allow future extension of the bulk alloc API;is done to;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.6003767251968384
allow future extension of the bulk alloc API;is done to;resurrects approach first proposed in [1];To fix this issue;Similar;0.5750486850738525
allow future extension of the bulk alloc API;is done to;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.616330623626709
allow future extension of the bulk alloc API;is done to;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5727267265319824
allow future extension of the bulk alloc API;is done to;unioned together;Conveniently;Similar;0.5109914541244507
allow future extension of the bulk alloc API;is done to;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5449396371841431
allow future extension of the bulk alloc API;is done to;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.6234993934631348
allow future extension of the bulk alloc API;is done to;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5291977524757385
allow future extension of the bulk alloc API;is done to;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5530834794044495
allow future extension of the bulk alloc API;is done to;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5267385244369507
allow future extension of the bulk alloc API;is done to;This will allow us to push more processing into common code later;improve readability;Similar;0.755508542060852
allow future extension of the bulk alloc API;is done to;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.5023142099380493
allow future extension of the bulk alloc API;is done to;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5852642059326172
allow future extension of the bulk alloc API;is done to;Fix gfp flags passed to lockdep;lockdep;Similar;0.5047332644462585
allow future extension of the bulk alloc API;is done to;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5001868605613708
allow future extension of the bulk alloc API;is done to;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5163469910621643
allow future extension of the bulk alloc API;is done to;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.542251467704773
allow future extension of the bulk alloc API;is done to;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5584450364112854
allow future extension of the bulk alloc API;is done to;refactor code for future changes;Impact;Similar;0.7796924114227295
allow future extension of the bulk alloc API;is done to;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5109636783599854
allow future extension of the bulk alloc API;is done to;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5258301496505737
allow future extension of the bulk alloc API;is done to;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.586280107498169
allow future extension of the bulk alloc API;is done to;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5164227485656738
allow future extension of the bulk alloc API;is done to;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6160489320755005
allow future extension of the bulk alloc API;is done to;enable and use this tracer;To enable and use this tracer;Similar;0.5380014181137085
allow future extension of the bulk alloc API;is done to;reduce external fragmentation by using three free lists;slob;Similar;0.5407571792602539
allow future extension of the bulk alloc API;is done to;improved alignment handling;improved;Similar;0.509382963180542
allow future extension of the bulk alloc API;is done to;rework freelist handling;slob;Similar;0.6240179538726807
allow future extension of the bulk alloc API;is done to;"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;Similar;0.5644930005073547
allow future extension of the bulk alloc API;is done to;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5257773995399475
allow future extension of the bulk alloc API;is done to;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5427583456039429
allow future extension of the bulk alloc API;is done to;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5136970281600952
allow future extension of the bulk alloc API;is done to;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5915431976318359
We always end up returning some objects at the cost of another cmpxchg  ;Else;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5487288236618042
We always end up returning some objects at the cost of another cmpxchg  ;Else;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5184935331344604
We always end up returning some objects at the cost of another cmpxchg  ;Else;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.5841389894485474
We always end up returning some objects at the cost of another cmpxchg  ;Else;a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;Similar;0.5377277731895447
We always end up returning some objects at the cost of another cmpxchg  ;Else;If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;Similar;0.6701269149780273
We always end up returning some objects at the cost of another cmpxchg  ;Else;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5667995810508728
We always end up returning some objects at the cost of another cmpxchg  ;Else;This will allow us to push more processing into common code later;improve readability;Similar;0.5225450396537781
We always end up returning some objects at the cost of another cmpxchg  ;Else;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5812511444091797
We always end up returning some objects at the cost of another cmpxchg  ;Else;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5053257346153259
We always end up returning some objects at the cost of another cmpxchg  ;Else;refactor code for future changes;Impact;Similar;0.5174902677536011
We always end up returning some objects at the cost of another cmpxchg  ;Else;use tracepoints;kmemtrace;Similar;0.5317632555961609
We always end up returning some objects at the cost of another cmpxchg  ;Else;enable and use this tracer;To enable and use this tracer;Similar;0.5502431392669678
We always end up returning some objects at the cost of another cmpxchg  ;Else;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5326705574989319
We always end up returning some objects at the cost of another cmpxchg  ;Else;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5483596324920654
We always end up returning some objects at the cost of another cmpxchg  ;Else;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5100220441818237
We always end up returning some objects at the cost of another cmpxchg  ;Else;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5617631673812866
We always end up returning some objects at the cost of another cmpxchg  ;Else;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5147539377212524
We always end up returning some objects at the cost of another cmpxchg  ;Else;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5050284266471863
We always end up returning some objects at the cost of another cmpxchg  ;Else;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5060743093490601
We always end up returning some objects at the cost of another cmpxchg  ;Else;Redo a lot of comments;Also;Similar;0.5097092390060425
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Similar;0.5064722299575806
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5893151164054871
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.521323561668396
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5629594326019287
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6401078104972839
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5168359875679016
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5053996443748474
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.6119272708892822
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;function naming changes;requires some;Similar;0.5438375473022461
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.628670334815979
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5991966128349304
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5289222002029419
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.6025307178497314
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.650852620601654
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;This will allow us to push more processing into common code later;improve readability;Similar;0.7058427929878235
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Fix early boot kernel crash;Slob;Similar;0.6227699518203735
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5877084136009216
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6327048540115356
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Fix gfp flags passed to lockdep;lockdep;Similar;0.5706403255462646
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.6036063432693481
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5684298276901245
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5321702361106873
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;refactor code for future changes;Impact;Similar;0.6501821279525757
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5624167919158936
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5850222110748291
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6076979637145996
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6482295989990234
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Fix to return wrong pointer;Fix;Similar;0.5029037594795227
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.513411283493042
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5667705535888672
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5075684785842896
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5404696464538574
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5951693058013916
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5041984915733337
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.516777515411377
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5537773370742798
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5387988090515137
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5040863752365112
hiding potentially buggy callers;except temporarily;These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Similar;0.5302802920341492
hiding potentially buggy callers;except temporarily;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5222157835960388
hiding potentially buggy callers;except temporarily;Improve trace accuracy;by correctly tracing reported size;Similar;0.5117529034614563
hiding potentially buggy callers;except temporarily;This affects RCU handling;somewhat;Similar;0.5156128406524658
hiding potentially buggy callers;except temporarily;Remove various small accessors;various small;Similar;0.6185165643692017
hiding potentially buggy callers;except temporarily;Fix to return wrong pointer;Fix;Similar;0.5561026334762573
hiding potentially buggy callers;except temporarily;fix memory corruption;memory corruption;Similar;0.5259849429130554
hiding potentially buggy callers;except temporarily;Handle that separately in krealloc();separately;Similar;0.5265948176383972
hiding potentially buggy callers;except temporarily;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5971876382827759
hiding potentially buggy callers;except temporarily;reduce list scanning;slob;Similar;0.5329612493515015
hiding potentially buggy callers;except temporarily;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6186701059341431
hiding potentially buggy callers;except temporarily;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5000659227371216
hiding potentially buggy callers;except temporarily;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5171154737472534
hiding potentially buggy callers;except temporarily;simplifies SLOB;at this point slob may be broken;Similar;0.5079187750816345
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5151888132095337
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.6242803335189819
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5582634210586548
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.6339038610458374
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.602807879447937
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.528508186340332
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5355095863342285
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5905134677886963
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5812914371490479
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5014948844909668
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6116127371788025
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Fix gfp flags passed to lockdep;lockdep;Similar;0.5819642543792725
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5545647740364075
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.6158607602119446
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5334429144859314
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.6134639978408813
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.597490668296814
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;use tracepoints;kmemtrace;Similar;0.5253686308860779
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5998234748840332
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;fix lockup in slob_free()  ;lockup;Similar;0.6733173131942749
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5705945491790771
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.6018747091293335
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.7048056125640869
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;enable and use this tracer;To enable and use this tracer;Similar;0.5668964385986328
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.552128791809082
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5261104106903076
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;reduce external fragmentation by using three free lists;slob;Similar;0.6039143800735474
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5233866572380066
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5266944169998169
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.6372311115264893
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5039796233177185
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;rework freelist handling;slob;Similar;0.729485034942627
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5360872149467468
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5279608368873596
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6281048059463501
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5375365018844604
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5286751985549927
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5035741329193115
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6204525828361511
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5826754570007324
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5052933692932129
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Make dead caches discard free slabs immediately;slub;Similar;0.5025584101676941
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.818953275680542
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.6325986385345459
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5052793622016907
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.6840596199035645
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.635217547416687
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.6206786632537842
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;This will allow us to push more processing into common code later;improve readability;Similar;0.5285992622375488
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5310162305831909
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5381885766983032
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5325521230697632
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5589675903320312
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.6474664211273193
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5099691152572632
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;fix lockup in slob_free()  ;lockup;Similar;0.5241800546646118
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5060617923736572
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.6062953472137451
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;reduce external fragmentation by using three free lists;slob;Similar;0.5155055522918701
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5867459774017334
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.514980673789978
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;fix memory corruption;memory corruption;Similar;0.5053986310958862
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;remove useless ctor parameter and reorder parameters;useless;Similar;0.5041542649269104
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5521406531333923
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.6106062531471252
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6407738924026489
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5150060057640076
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;fix page order calculation on not 4KB page;-;Similar;0.5490866303443909
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6118036508560181
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6074186563491821
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5814511775970459
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5221900343894958
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5939270257949829
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5071695446968079
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6251404285430908
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5159277319908142
Make dead caches discard free slabs immediately;slub;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.6284559369087219
Make dead caches discard free slabs immediately;slub;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.5913602113723755
Make dead caches discard free slabs immediately;slub;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5366915464401245
Make dead caches discard free slabs immediately;slub;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.5610800981521606
Make dead caches discard free slabs immediately;slub;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5137424468994141
Make dead caches discard free slabs immediately;slub;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5317732691764832
Make dead caches discard free slabs immediately;slub;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5051772594451904
Make dead caches discard free slabs immediately;slub;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.5460473299026489
Make dead caches discard free slabs immediately;slub;fix lockup in slob_free()  ;lockup;Similar;0.5128451585769653
Make dead caches discard free slabs immediately;slub;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5444999933242798
Make dead caches discard free slabs immediately;slub;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5411947965621948
Make dead caches discard free slabs immediately;slub;remove useless ctor parameter and reorder parameters;useless;Similar;0.5292048454284668
Make dead caches discard free slabs immediately;slub;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.6180614233016968
Make dead caches discard free slabs immediately;slub;Cleanup zeroing allocations;Slab allocators;Similar;0.510847806930542
Make dead caches discard free slabs immediately;slub;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5946107506752014
Make dead caches discard free slabs immediately;slub;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5650908350944519
Make dead caches discard free slabs immediately;slub;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.642063319683075
Make dead caches discard free slabs immediately;slub;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5130725502967834
Make dead caches discard free slabs immediately;slub;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5264018774032593
Make dead caches discard free slabs immediately;slub;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.539000391960144
Make dead caches discard free slabs immediately;slub;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.558362603187561
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Similar;0.5604565143585205
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.6144967675209045
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.6503852605819702
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5220963954925537
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.6806572675704956
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.6286312341690063
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5727742314338684
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.651419460773468
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5600507855415344
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5004394054412842
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;This will allow us to push more processing into common code later;improve readability;Similar;0.5193943977355957
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6072750091552734
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Remove various small accessors;various small;Similar;0.5416203141212463
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.5203944444656372
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.506402850151062
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;cleans up some bitrot in slob.c;Also;Similar;0.5456258654594421
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5392407178878784
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5234583616256714
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5500428676605225
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5610176920890808
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.657118558883667
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;fix lockup in slob_free()  ;lockup;Similar;0.5654647350311279
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5351448059082031
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5622239708900452
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Fix to return wrong pointer;Fix;Similar;0.5112813711166382
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5990135669708252
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;reduce external fragmentation by using three free lists;slob;Similar;0.5290635228157043
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5662974119186401
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5783863067626953
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;fix memory corruption;memory corruption;Similar;0.5753744840621948
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;remove useless ctor parameter and reorder parameters;useless;Similar;0.5357158184051514
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5271854400634766
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;reduce list scanning;slob;Similar;0.5340572595596313
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5557874441146851
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5685263872146606
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5929690599441528
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.7489611506462097
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5458315014839172
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5490838289260864
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5083370208740234
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5172971487045288
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;fix page order calculation on not 4KB page;-;Similar;0.6063133478164673
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6118457317352295
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5930006504058838
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5118275284767151
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5764480233192444
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6720637083053589
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6645673513412476
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5528643727302551
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6137173771858215
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5029458999633789
resurrects approach first proposed in [1];To fix this issue;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5576086044311523
resurrects approach first proposed in [1];To fix this issue;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.6436942219734192
resurrects approach first proposed in [1];To fix this issue;unioned together;Conveniently;Similar;0.6910016536712646
resurrects approach first proposed in [1];To fix this issue;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5477138161659241
resurrects approach first proposed in [1];To fix this issue;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.6473821401596069
resurrects approach first proposed in [1];To fix this issue;function naming changes;requires some;Similar;0.6690210700035095
resurrects approach first proposed in [1];To fix this issue;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.6075990200042725
resurrects approach first proposed in [1];To fix this issue;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5211130380630493
resurrects approach first proposed in [1];To fix this issue;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5956947803497314
resurrects approach first proposed in [1];To fix this issue;Improve trace accuracy;by correctly tracing reported size;Similar;0.5397334098815918
resurrects approach first proposed in [1];To fix this issue;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.614033579826355
resurrects approach first proposed in [1];To fix this issue;This will allow us to push more processing into common code later;improve readability;Similar;0.5863633155822754
resurrects approach first proposed in [1];To fix this issue;This affects RCU handling;somewhat;Similar;0.5621898174285889
resurrects approach first proposed in [1];To fix this issue;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.598320722579956
resurrects approach first proposed in [1];To fix this issue;Remove various small accessors;various small;Similar;0.5152194499969482
resurrects approach first proposed in [1];To fix this issue;Fix gfp flags passed to lockdep;lockdep;Similar;0.5864258408546448
resurrects approach first proposed in [1];To fix this issue;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6730639934539795
resurrects approach first proposed in [1];To fix this issue;Adding this mask;fixes the bug;Similar;0.6531407833099365
resurrects approach first proposed in [1];To fix this issue;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6846011877059937
resurrects approach first proposed in [1];To fix this issue;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.6201220750808716
resurrects approach first proposed in [1];To fix this issue;Remove kmemtrace ftrace plugin;tracing;Similar;0.5288770198822021
resurrects approach first proposed in [1];To fix this issue;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5612212419509888
resurrects approach first proposed in [1];To fix this issue;refactor code for future changes;Impact;Similar;0.6859439611434937
resurrects approach first proposed in [1];To fix this issue;use tracepoints;kmemtrace;Similar;0.6892132759094238
resurrects approach first proposed in [1];To fix this issue;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5929608345031738
resurrects approach first proposed in [1];To fix this issue;fix typo in mm/slob.c;build fix;Similar;0.5671979188919067
resurrects approach first proposed in [1];To fix this issue;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5516327619552612
resurrects approach first proposed in [1];To fix this issue;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5722782611846924
resurrects approach first proposed in [1];To fix this issue;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5310545563697815
resurrects approach first proposed in [1];To fix this issue;enable and use this tracer;To enable and use this tracer;Similar;0.6909974217414856
resurrects approach first proposed in [1];To fix this issue;I find it more readable  ;personal opinion;Similar;0.5220431089401245
resurrects approach first proposed in [1];To fix this issue;Drop it  ;if you want;Similar;0.5048894286155701
resurrects approach first proposed in [1];To fix this issue;fix bogus ksize calculation fix  ;SLOB;Similar;0.6068912744522095
resurrects approach first proposed in [1];To fix this issue;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.7659551501274109
resurrects approach first proposed in [1];To fix this issue;fix bogus ksize calculation;bogus;Similar;0.6103682518005371
resurrects approach first proposed in [1];To fix this issue;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5434571504592896
resurrects approach first proposed in [1];To fix this issue;record page flag overlays explicitly;slob;Similar;0.5275022387504578
resurrects approach first proposed in [1];To fix this issue;Fix to return wrong pointer;Fix;Similar;0.526106595993042
resurrects approach first proposed in [1];To fix this issue;Handle that separately in krealloc();separately;Similar;0.5733920335769653
resurrects approach first proposed in [1];To fix this issue;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5663661956787109
resurrects approach first proposed in [1];To fix this issue;improved alignment handling;improved;Similar;0.5213736891746521
resurrects approach first proposed in [1];To fix this issue;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5277678370475769
resurrects approach first proposed in [1];To fix this issue;remove bigblock tracking;slob;Similar;0.5684601068496704
resurrects approach first proposed in [1];To fix this issue;rework freelist handling;slob;Similar;0.6325036287307739
resurrects approach first proposed in [1];To fix this issue;"align them to word size
";it is best in practice;Similar;0.6180480718612671
resurrects approach first proposed in [1];To fix this issue;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5748096704483032
resurrects approach first proposed in [1];To fix this issue;SLOB to be used on SMP  ;allows;Similar;0.6122382879257202
resurrects approach first proposed in [1];To fix this issue;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5207456350326538
resurrects approach first proposed in [1];To fix this issue;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5151997208595276
resurrects approach first proposed in [1];To fix this issue;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.7392587661743164
resurrects approach first proposed in [1];To fix this issue;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5276968479156494
resurrects approach first proposed in [1];To fix this issue;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5311309099197388
resurrects approach first proposed in [1];To fix this issue;Redo a lot of comments;Also;Similar;0.5327224731445312
resurrects approach first proposed in [1];To fix this issue;simplifies SLOB;at this point slob may be broken;Similar;0.592981219291687
resurrects approach first proposed in [1];To fix this issue;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5459479093551636
resurrects approach first proposed in [1];To fix this issue;fix;SLOB=y && SMP=y fix;Similar;0.6797302961349487
resurrects approach first proposed in [1];To fix this issue;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.54060959815979
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Similar;0.6342248916625977
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Similar;0.5104435682296753
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.5229769349098206
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.5566402673721313
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5024738311767578
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5096052289009094
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;function naming changes;requires some;Similar;0.5277501344680786
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.6692371368408203
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.6536269783973694
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5339840650558472
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.50281822681427
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5663002133369446
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.6086077690124512
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.6512335538864136
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6742337942123413
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Fix early boot kernel crash;Slob;Similar;0.5961734056472778
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;cleans up some bitrot in slob.c;Also;Similar;0.5676859617233276
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.7147518992424011
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Fix gfp flags passed to lockdep;lockdep;Similar;0.6085600256919861
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6087924838066101
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.7459550499916077
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5916752815246582
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Remove kmemtrace ftrace plugin;tracing;Similar;0.548184871673584
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6456135511398315
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.6075683832168579
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.6524848937988281
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6079114079475403
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;fix typo in mm/slob.c;build fix;Similar;0.6581500768661499
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;fix lockup in slob_free()  ;lockup;Similar;0.5630550384521484
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5948346853256226
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.6123369932174683
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5809154510498047
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;fix bogus ksize calculation fix  ;SLOB;Similar;0.5487132668495178
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;fix bogus ksize calculation;bogus;Similar;0.5428767204284668
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Fix to return wrong pointer;Fix;Similar;0.542171835899353
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5193578004837036
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5019707679748535
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;fix memory corruption;memory corruption;Similar;0.5944758653640747
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5434623956680298
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5894082188606262
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.6090047955513
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5898232460021973
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5041751265525818
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5767934322357178
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5526779890060425
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5458984375
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;fix page order calculation on not 4KB page;-;Similar;0.5656566619873047
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.57950359582901
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5120819211006165
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5451003909111023
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5911858677864075
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5558439493179321
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6134274005889893
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.7019328474998474
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7540299296379089
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5300065279006958
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.6421073079109192
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Similar;0.6025776863098145
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.6635473370552063
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.6869861483573914
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5781493186950684
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5471560955047607
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5342830419540405
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.6648937463760376
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6397837996482849
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Fix early boot kernel crash;Slob;Similar;0.607966423034668
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;cleans up some bitrot in slob.c;Also;Similar;0.6730391979217529
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5476516485214233
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Fix gfp flags passed to lockdep;lockdep;Similar;0.5153710842132568
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5059940814971924
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.518208384513855
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Remove kmemtrace ftrace plugin;tracing;Similar;0.6021209359169006
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.7271323204040527
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5192921757698059
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.597666323184967
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6105237007141113
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;fix typo in mm/slob.c;build fix;Similar;0.5369724035263062
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;fix lockup in slob_free()  ;lockup;Similar;0.557837724685669
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5640943050384521
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.6128792762756348
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5433355569839478
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.532609760761261
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5090416669845581
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.6648709774017334
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5686800479888916
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6747511625289917
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5421209335327148
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.6913943290710449
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5001521110534668
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6169365048408508
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.6808581352233887
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5189006328582764
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5683947801589966
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5099525451660156
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6522039771080017
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.7058514952659607
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5505773425102234
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6663338541984558
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5459468960762024
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Similar;0.5090141892433167
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;unioned together;Conveniently;Similar;0.5274364948272705
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.5142315030097961
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5049335360527039
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";Similar;0.5275372266769409
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Improve trace accuracy;by correctly tracing reported size;Similar;0.5855442881584167
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5150668025016785
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;This affects RCU handling;somewhat;Similar;0.5736318230628967
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.5211730003356934
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Remove various small accessors;various small;Similar;0.5186256170272827
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;cleans up some bitrot in slob.c;Also;Similar;0.5011367797851562
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Adding this mask;fixes the bug;Similar;0.544619083404541
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5709754228591919
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;use tracepoints;kmemtrace;Similar;0.5503871440887451
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;fix typo in mm/slob.c;build fix;Similar;0.5252629518508911
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5556197762489319
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;enable and use this tracer;To enable and use this tracer;Similar;0.5794082880020142
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;I find it more readable  ;personal opinion;Similar;0.5398973226547241
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;fix bogus ksize calculation fix  ;SLOB;Similar;0.5413872599601746
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;fix bogus ksize calculation;bogus;Similar;0.5567712187767029
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5522866249084473
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Cleanup zeroing allocations;Slab allocators;Similar;0.5222330689430237
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5155835747718811
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;improved alignment handling;improved;Similar;0.5436730980873108
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;remove bigblock tracking;slob;Similar;0.5559450387954712
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;"align them to word size
";it is best in practice;Similar;0.5295907855033875
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Code size goes up about 1K;dynamic memory usage should be lower even on relatively small memory systems;Similar;0.542034387588501
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5118332505226135
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;SLOB to be used on SMP  ;allows;Similar;0.5105624198913574
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;handle SLAB_PANIC flag;slob;Similar;0.5534266233444214
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5124254822731018
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;simplifies SLOB;at this point slob may be broken;Similar;0.638960599899292
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;fix;SLOB=y && SMP=y fix;Similar;0.5670868158340454
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5093258619308472
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.5831071138381958
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;unioned together;Conveniently;Similar;0.5433226823806763
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;function naming changes;requires some;Similar;0.5026296973228455
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;This affects RCU handling;somewhat;Similar;0.6143554449081421
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.5680147409439087
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Remove various small accessors;various small;Similar;0.6702224016189575
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;They are no longer needed;They have become so simple;Similar;0.8125553131103516
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.673174262046814
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.6763069033622742
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.6754963397979736
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;use tracepoints;kmemtrace;Similar;0.5378659963607788
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5925668478012085
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;I find it more readable  ;personal opinion;Similar;0.5393633842468262
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Drop it  ;if you want;Similar;0.7645640969276428
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;fix bogus ksize calculation fix  ;SLOB;Similar;0.551426887512207
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;fix bogus ksize calculation;bogus;Similar;0.5875040292739868
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5712655782699585
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.6599705219268799
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5313845276832581
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;remove useless ctor parameter and reorder parameters;useless;Similar;0.630064845085144
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Handle that separately in krealloc();separately;Similar;0.6683303117752075
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;reduce list scanning;slob;Similar;0.6725614070892334
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Cleanup zeroing allocations;Slab allocators;Similar;0.6327823400497437
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;remove bigblock tracking;slob;Similar;0.5102798342704773
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6573305726051331
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;SLOB to be used on SMP  ;allows;Similar;0.5014593601226807
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;fix page order calculation on not 4KB page;-;Similar;0.5247934460639954
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;handle SLAB_PANIC flag;slob;Similar;0.5039265155792236
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5103877782821655
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5376427173614502
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.7442901134490967
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;simplifies SLOB;at this point slob may be broken;Similar;0.5867283940315247
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;fix;SLOB=y && SMP=y fix;Similar;0.673880934715271
"Introduce much more complexity to both SLUB and memcg
";than this small patch;function naming changes;requires some;Similar;0.5688319206237793
"Introduce much more complexity to both SLUB and memcg
";than this small patch;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5029231309890747
"Introduce much more complexity to both SLUB and memcg
";than this small patch;a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;Similar;0.6772210001945496
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Improve trace accuracy;by correctly tracing reported size;Similar;0.5122994184494019
"Introduce much more complexity to both SLUB and memcg
";than this small patch;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5098300576210022
"Introduce much more complexity to both SLUB and memcg
";than this small patch;This affects RCU handling;somewhat;Similar;0.6454325318336487
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Remove various small accessors;various small;Similar;0.5651798844337463
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Adding this mask;fixes the bug;Similar;0.6424636840820312
"Introduce much more complexity to both SLUB and memcg
";than this small patch;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5124332904815674
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Remove kmemtrace ftrace plugin;tracing;Similar;0.526412844657898
"Introduce much more complexity to both SLUB and memcg
";than this small patch;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5571510791778564
"Introduce much more complexity to both SLUB and memcg
";than this small patch;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5549384355545044
"Introduce much more complexity to both SLUB and memcg
";than this small patch;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.5040485262870789
"Introduce much more complexity to both SLUB and memcg
";than this small patch;fix typo in mm/slob.c;build fix;Similar;0.53533935546875
"Introduce much more complexity to both SLUB and memcg
";than this small patch;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5233848094940186
"Introduce much more complexity to both SLUB and memcg
";than this small patch;enable and use this tracer;To enable and use this tracer;Similar;0.5104954242706299
"Introduce much more complexity to both SLUB and memcg
";than this small patch;I find it more readable  ;personal opinion;Similar;0.5574649572372437
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Drop it  ;if you want;Similar;0.5039354562759399
"Introduce much more complexity to both SLUB and memcg
";than this small patch;fix bogus ksize calculation fix  ;SLOB;Similar;0.5279518365859985
"Introduce much more complexity to both SLUB and memcg
";than this small patch;fix bogus ksize calculation;bogus;Similar;0.5498901605606079
"Introduce much more complexity to both SLUB and memcg
";than this small patch;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5285175442695618
"Introduce much more complexity to both SLUB and memcg
";than this small patch;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5120491981506348
"Introduce much more complexity to both SLUB and memcg
";than this small patch;remove useless ctor parameter and reorder parameters;useless;Similar;0.5175130367279053
"Introduce much more complexity to both SLUB and memcg
";than this small patch;reduce list scanning;slob;Similar;0.5571763515472412
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5321518778800964
"Introduce much more complexity to both SLUB and memcg
";than this small patch;remove bigblock tracking;slob;Similar;0.6935744285583496
"Introduce much more complexity to both SLUB and memcg
";than this small patch;can be slightly faster too;skip almost-full freelist pages completely;Similar;0.5160055756568909
"Introduce much more complexity to both SLUB and memcg
";than this small patch;"Taking a spinlock in a destructor is a bit risky
";since the slab allocators may run the destructors anytime they decide a slab is no longer needed;Similar;0.542470395565033
"Introduce much more complexity to both SLUB and memcg
";than this small patch;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5519328117370605
"Introduce much more complexity to both SLUB and memcg
";than this small patch;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5008406639099121
"Introduce much more complexity to both SLUB and memcg
";than this small patch;SLOB to be used on SMP  ;allows;Similar;0.563084602355957
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5421022176742554
"Introduce much more complexity to both SLUB and memcg
";than this small patch;handle SLAB_PANIC flag;slob;Similar;0.6602039337158203
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5588260889053345
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Redo a lot of comments;Also;Similar;0.6772721409797668
"Introduce much more complexity to both SLUB and memcg
";than this small patch;simplifies SLOB;at this point slob may be broken;Similar;0.6440712809562683
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5046384334564209
"Introduce much more complexity to both SLUB and memcg
";than this small patch;fix;SLOB=y && SMP=y fix;Similar;0.5434653759002686
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5338256359100342
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Similar;0.6856122016906738
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.773327112197876
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5108296275138855
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;function naming changes;requires some;Similar;0.5118127465248108
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5367733240127563
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.6428418159484863
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5719074010848999
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6840583086013794
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Remove various small accessors;various small;Similar;0.5779039263725281
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;They are no longer needed;They have become so simple;Similar;0.5419682264328003
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.6987605094909668
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Fix gfp flags passed to lockdep;lockdep;Similar;0.5710883736610413
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5105512142181396
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.6899688243865967
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Remove kmemtrace ftrace plugin;tracing;Similar;0.7486088275909424
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.6755014657974243
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6597728729248047
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5062832236289978
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.6644356846809387
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5798377394676208
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;fix typo in mm/slob.c;build fix;Similar;0.5176079273223877
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;fix lockup in slob_free()  ;lockup;Similar;0.5420265197753906
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5176408290863037
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5373929142951965
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Drop it  ;if you want;Similar;0.5068261623382568
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.8038849830627441
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Fix to return wrong pointer;Fix;Similar;0.5061802864074707
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5466794967651367
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.533190906047821
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5215930938720703
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;remove useless ctor parameter and reorder parameters;useless;Similar;0.7840169668197632
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Handle that separately in krealloc();separately;Similar;0.5636404752731323
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5060834288597107
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;reduce list scanning;slob;Similar;0.7130419611930847
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5386365652084351
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5096426010131836
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Cleanup zeroing allocations;Slab allocators;Similar;0.6315097212791443
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5023919343948364
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6964327096939087
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;remove bigblock tracking;slob;Similar;0.608487606048584
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.7180838584899902
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5205641984939575
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5114498138427734
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;fix page order calculation on not 4KB page;-;Similar;0.6532435417175293
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5820183753967285
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.7140907645225525
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.7368133068084717
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.7568022012710571
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.5189070105552673
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6441344618797302
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6200896501541138
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6375508308410645
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Similar;0.519302487373352
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;unioned together;Conveniently;Similar;0.5118976831436157
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.6300636529922485
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.6062914133071899
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;function naming changes;requires some;Similar;0.5484961867332458
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.5674234628677368
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5668516159057617
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.6130921244621277
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.522323727607727
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";Similar;0.5445782542228699
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Improve trace accuracy;by correctly tracing reported size;Similar;0.6511714458465576
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5936123728752136
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.7518672943115234
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.6823252439498901
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Similar;0.6833223104476929
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;This will allow us to push more processing into common code later;improve readability;Similar;0.6001653671264648
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5007278323173523
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Fix early boot kernel crash;Slob;Similar;0.5646559000015259
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.59881591796875
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;cleans up some bitrot in slob.c;Also;Similar;0.6299875974655151
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5655572414398193
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Fix gfp flags passed to lockdep;lockdep;Similar;0.5848407745361328
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6553506851196289
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Adding this mask;fixes the bug;Similar;0.5392918586730957
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5417482256889343
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6873789429664612
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5458431243896484
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5332750082015991
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5691525936126709
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;refactor code for future changes;Impact;Similar;0.6210562586784363
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5600764751434326
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;use tracepoints;kmemtrace;Similar;0.6265150308609009
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.648545503616333
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;fix typo in mm/slob.c;build fix;Similar;0.6025568246841431
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;fix lockup in slob_free()  ;lockup;Similar;0.6330834627151489
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.705785870552063
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5144116878509521
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.7555961012840271
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;enable and use this tracer;To enable and use this tracer;Similar;0.7258083820343018
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;fix bogus ksize calculation fix  ;SLOB;Similar;0.5546374917030334
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5633699893951416
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;fix bogus ksize calculation;bogus;Similar;0.5338581800460815
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Fix to return wrong pointer;Fix;Similar;0.6269596815109253
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5542728900909424
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;fix memory corruption;memory corruption;Similar;0.5809402465820312
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5651353597640991
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5317723155021667
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.538603663444519
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;improved alignment handling;improved;Similar;0.5847415924072266
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5611376762390137
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;rework freelist handling;slob;Similar;0.6415929794311523
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6666556596755981
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6214097738265991
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5566960573196411
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5985450744628906
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5113313794136047
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5783348083496094
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.6454899311065674
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5520507097244263
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5087705850601196
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;simplifies SLOB;at this point slob may be broken;Similar;0.5260536670684814
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5168927311897278
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5435062050819397
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;unioned together;Conveniently;Similar;0.6842272281646729
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.7599544525146484
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5137850046157837
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;function naming changes;requires some;Similar;0.6036330461502075
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.5269572734832764
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5119124054908752
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";Similar;0.5650275945663452
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Improve trace accuracy;by correctly tracing reported size;Similar;0.6973283886909485
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;This will allow us to push more processing into common code later;improve readability;Similar;0.5752643346786499
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;This affects RCU handling;somewhat;Similar;0.6476912498474121
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.7437918186187744
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Remove various small accessors;various small;Similar;0.6028039455413818
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;They are no longer needed;They have become so simple;Similar;0.6383038759231567
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Fix gfp flags passed to lockdep;lockdep;Similar;0.5280590057373047
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Adding this mask;fixes the bug;Similar;0.6112207770347595
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5862472653388977
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.6201396584510803
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5447258949279785
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;refactor code for future changes;Impact;Similar;0.6572620868682861
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;use tracepoints;kmemtrace;Similar;0.5959255695343018
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5696189403533936
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;fix typo in mm/slob.c;build fix;Similar;0.5481064319610596
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;fix lockup in slob_free()  ;lockup;Similar;0.6349167823791504
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.7877094745635986
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;enable and use this tracer;To enable and use this tracer;Similar;0.7167587876319885
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;I find it more readable  ;personal opinion;Similar;0.7169234752655029
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Drop it  ;if you want;Similar;0.616169810295105
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;fix bogus ksize calculation fix  ;SLOB;Similar;0.6573353409767151
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.6655546426773071
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;fix bogus ksize calculation;bogus;Similar;0.6709400415420532
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5022701025009155
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Fix to return wrong pointer;Fix;Similar;0.6117292642593384
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5912216901779175
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5610818862915039
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;fix memory corruption;memory corruption;Similar;0.5403420925140381
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Handle that separately in krealloc();separately;Similar;0.5492826700210571
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;reduce list scanning;slob;Similar;0.5805575847625732
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Cleanup zeroing allocations;Slab allocators;Similar;0.5544695854187012
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;improved alignment handling;improved;Similar;0.7123328447341919
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;remove bigblock tracking;slob;Similar;0.5034754276275635
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;rework freelist handling;slob;Similar;0.7096319794654846
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;can be slightly faster too;skip almost-full freelist pages completely;Similar;0.5665985345840454
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;"align them to word size
";it is best in practice;Similar;0.5690248608589172
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.512739896774292
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;SLOB to be used on SMP  ;allows;Similar;0.5105608105659485
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5704003572463989
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5978944301605225
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.618927538394928
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5073337554931641
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Redo a lot of comments;Also;Similar;0.6002154350280762
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5167033672332764
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;simplifies SLOB;at this point slob may be broken;Similar;0.721358060836792
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;fix;SLOB=y && SMP=y fix;Similar;0.772680401802063
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.6631374359130859
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Similar;0.5687543153762817
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5780805945396423
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5428595542907715
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.6121252775192261
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.7003783583641052
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5790356397628784
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Fix early boot kernel crash;Slob;Similar;0.5919284224510193
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.5044865012168884
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;cleans up some bitrot in slob.c;Also;Similar;0.5329241752624512
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5619434118270874
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Remove kmemtrace ftrace plugin;tracing;Similar;0.6135916113853455
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.7236887216567993
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.548945426940918
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.7585184574127197
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6331202983856201
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;fix lockup in slob_free()  ;lockup;Similar;0.553942084312439
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5113982558250427
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5735194087028503
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;fix bogus ksize calculation fix  ;SLOB;Similar;0.526301383972168
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;fix bogus ksize calculation;bogus;Similar;0.5166503190994263
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5408229827880859
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Fix to return wrong pointer;Fix;Similar;0.5860071778297424
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.6398704648017883
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5282235145568848
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;fix memory corruption;memory corruption;Similar;0.5172197222709656
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;remove useless ctor parameter and reorder parameters;useless;Similar;0.6465438008308411
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;reduce list scanning;slob;Similar;0.5343364477157593
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.6472160816192627
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Cleanup zeroing allocations;Slab allocators;Similar;0.5126478672027588
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5624980926513672
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.7044260501861572
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;remove bigblock tracking;slob;Similar;0.5269533395767212
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5488015413284302
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;"Taking a spinlock in a destructor is a bit risky
";since the slab allocators may run the destructors anytime they decide a slab is no longer needed;Similar;0.5478442907333374
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6143304705619812
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.6517735719680786
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;fix page order calculation on not 4KB page;-;Similar;0.6203442811965942
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.641165018081665
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6352529525756836
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;handle SLAB_PANIC flag;slob;Similar;0.5167556405067444
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5479118824005127
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.613924503326416
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6614618897438049
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5354333519935608
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5198121070861816
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6524614095687866
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.533174991607666
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6665070056915283
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.58586585521698
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5644625425338745
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5361970663070679
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;function naming changes;requires some;Similar;0.5080333948135376
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5859506130218506
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5904140472412109
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.6845594644546509
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5969760417938232
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;can be done in __kmem_cache_shutdown;What is done there;Similar;0.803275465965271
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Remove various small accessors;various small;Similar;0.6689892411231995
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;They are no longer needed;They have become so simple;Similar;0.5803255438804626
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.6806272268295288
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;cleans up some bitrot in slob.c;Also;Similar;0.5264843106269836
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5251123905181885
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5163956880569458
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5930098295211792
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Remove kmemtrace ftrace plugin;tracing;Similar;0.8294204473495483
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.7950586080551147
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6930489540100098
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.6556178331375122
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.58784019947052
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;fix typo in mm/slob.c;build fix;Similar;0.5720980167388916
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;fix lockup in slob_free()  ;lockup;Similar;0.6012767553329468
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5388997793197632
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5006603002548218
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Drop it  ;if you want;Similar;0.6553635597229004
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;fix bogus ksize calculation fix  ;SLOB;Similar;0.561581552028656
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;fix bogus ksize calculation;bogus;Similar;0.5858421921730042
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.785911500453949
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Fix to return wrong pointer;Fix;Similar;0.5597870349884033
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5286449193954468
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5385989546775818
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.6183993816375732
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;fix memory corruption;memory corruption;Similar;0.5323625206947327
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;remove useless ctor parameter and reorder parameters;useless;Similar;0.8273647427558899
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Handle that separately in krealloc();separately;Similar;0.5893866419792175
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;reduce list scanning;slob;Similar;0.786022424697876
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5235623121261597
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5007631182670593
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Cleanup zeroing allocations;Slab allocators;Similar;0.7377314567565918
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5553784370422363
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.7143163681030273
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;remove bigblock tracking;slob;Similar;0.731421947479248
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6294459104537964
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5475292801856995
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5969557762145996
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;SLOB to be used on SMP  ;allows;Similar;0.5115160942077637
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;fix page order calculation on not 4KB page;-;Similar;0.6412097215652466
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5777950286865234
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.8448187112808228
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;handle SLAB_PANIC flag;slob;Similar;0.5657416582107544
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5006909370422363
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.6278361082077026
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6184680461883545
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5773730278015137
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.7657666206359863
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.702628493309021
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5748757123947144
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7598795890808105
unioned together;Conveniently;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Similar;0.6980111002922058
unioned together;Conveniently;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5543496608734131
unioned together;Conveniently;function naming changes;requires some;Similar;0.627934455871582
unioned together;Conveniently;Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Similar;0.5280457735061646
unioned together;Conveniently;Improve trace accuracy;by correctly tracing reported size;Similar;0.620902419090271
unioned together;Conveniently;This affects RCU handling;somewhat;Similar;0.6918154358863831
unioned together;Conveniently;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.6562497615814209
unioned together;Conveniently;Remove various small accessors;various small;Similar;0.5460690259933472
unioned together;Conveniently;They are no longer needed;They have become so simple;Similar;0.5252642035484314
unioned together;Conveniently;Fix gfp flags passed to lockdep;lockdep;Similar;0.57821124792099
unioned together;Conveniently;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5772222876548767
unioned together;Conveniently;Adding this mask;fixes the bug;Similar;0.6049380302429199
unioned together;Conveniently;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5703481435775757
unioned together;Conveniently;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5358654260635376
unioned together;Conveniently;refactor code for future changes;Impact;Similar;0.5857105255126953
unioned together;Conveniently;use tracepoints;kmemtrace;Similar;0.679919958114624
unioned together;Conveniently;fix typo in mm/slob.c;build fix;Similar;0.537121057510376
unioned together;Conveniently;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.6926981806755066
unioned together;Conveniently;enable and use this tracer;To enable and use this tracer;Similar;0.7132507562637329
unioned together;Conveniently;I find it more readable  ;personal opinion;Similar;0.6271389722824097
unioned together;Conveniently;Drop it  ;if you want;Similar;0.534699559211731
unioned together;Conveniently;fix bogus ksize calculation fix  ;SLOB;Similar;0.5705734491348267
unioned together;Conveniently;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.6906663775444031
unioned together;Conveniently;fix bogus ksize calculation;bogus;Similar;0.570451557636261
unioned together;Conveniently;Handle that separately in krealloc();separately;Similar;0.5199365615844727
unioned together;Conveniently;improved alignment handling;improved;Similar;0.6567375659942627
unioned together;Conveniently;rework freelist handling;slob;Similar;0.6720916032791138
unioned together;Conveniently;"align them to word size
";it is best in practice;Similar;0.6476595401763916
unioned together;Conveniently;SLOB to be used on SMP  ;allows;Similar;0.6199564933776855
unioned together;Conveniently;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6517165899276733
unioned together;Conveniently;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5083341002464294
unioned together;Conveniently;Redo a lot of comments;Also;Similar;0.5920209884643555
unioned together;Conveniently;simplifies SLOB;at this point slob may be broken;Similar;0.6926159858703613
unioned together;Conveniently;fix;SLOB=y && SMP=y fix;Similar;0.7467100024223328
unioned together;Conveniently;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6029475927352905
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.5340229272842407
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;function naming changes;requires some;Similar;0.5228543877601624
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";Similar;0.6958832740783691
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Improve trace accuracy;by correctly tracing reported size;Similar;0.7161298990249634
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5766482353210449
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5086395144462585
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Similar;0.6698222756385803
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;This will allow us to push more processing into common code later;improve readability;Similar;0.5652287602424622
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;This affects RCU handling;somewhat;Similar;0.6172116994857788
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.7499682307243347
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Remove various small accessors;various small;Similar;0.5177852511405945
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;They are no longer needed;They have become so simple;Similar;0.535228431224823
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5061970949172974
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Adding this mask;fixes the bug;Similar;0.5579630732536316
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5699578523635864
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.6061524152755737
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;refactor code for future changes;Impact;Similar;0.5114172697067261
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;use tracepoints;kmemtrace;Similar;0.6399391293525696
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;fix typo in mm/slob.c;build fix;Similar;0.5217608213424683
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;fix lockup in slob_free()  ;lockup;Similar;0.6096099019050598
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.7464648485183716
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5197576880455017
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;enable and use this tracer;To enable and use this tracer;Similar;0.7856768369674683
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;I find it more readable  ;personal opinion;Similar;0.7476248741149902
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;fix bogus ksize calculation fix  ;SLOB;Similar;0.5361759662628174
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.6448233127593994
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;fix bogus ksize calculation;bogus;Similar;0.5451189279556274
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5606048703193665
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5554049015045166
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;fix memory corruption;memory corruption;Similar;0.5147266387939453
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;improved alignment handling;improved;Similar;0.7301950454711914
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;rework freelist handling;slob;Similar;0.7283533215522766
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;can be slightly faster too;skip almost-full freelist pages completely;Similar;0.5674021244049072
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;"align them to word size
";it is best in practice;Similar;0.6203256845474243
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;SLOB to be used on SMP  ;allows;Similar;0.5209499597549438
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5340564250946045
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5353515148162842
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6612454056739807
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5283985733985901
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Redo a lot of comments;Also;Similar;0.6048382520675659
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5099143981933594
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;simplifies SLOB;at this point slob may be broken;Similar;0.6948801279067993
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;fix;SLOB=y && SMP=y fix;Similar;0.6518857479095459
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5228379964828491
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;The new rule is: page->lru is what you use;if you want to keep your page on a list;Similar;0.592754602432251
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.5651193857192993
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.5097043514251709
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5550084114074707
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.6020674109458923
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5642915964126587
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5862442255020142
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Fix gfp flags passed to lockdep;lockdep;Similar;0.5007021427154541
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5947288274765015
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.6036796569824219
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5660615563392639
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5643409490585327
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;fix lockup in slob_free()  ;lockup;Similar;0.5873714685440063
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5366804599761963
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.6022968292236328
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.668502151966095
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.6419484615325928
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;reduce external fragmentation by using three free lists;slob;Similar;0.5365680456161499
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5742616653442383
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.6583996415138245
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.7162848711013794
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5290631651878357
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;rework freelist handling;slob;Similar;0.5331742167472839
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.508979082107544
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;fix page order calculation on not 4KB page;-;Similar;0.5419877171516418
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6477931141853333
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6158472299575806
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5292835235595703
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.6314690709114075
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5757343769073486
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6320947408676147
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6599729061126709
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.535293698310852
The new rule is: page->lru is what you use;if you want to keep your page on a list;function naming changes;requires some;Similar;0.6183784008026123
The new rule is: page->lru is what you use;if you want to keep your page on a list;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.6372761726379395
The new rule is: page->lru is what you use;if you want to keep your page on a list;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.6973344087600708
The new rule is: page->lru is what you use;if you want to keep your page on a list;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5083475112915039
The new rule is: page->lru is what you use;if you want to keep your page on a list;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.727378249168396
The new rule is: page->lru is what you use;if you want to keep your page on a list;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.661979079246521
The new rule is: page->lru is what you use;if you want to keep your page on a list;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5046707987785339
The new rule is: page->lru is what you use;if you want to keep your page on a list;This will allow us to push more processing into common code later;improve readability;Similar;0.5115303993225098
The new rule is: page->lru is what you use;if you want to keep your page on a list;Fix early boot kernel crash;Slob;Similar;0.5316693782806396
The new rule is: page->lru is what you use;if you want to keep your page on a list;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5090787410736084
The new rule is: page->lru is what you use;if you want to keep your page on a list;cleans up some bitrot in slob.c;Also;Similar;0.501923143863678
The new rule is: page->lru is what you use;if you want to keep your page on a list;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6306717991828918
The new rule is: page->lru is what you use;if you want to keep your page on a list;Fix gfp flags passed to lockdep;lockdep;Similar;0.6570332646369934
The new rule is: page->lru is what you use;if you want to keep your page on a list;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6199058294296265
The new rule is: page->lru is what you use;if you want to keep your page on a list;Adding this mask;fixes the bug;Similar;0.5166460871696472
The new rule is: page->lru is what you use;if you want to keep your page on a list;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.6916220188140869
The new rule is: page->lru is what you use;if you want to keep your page on a list;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6695317029953003
The new rule is: page->lru is what you use;if you want to keep your page on a list;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5247528553009033
The new rule is: page->lru is what you use;if you want to keep your page on a list;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5550713539123535
The new rule is: page->lru is what you use;if you want to keep your page on a list;refactor code for future changes;Impact;Similar;0.5797778367996216
The new rule is: page->lru is what you use;if you want to keep your page on a list;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.618625819683075
The new rule is: page->lru is what you use;if you want to keep your page on a list;use tracepoints;kmemtrace;Similar;0.5546293258666992
The new rule is: page->lru is what you use;if you want to keep your page on a list;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6263817548751831
The new rule is: page->lru is what you use;if you want to keep your page on a list;fix typo in mm/slob.c;build fix;Similar;0.6143310070037842
The new rule is: page->lru is what you use;if you want to keep your page on a list;fix lockup in slob_free()  ;lockup;Similar;0.5860428810119629
The new rule is: page->lru is what you use;if you want to keep your page on a list;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5894886255264282
The new rule is: page->lru is what you use;if you want to keep your page on a list;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6099570989608765
The new rule is: page->lru is what you use;if you want to keep your page on a list;enable and use this tracer;To enable and use this tracer;Similar;0.5828843712806702
The new rule is: page->lru is what you use;if you want to keep your page on a list;fix bogus ksize calculation fix  ;SLOB;Similar;0.5020690560340881
The new rule is: page->lru is what you use;if you want to keep your page on a list;record page flag overlays explicitly;slob;Similar;0.5891116261482239
The new rule is: page->lru is what you use;if you want to keep your page on a list;reduce external fragmentation by using three free lists;slob;Similar;0.5227779150009155
The new rule is: page->lru is what you use;if you want to keep your page on a list;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.521942138671875
The new rule is: page->lru is what you use;if you want to keep your page on a list;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.6032805442810059
The new rule is: page->lru is what you use;if you want to keep your page on a list;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5778782963752747
The new rule is: page->lru is what you use;if you want to keep your page on a list;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.573002278804779
The new rule is: page->lru is what you use;if you want to keep your page on a list;rework freelist handling;slob;Similar;0.6577925682067871
The new rule is: page->lru is what you use;if you want to keep your page on a list;"align them to word size
";it is best in practice;Similar;0.5870254635810852
The new rule is: page->lru is what you use;if you want to keep your page on a list;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5942625999450684
The new rule is: page->lru is what you use;if you want to keep your page on a list;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5525020360946655
The new rule is: page->lru is what you use;if you want to keep your page on a list;SLOB to be used on SMP  ;allows;Similar;0.5801278352737427
The new rule is: page->lru is what you use;if you want to keep your page on a list;fix page order calculation on not 4KB page;-;Similar;0.5448209047317505
The new rule is: page->lru is what you use;if you want to keep your page on a list;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5634777545928955
The new rule is: page->lru is what you use;if you want to keep your page on a list;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5785602331161499
The new rule is: page->lru is what you use;if you want to keep your page on a list;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5661113858222961
The new rule is: page->lru is what you use;if you want to keep your page on a list;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5640621185302734
The new rule is: page->lru is what you use;if you want to keep your page on a list;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5235418081283569
The new rule is: page->lru is what you use;if you want to keep your page on a list;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5637224912643433
The new rule is: page->lru is what you use;if you want to keep your page on a list;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5152393579483032
The new rule is: page->lru is what you use;if you want to keep your page on a list;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6302585005760193
function naming changes;requires some;Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Similar;0.6696786880493164
function naming changes;requires some;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.7039527297019958
function naming changes;requires some;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5853928327560425
function naming changes;requires some;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5152905583381653
function naming changes;requires some;Improve trace accuracy;by correctly tracing reported size;Similar;0.5369372963905334
function naming changes;requires some;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5030344128608704
function naming changes;requires some;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.6815304756164551
function naming changes;requires some;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5619008541107178
function naming changes;requires some;This will allow us to push more processing into common code later;improve readability;Similar;0.5599155426025391
function naming changes;requires some;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5948982834815979
function naming changes;requires some;This affects RCU handling;somewhat;Similar;0.6974284648895264
function naming changes;requires some;Fix early boot kernel crash;Slob;Similar;0.5199292898178101
function naming changes;requires some;Remove various small accessors;various small;Similar;0.6655726432800293
function naming changes;requires some;cleans up some bitrot in slob.c;Also;Similar;0.5415047407150269
function naming changes;requires some;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5650390386581421
function naming changes;requires some;Fix gfp flags passed to lockdep;lockdep;Similar;0.694250226020813
function naming changes;requires some;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6858948469161987
function naming changes;requires some;Adding this mask;fixes the bug;Similar;0.6736470460891724
function naming changes;requires some;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5578318238258362
function naming changes;requires some;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6832914352416992
function naming changes;requires some;Remove kmemtrace ftrace plugin;tracing;Similar;0.6519467830657959
function naming changes;requires some;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.6259146928787231
function naming changes;requires some;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5446586608886719
function naming changes;requires some;refactor code for future changes;Impact;Similar;0.6733496189117432
function naming changes;requires some;use tracepoints;kmemtrace;Similar;0.5890473127365112
function naming changes;requires some;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6701637506484985
function naming changes;requires some;fix typo in mm/slob.c;build fix;Similar;0.6733907461166382
function naming changes;requires some;fix lockup in slob_free()  ;lockup;Similar;0.502037763595581
function naming changes;requires some;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6878271698951721
function naming changes;requires some;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5508537292480469
function naming changes;requires some;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5109952688217163
function naming changes;requires some;enable and use this tracer;To enable and use this tracer;Similar;0.5769528150558472
function naming changes;requires some;I find it more readable  ;personal opinion;Similar;0.5284330248832703
function naming changes;requires some;Drop it  ;if you want;Similar;0.5742967128753662
function naming changes;requires some;fix bogus ksize calculation fix  ;SLOB;Similar;0.6451756954193115
function naming changes;requires some;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.6149239540100098
function naming changes;requires some;fix bogus ksize calculation;bogus;Similar;0.6446360349655151
function naming changes;requires some;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5073329210281372
function naming changes;requires some;Fix to return wrong pointer;Fix;Similar;0.630465030670166
function naming changes;requires some;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.507451057434082
function naming changes;requires some;fix memory corruption;memory corruption;Similar;0.5753746628761292
function naming changes;requires some;remove useless ctor parameter and reorder parameters;useless;Similar;0.530795693397522
function naming changes;requires some;Handle that separately in krealloc();separately;Similar;0.6235941052436829
function naming changes;requires some;reduce list scanning;slob;Similar;0.6788085699081421
function naming changes;requires some;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5455678701400757
function naming changes;requires some;Cleanup zeroing allocations;Slab allocators;Similar;0.5293565988540649
function naming changes;requires some;improved alignment handling;improved;Similar;0.560148298740387
function naming changes;requires some;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5245144367218018
function naming changes;requires some;remove bigblock tracking;slob;Similar;0.6354503631591797
function naming changes;requires some;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5176399946212769
function naming changes;requires some;rework freelist handling;slob;Similar;0.6156799793243408
function naming changes;requires some;"align them to word size
";it is best in practice;Similar;0.5992696285247803
function naming changes;requires some;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6020992994308472
function naming changes;requires some;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6493722796440125
function naming changes;requires some;SLOB to be used on SMP  ;allows;Similar;0.6152754426002502
function naming changes;requires some;fix page order calculation on not 4KB page;-;Similar;0.5249837636947632
function naming changes;requires some;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6249467134475708
function naming changes;requires some;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6121567487716675
function naming changes;requires some;handle SLAB_PANIC flag;slob;Similar;0.6066251993179321
function naming changes;requires some;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.67339688539505
function naming changes;requires some;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5368192195892334
function naming changes;requires some;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6213893294334412
function naming changes;requires some;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5529544353485107
function naming changes;requires some;Redo a lot of comments;Also;Similar;0.6981028318405151
function naming changes;requires some;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6099408864974976
function naming changes;requires some;simplifies SLOB;at this point slob may be broken;Similar;0.5978829860687256
function naming changes;requires some;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5823451280593872
function naming changes;requires some;fix;SLOB=y && SMP=y fix;Similar;0.6749764680862427
function naming changes;requires some;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6013706922531128
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5185607671737671
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Fix gfp flags passed to lockdep;lockdep;Similar;0.5278067588806152
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6064672470092773
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.6156089305877686
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.616940975189209
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;refactor code for future changes;Impact;Similar;0.5018791556358337
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;use tracepoints;kmemtrace;Similar;0.5543445348739624
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5084766149520874
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;fix lockup in slob_free()  ;lockup;Similar;0.5074522495269775
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;enable and use this tracer;To enable and use this tracer;Similar;0.5836697816848755
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;fix bogus ksize calculation fix  ;SLOB;Similar;0.5722948908805847
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5668773055076599
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;fix bogus ksize calculation;bogus;Similar;0.5707463026046753
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.7265671491622925
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Fix to return wrong pointer;Fix;Similar;0.564206063747406
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5309940576553345
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5195062160491943
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;rework freelist handling;slob;Similar;0.5182517170906067
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5041900277137756
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5412077903747559
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6088311672210693
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5341737866401672
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;fix;SLOB=y && SMP=y fix;Similar;0.516701340675354
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.8565096259117126
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.6337740421295166
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5785318613052368
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.6418452262878418
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.7310835123062134
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.670648455619812
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6151720285415649
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Fix early boot kernel crash;Slob;Similar;0.5187381505966187
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5214191675186157
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;cleans up some bitrot in slob.c;Also;Similar;0.5178505182266235
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6428185701370239
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Fix gfp flags passed to lockdep;lockdep;Similar;0.6662110090255737
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6649629473686218
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Adding this mask;fixes the bug;Similar;0.5074042677879333
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.6592389345169067
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6890552043914795
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Remove kmemtrace ftrace plugin;tracing;Similar;0.6016441583633423
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6882592439651489
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.566327691078186
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;refactor code for future changes;Impact;Similar;0.5648325681686401
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.6355746388435364
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.7164421081542969
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;fix typo in mm/slob.c;build fix;Similar;0.6513245105743408
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;fix lockup in slob_free()  ;lockup;Similar;0.5385226607322693
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6908363699913025
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5090478658676147
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6081770658493042
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Fix to return wrong pointer;Fix;Similar;0.5688704252243042
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Handle that separately in krealloc();separately;Similar;0.5028214454650879
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5199983716011047
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;reduce list scanning;slob;Similar;0.5228872299194336
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5900822877883911
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5161595344543457
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5534782409667969
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5175495743751526
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.6864801645278931
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;remove bigblock tracking;slob;Similar;0.5647711753845215
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;"align them to word size
";it is best in practice;Similar;0.5097817182540894
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6364977359771729
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5604506134986877
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;SLOB to be used on SMP  ;allows;Similar;0.5298444032669067
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;fix page order calculation on not 4KB page;-;Similar;0.5549353957176208
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6288043856620789
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6671591997146606
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;handle SLAB_PANIC flag;slob;Similar;0.5475519895553589
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5319232940673828
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.550696849822998
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.502909779548645
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6027941107749939
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.682458758354187
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5845034718513489
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.7028818130493164
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6187125444412231
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6774917840957642
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.629341721534729
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5621576309204102
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Similar;0.5695364475250244
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Similar;0.5470137596130371
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.6879503726959229
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.73683100938797
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.6688361167907715
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;This will allow us to push more processing into common code later;improve readability;Similar;0.5226584672927856
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6778613328933716
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Fix early boot kernel crash;Slob;Similar;0.5867251753807068
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5202193260192871
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;cleans up some bitrot in slob.c;Also;Similar;0.5755870342254639
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6892938613891602
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Fix gfp flags passed to lockdep;lockdep;Similar;0.7126975059509277
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.695389986038208
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Adding this mask;fixes the bug;Similar;0.57953941822052
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.690703809261322
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.7544480562210083
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Remove kmemtrace ftrace plugin;tracing;Similar;0.6560553312301636
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5342656373977661
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.7099636197090149
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5395644903182983
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;refactor code for future changes;Impact;Similar;0.6115590333938599
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.6470714807510376
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;use tracepoints;kmemtrace;Similar;0.5527126789093018
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.7685621380805969
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;fix typo in mm/slob.c;build fix;Similar;0.7276909351348877
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;fix lockup in slob_free()  ;lockup;Similar;0.5753530263900757
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.7215937376022339
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6658217310905457
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;enable and use this tracer;To enable and use this tracer;Similar;0.5507848262786865
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;fix bogus ksize calculation fix  ;SLOB;Similar;0.5874606370925903
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.500813364982605
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;fix bogus ksize calculation;bogus;Similar;0.5773596167564392
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Fix to return wrong pointer;Fix;Similar;0.6142983436584473
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5399868488311768
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;fix memory corruption;memory corruption;Similar;0.5263544321060181
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Handle that separately in krealloc();separately;Similar;0.5328468680381775
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5166454315185547
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;reduce list scanning;slob;Similar;0.5856170058250427
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.6402281522750854
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5167245864868164
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5032832622528076
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5968635082244873
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;remove bigblock tracking;slob;Similar;0.6416792273521423
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.507540762424469
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;rework freelist handling;slob;Similar;0.5542891025543213
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;"align them to word size
";it is best in practice;Similar;0.5604982376098633
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6565025448799133
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6228386163711548
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5474647879600525
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;SLOB to be used on SMP  ;allows;Similar;0.5813734531402588
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;fix page order calculation on not 4KB page;-;Similar;0.5780042409896851
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6733541488647461
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6767144799232483
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;handle SLAB_PANIC flag;slob;Similar;0.6184794306755066
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6122004985809326
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5274820923805237
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5837017297744751
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6699296832084656
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.559894859790802
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.602523922920227
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Redo a lot of comments;Also;Similar;0.5168145895004272
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5896924138069153
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6901915073394775
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;simplifies SLOB;at this point slob may be broken;Similar;0.5027984976768494
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5387381911277771
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6807297468185425
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;fix;SLOB=y && SMP=y fix;Similar;0.5164580345153809
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.642748236656189
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5787683725357056
a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;Similar;0.5413334965705872
a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;This will allow us to push more processing into common code later;improve readability;Similar;0.5469435453414917
a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.595447301864624
a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5483648777008057
a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;can be slightly faster too;skip almost-full freelist pages completely;Similar;0.5264379978179932
a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;"Taking a spinlock in a destructor is a bit risky
";since the slab allocators may run the destructors anytime they decide a slab is no longer needed;Similar;0.5127345323562622
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5075563788414001
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5124950408935547
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.547026515007019
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.5984830856323242
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5034590363502502
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;This affects RCU handling;somewhat;Similar;0.5746192932128906
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5920486450195312
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Fix gfp flags passed to lockdep;lockdep;Similar;0.541483461856842
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5945436358451843
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Adding this mask;fixes the bug;Similar;0.5125331282615662
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5213696956634521
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6115151643753052
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5325976610183716
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.593673825263977
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;use tracepoints;kmemtrace;Similar;0.5126885175704956
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5083619952201843
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;fix typo in mm/slob.c;build fix;Similar;0.5772396922111511
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5781174898147583
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;enable and use this tracer;To enable and use this tracer;Similar;0.5044731497764587
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;record page flag overlays explicitly;slob;Similar;0.5705164074897766
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Handle that separately in krealloc();separately;Similar;0.5429285764694214
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5116977691650391
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5662672519683838
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.6768583059310913
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5372796058654785
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5253854990005493
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;Similar;0.5287858247756958
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;"align them to word size
";it is best in practice;Similar;0.5912009477615356
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5869050621986389
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5462861061096191
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;SLOB to be used on SMP  ;allows;Similar;0.5590878129005432
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;fix page order calculation on not 4KB page;-;Similar;0.514153778553009
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5086051821708679
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;handle SLAB_PANIC flag;slob;Similar;0.5585475564002991
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.558469831943512
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5495463609695435
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5531290173530579
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5303640365600586
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5732523202896118
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6759436130523682
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";Improve trace accuracy;by correctly tracing reported size;Similar;0.6232315301895142
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Similar;0.5945404171943665
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";kmalloc_track_caller() is correctly implemented;tracing the specified caller;Similar;0.6464580297470093
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.5303434133529663
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";fix lockup in slob_free()  ;lockup;Similar;0.6171107292175293
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5867940187454224
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";enable and use this tracer;To enable and use this tracer;Similar;0.57298743724823
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";I find it more readable  ;personal opinion;Similar;0.6169507503509521
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5959229469299316
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5351547002792358
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.6001412868499756
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5230465531349182
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";improved alignment handling;improved;Similar;0.6367433667182922
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5381888151168823
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";rework freelist handling;slob;Similar;0.6056351661682129
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";"align them to word size
";it is best in practice;Similar;0.533656120300293
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6777817010879517
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.531256914138794
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";simplifies SLOB;at this point slob may be broken;Similar;0.5689951181411743
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.5497596263885498
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;can be done in __kmem_cache_shutdown;What is done there;Similar;0.586037278175354
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Remove various small accessors;various small;Similar;0.5563356876373291
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.5070515871047974
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;cleans up some bitrot in slob.c;Also;Similar;0.5367406010627747
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Fix gfp flags passed to lockdep;lockdep;Similar;0.521054744720459
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.513126015663147
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Remove kmemtrace ftrace plugin;tracing;Similar;0.598304033279419
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5793479681015015
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.6133155822753906
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5304751396179199
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;fix typo in mm/slob.c;build fix;Similar;0.5142214298248291
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;fix lockup in slob_free()  ;lockup;Similar;0.5699890851974487
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5133915543556213
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5878639221191406
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Fix to return wrong pointer;Fix;Similar;0.6145148873329163
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.525743305683136
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5317468643188477
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;fix memory corruption;memory corruption;Similar;0.532952070236206
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;remove useless ctor parameter and reorder parameters;useless;Similar;0.5852355360984802
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;reduce list scanning;slob;Similar;0.6086310148239136
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Cleanup zeroing allocations;Slab allocators;Similar;0.5733217597007751
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5082523822784424
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6616678237915039
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5251879096031189
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;fix page order calculation on not 4KB page;-;Similar;0.5212434530258179
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6790401935577393
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5612825751304626
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5774052739143372
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5035930871963501
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5560358762741089
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6814416646957397
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5446602702140808
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.612545371055603
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.520319938659668
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5903280973434448
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5264089107513428
Improve trace accuracy;by correctly tracing reported size;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Similar;0.5586260557174683
Improve trace accuracy;by correctly tracing reported size;This will allow us to push more processing into common code later;improve readability;Similar;0.589407205581665
Improve trace accuracy;by correctly tracing reported size;This affects RCU handling;somewhat;Similar;0.6536513566970825
Improve trace accuracy;by correctly tracing reported size;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.6574440002441406
Improve trace accuracy;by correctly tracing reported size;Remove various small accessors;various small;Similar;0.5826351046562195
Improve trace accuracy;by correctly tracing reported size;They are no longer needed;They have become so simple;Similar;0.5630799531936646
Improve trace accuracy;by correctly tracing reported size;cleans up some bitrot in slob.c;Also;Similar;0.5018308162689209
Improve trace accuracy;by correctly tracing reported size;Adding this mask;fixes the bug;Similar;0.6368107199668884
Improve trace accuracy;by correctly tracing reported size;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6297935843467712
Improve trace accuracy;by correctly tracing reported size;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.6356621980667114
Improve trace accuracy;by correctly tracing reported size;refactor code for future changes;Impact;Similar;0.5671299695968628
Improve trace accuracy;by correctly tracing reported size;use tracepoints;kmemtrace;Similar;0.7426888942718506
Improve trace accuracy;by correctly tracing reported size;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5069785118103027
Improve trace accuracy;by correctly tracing reported size;fix typo in mm/slob.c;build fix;Similar;0.5438376665115356
Improve trace accuracy;by correctly tracing reported size;fix lockup in slob_free()  ;lockup;Similar;0.5634196996688843
Improve trace accuracy;by correctly tracing reported size;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.7929502725601196
Improve trace accuracy;by correctly tracing reported size;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.505623459815979
Improve trace accuracy;by correctly tracing reported size;enable and use this tracer;To enable and use this tracer;Similar;0.8528776168823242
Improve trace accuracy;by correctly tracing reported size;I find it more readable  ;personal opinion;Similar;0.778107762336731
Improve trace accuracy;by correctly tracing reported size;fix bogus ksize calculation fix  ;SLOB;Similar;0.5889127254486084
Improve trace accuracy;by correctly tracing reported size;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.6258347630500793
Improve trace accuracy;by correctly tracing reported size;fix bogus ksize calculation;bogus;Similar;0.6077194809913635
Improve trace accuracy;by correctly tracing reported size;Fix to return wrong pointer;Fix;Similar;0.5893729329109192
Improve trace accuracy;by correctly tracing reported size;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5879520177841187
Improve trace accuracy;by correctly tracing reported size;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.6210269331932068
Improve trace accuracy;by correctly tracing reported size;fix memory corruption;memory corruption;Similar;0.6533108949661255
Improve trace accuracy;by correctly tracing reported size;reduce list scanning;slob;Similar;0.5464242100715637
Improve trace accuracy;by correctly tracing reported size;Cleanup zeroing allocations;Slab allocators;Similar;0.5166124105453491
Improve trace accuracy;by correctly tracing reported size;improved alignment handling;improved;Similar;0.8486950397491455
Improve trace accuracy;by correctly tracing reported size;rework freelist handling;slob;Similar;0.6491772532463074
Improve trace accuracy;by correctly tracing reported size;can be slightly faster too;skip almost-full freelist pages completely;Similar;0.6382267475128174
Improve trace accuracy;by correctly tracing reported size;"align them to word size
";it is best in practice;Similar;0.5875716209411621
Improve trace accuracy;by correctly tracing reported size;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5706027150154114
Improve trace accuracy;by correctly tracing reported size;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5394644141197205
Improve trace accuracy;by correctly tracing reported size;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.515240490436554
Improve trace accuracy;by correctly tracing reported size;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.609602153301239
Improve trace accuracy;by correctly tracing reported size;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5735916495323181
Improve trace accuracy;by correctly tracing reported size;Redo a lot of comments;Also;Similar;0.5940566658973694
Improve trace accuracy;by correctly tracing reported size;simplifies SLOB;at this point slob may be broken;Similar;0.7368854284286499
Improve trace accuracy;by correctly tracing reported size;fix;SLOB=y && SMP=y fix;Similar;0.6897174715995789
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.6057816743850708
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5453217029571533
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Similar;0.5559791326522827
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6495003700256348
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Fix gfp flags passed to lockdep;lockdep;Similar;0.5325983762741089
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.600816011428833
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.6512899398803711
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6149461269378662
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5856805443763733
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5772006511688232
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5709444284439087
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;use tracepoints;kmemtrace;Similar;0.5065070986747742
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5431897640228271
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;fix typo in mm/slob.c;build fix;Similar;0.5831081867218018
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;fix lockup in slob_free()  ;lockup;Similar;0.5653451681137085
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5655622482299805
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.562631368637085
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;enable and use this tracer;To enable and use this tracer;Similar;0.5668450593948364
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;I find it more readable  ;personal opinion;Similar;0.523455023765564
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;record page flag overlays explicitly;slob;Similar;0.5665453672409058
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;reduce external fragmentation by using three free lists;slob;Similar;0.5003440380096436
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5805366635322571
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.6051977872848511
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5317865014076233
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.6202364563941956
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.6454092264175415
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;rework freelist handling;slob;Similar;0.5902633666992188
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;Similar;0.5304433107376099
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;"align them to word size
";it is best in practice;Similar;0.7138049602508545
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;fix page order calculation on not 4KB page;-;Similar;0.6073851585388184
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5485551357269287
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.526138424873352
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5260313749313354
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.509768009185791
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5139541625976562
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5212857723236084
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5592158436775208
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5092472434043884
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5473018288612366
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6459122896194458
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Similar;0.7460854649543762
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Similar;0.5714597702026367
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;can be done in __kmem_cache_shutdown;What is done there;Similar;0.5846556425094604
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Fix early boot kernel crash;Slob;Similar;0.518628716468811
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;cleans up some bitrot in slob.c;Also;Similar;0.5367307662963867
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6043423414230347
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Fix gfp flags passed to lockdep;lockdep;Similar;0.6621658802032471
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.7879407405853271
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Adding this mask;fixes the bug;Similar;0.5906328558921814
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5678674578666687
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.8170105814933777
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Remove kmemtrace ftrace plugin;tracing;Similar;0.6065287590026855
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6246585845947266
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5444833040237427
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;refactor code for future changes;Impact;Similar;0.548019528388977
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.6884167194366455
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;use tracepoints;kmemtrace;Similar;0.6647156476974487
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.7093499898910522
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;fix typo in mm/slob.c;build fix;Similar;0.693002462387085
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;fix lockup in slob_free()  ;lockup;Similar;0.5918189287185669
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.720855712890625
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6668933033943176
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;enable and use this tracer;To enable and use this tracer;Similar;0.650536298751831
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;fix bogus ksize calculation fix  ;SLOB;Similar;0.5705296993255615
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;fix bogus ksize calculation;bogus;Similar;0.5486671924591064
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Fix to return wrong pointer;Fix;Similar;0.5905794501304626
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Handle that separately in krealloc();separately;Similar;0.5453519225120544
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5804528594017029
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5155225396156311
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.6269786357879639
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;remove bigblock tracking;slob;Similar;0.5670347809791565
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;rework freelist handling;slob;Similar;0.580960214138031
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;"align them to word size
";it is best in practice;Similar;0.5418698787689209
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.697693943977356
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6030362844467163
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;SLOB to be used on SMP  ;allows;Similar;0.5920355319976807
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5864084959030151
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5573005676269531
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;handle SLAB_PANIC flag;slob;Similar;0.5910947322845459
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.7074222564697266
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5216768383979797
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6147173643112183
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5264064073562622
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Redo a lot of comments;Also;Similar;0.5013023614883423
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.57687908411026
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5153143405914307
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;simplifies SLOB;at this point slob may be broken;Similar;0.5103095173835754
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5306271314620972
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6485928297042847
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;fix;SLOB=y && SMP=y fix;Similar;0.5090051889419556
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6397277116775513
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;can be done in __kmem_cache_shutdown;What is done there;Similar;0.6596077084541321
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Fix early boot kernel crash;Slob;Similar;0.7154227495193481
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;cleans up some bitrot in slob.c;Also;Similar;0.6292425394058228
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6127073764801025
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Fix gfp flags passed to lockdep;lockdep;Similar;0.6236193180084229
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6400415897369385
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.558178722858429
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6499596834182739
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Remove kmemtrace ftrace plugin;tracing;Similar;0.6444676518440247
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.8026189804077148
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5776830911636353
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.6152865886688232
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5838688015937805
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.7582000494003296
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;fix typo in mm/slob.c;build fix;Similar;0.6675500869750977
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;fix lockup in slob_free()  ;lockup;Similar;0.6279194951057434
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6592115163803101
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5588703155517578
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6581501960754395
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;fix bogus ksize calculation fix  ;SLOB;Similar;0.6424381732940674
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;fix bogus ksize calculation;bogus;Similar;0.6180042028427124
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Fix to return wrong pointer;Fix;Similar;0.693428635597229
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5921367406845093
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;fix memory corruption;memory corruption;Similar;0.6429356336593628
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.557180643081665
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5317027568817139
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.6214630603790283
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.522072434425354
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6080803275108337
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5068304538726807
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;remove bigblock tracking;slob;Similar;0.5824460983276367
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6092504262924194
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6637827157974243
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.6714465022087097
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;fix page order calculation on not 4KB page;-;Similar;0.5505173206329346
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6747034788131714
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6380869150161743
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;handle SLAB_PANIC flag;slob;Similar;0.584588885307312
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5122255086898804
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.6471027135848999
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5805017948150635
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5877794027328491
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5641245245933533
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5711199045181274
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6039080023765564
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6272412538528442
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.599982500076294
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6406342387199402
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5953069925308228
kmalloc_track_caller() is correctly implemented;tracing the specified caller;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.5316702723503113
kmalloc_track_caller() is correctly implemented;tracing the specified caller;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5146276950836182
kmalloc_track_caller() is correctly implemented;tracing the specified caller;fix lockup in slob_free()  ;lockup;Similar;0.5368765592575073
kmalloc_track_caller() is correctly implemented;tracing the specified caller;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5283028483390808
kmalloc_track_caller() is correctly implemented;tracing the specified caller;enable and use this tracer;To enable and use this tracer;Similar;0.6510801315307617
kmalloc_track_caller() is correctly implemented;tracing the specified caller;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5091215968132019
kmalloc_track_caller() is correctly implemented;tracing the specified caller;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5083179473876953
kmalloc_track_caller() is correctly implemented;tracing the specified caller;improved alignment handling;improved;Similar;0.5580116510391235
kmalloc_track_caller() is correctly implemented;tracing the specified caller;rework freelist handling;slob;Similar;0.5184187889099121
kmalloc_track_caller() is correctly implemented;tracing the specified caller;simplifies SLOB;at this point slob may be broken;Similar;0.5038522481918335
This will allow us to push more processing into common code later;improve readability;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5129899978637695
This will allow us to push more processing into common code later;improve readability;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5765877962112427
This will allow us to push more processing into common code later;improve readability;refactor code for future changes;Impact;Similar;0.8159323930740356
This will allow us to push more processing into common code later;improve readability;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5221326351165771
This will allow us to push more processing into common code later;improve readability;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5927589535713196
This will allow us to push more processing into common code later;improve readability;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5533076524734497
This will allow us to push more processing into common code later;improve readability;enable and use this tracer;To enable and use this tracer;Similar;0.5355798006057739
This will allow us to push more processing into common code later;improve readability;I find it more readable  ;personal opinion;Similar;0.566259503364563
This will allow us to push more processing into common code later;improve readability;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5713009834289551
This will allow us to push more processing into common code later;improve readability;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5894944667816162
This will allow us to push more processing into common code later;improve readability;improved alignment handling;improved;Similar;0.5793536901473999
This will allow us to push more processing into common code later;improve readability;rework freelist handling;slob;Similar;0.5316932797431946
This will allow us to push more processing into common code later;improve readability;can be slightly faster too;skip almost-full freelist pages completely;Similar;0.5195453763008118
This will allow us to push more processing into common code later;improve readability;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5610678791999817
This will allow us to push more processing into common code later;improve readability;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5560407042503357
This will allow us to push more processing into common code later;improve readability;Redo a lot of comments;Also;Similar;0.511806845664978
This will allow us to push more processing into common code later;improve readability;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5041356086730957
can be done in __kmem_cache_shutdown;What is done there;This affects RCU handling;somewhat;Similar;0.5219945311546326
can be done in __kmem_cache_shutdown;What is done there;Fix early boot kernel crash;Slob;Similar;0.5432180166244507
can be done in __kmem_cache_shutdown;What is done there;Remove various small accessors;various small;Similar;0.6556257009506226
can be done in __kmem_cache_shutdown;What is done there;They are no longer needed;They have become so simple;Similar;0.517863392829895
can be done in __kmem_cache_shutdown;What is done there;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.5596302151679993
can be done in __kmem_cache_shutdown;What is done there;cleans up some bitrot in slob.c;Also;Similar;0.590671181678772
can be done in __kmem_cache_shutdown;What is done there;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5592108964920044
can be done in __kmem_cache_shutdown;What is done there;Fix gfp flags passed to lockdep;lockdep;Similar;0.6299917697906494
can be done in __kmem_cache_shutdown;What is done there;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6273940801620483
can be done in __kmem_cache_shutdown;What is done there;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5555661916732788
can be done in __kmem_cache_shutdown;What is done there;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6970396637916565
can be done in __kmem_cache_shutdown;What is done there;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5846199989318848
can be done in __kmem_cache_shutdown;What is done there;Remove kmemtrace ftrace plugin;tracing;Similar;0.8111629486083984
can be done in __kmem_cache_shutdown;What is done there;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.7783864736557007
can be done in __kmem_cache_shutdown;What is done there;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.7621328830718994
can be done in __kmem_cache_shutdown;What is done there;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5366224050521851
can be done in __kmem_cache_shutdown;What is done there;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.5637290477752686
can be done in __kmem_cache_shutdown;What is done there;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5010658502578735
can be done in __kmem_cache_shutdown;What is done there;use tracepoints;kmemtrace;Similar;0.544058620929718
can be done in __kmem_cache_shutdown;What is done there;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6548376083374023
can be done in __kmem_cache_shutdown;What is done there;fix typo in mm/slob.c;build fix;Similar;0.7027235627174377
can be done in __kmem_cache_shutdown;What is done there;fix lockup in slob_free()  ;lockup;Similar;0.6945544481277466
can be done in __kmem_cache_shutdown;What is done there;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5604845285415649
can be done in __kmem_cache_shutdown;What is done there;Drop it  ;if you want;Similar;0.6691919565200806
can be done in __kmem_cache_shutdown;What is done there;fix bogus ksize calculation fix  ;SLOB;Similar;0.6502659320831299
can be done in __kmem_cache_shutdown;What is done there;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5230047106742859
can be done in __kmem_cache_shutdown;What is done there;fix bogus ksize calculation;bogus;Similar;0.6710693836212158
can be done in __kmem_cache_shutdown;What is done there;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.7081271409988403
can be done in __kmem_cache_shutdown;What is done there;Fix to return wrong pointer;Fix;Similar;0.621226966381073
can be done in __kmem_cache_shutdown;What is done there;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5703195333480835
can be done in __kmem_cache_shutdown;What is done there;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5865657329559326
can be done in __kmem_cache_shutdown;What is done there;fix memory corruption;memory corruption;Similar;0.6413205862045288
can be done in __kmem_cache_shutdown;What is done there;remove useless ctor parameter and reorder parameters;useless;Similar;0.6876451969146729
can be done in __kmem_cache_shutdown;What is done there;Handle that separately in krealloc();separately;Similar;0.6465777158737183
can be done in __kmem_cache_shutdown;What is done there;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5697180032730103
can be done in __kmem_cache_shutdown;What is done there;reduce list scanning;slob;Similar;0.7560475468635559
can be done in __kmem_cache_shutdown;What is done there;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.6470497846603394
can be done in __kmem_cache_shutdown;What is done there;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5157153010368347
can be done in __kmem_cache_shutdown;What is done there;Cleanup zeroing allocations;Slab allocators;Similar;0.7237122654914856
can be done in __kmem_cache_shutdown;What is done there;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5279890298843384
can be done in __kmem_cache_shutdown;What is done there;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6631808280944824
can be done in __kmem_cache_shutdown;What is done there;remove bigblock tracking;slob;Similar;0.7377164959907532
can be done in __kmem_cache_shutdown;What is done there;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5824090242385864
can be done in __kmem_cache_shutdown;What is done there;rework freelist handling;slob;Similar;0.5561151504516602
can be done in __kmem_cache_shutdown;What is done there;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5527105331420898
can be done in __kmem_cache_shutdown;What is done there;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6520721316337585
can be done in __kmem_cache_shutdown;What is done there;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6137893795967102
can be done in __kmem_cache_shutdown;What is done there;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.6063268184661865
can be done in __kmem_cache_shutdown;What is done there;SLOB to be used on SMP  ;allows;Similar;0.5933572053909302
can be done in __kmem_cache_shutdown;What is done there;fix page order calculation on not 4KB page;-;Similar;0.6200027465820312
can be done in __kmem_cache_shutdown;What is done there;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5971933007240295
can be done in __kmem_cache_shutdown;What is done there;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.7773526906967163
can be done in __kmem_cache_shutdown;What is done there;handle SLAB_PANIC flag;slob;Similar;0.6436654329299927
can be done in __kmem_cache_shutdown;What is done there;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5032332539558411
can be done in __kmem_cache_shutdown;What is done there;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.539936363697052
can be done in __kmem_cache_shutdown;What is done there;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5996891260147095
can be done in __kmem_cache_shutdown;What is done there;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5679786205291748
can be done in __kmem_cache_shutdown;What is done there;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.7056739330291748
can be done in __kmem_cache_shutdown;What is done there;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5480037331581116
can be done in __kmem_cache_shutdown;What is done there;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6122642755508423
can be done in __kmem_cache_shutdown;What is done there;simplifies SLOB;at this point slob may be broken;Similar;0.5506279468536377
can be done in __kmem_cache_shutdown;What is done there;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5788888335227966
can be done in __kmem_cache_shutdown;What is done there;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.8259381055831909
can be done in __kmem_cache_shutdown;What is done there;fix;SLOB=y && SMP=y fix;Similar;0.5526623725891113
can be done in __kmem_cache_shutdown;What is done there;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.516524076461792
can be done in __kmem_cache_shutdown;What is done there;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.572529673576355
This affects RCU handling;somewhat;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Similar;0.5501338243484497
This affects RCU handling;somewhat;Remove various small accessors;various small;Similar;0.648934543132782
This affects RCU handling;somewhat;They are no longer needed;They have become so simple;Similar;0.6098209619522095
This affects RCU handling;somewhat;Fix gfp flags passed to lockdep;lockdep;Similar;0.6027959585189819
This affects RCU handling;somewhat;Adding this mask;fixes the bug;Similar;0.6294742226600647
This affects RCU handling;somewhat;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6077627539634705
This affects RCU handling;somewhat;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5520125031471252
This affects RCU handling;somewhat;use tracepoints;kmemtrace;Similar;0.620315432548523
This affects RCU handling;somewhat;fix typo in mm/slob.c;build fix;Similar;0.6040246486663818
This affects RCU handling;somewhat;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.6548010110855103
This affects RCU handling;somewhat;enable and use this tracer;To enable and use this tracer;Similar;0.6637036204338074
This affects RCU handling;somewhat;I find it more readable  ;personal opinion;Similar;0.6447742581367493
This affects RCU handling;somewhat;Drop it  ;if you want;Similar;0.5830902457237244
This affects RCU handling;somewhat;fix bogus ksize calculation fix  ;SLOB;Similar;0.6370143890380859
This affects RCU handling;somewhat;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.6543323993682861
This affects RCU handling;somewhat;fix bogus ksize calculation;bogus;Similar;0.659896969795227
This affects RCU handling;somewhat;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5213338136672974
This affects RCU handling;somewhat;record page flag overlays explicitly;slob;Similar;0.5336422920227051
This affects RCU handling;somewhat;Fix to return wrong pointer;Fix;Similar;0.5884969234466553
This affects RCU handling;somewhat;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.548943042755127
This affects RCU handling;somewhat;fix memory corruption;memory corruption;Similar;0.5780341029167175
This affects RCU handling;somewhat;Handle that separately in krealloc();separately;Similar;0.5673410892486572
This affects RCU handling;somewhat;reduce list scanning;slob;Similar;0.5843232870101929
This affects RCU handling;somewhat;Cleanup zeroing allocations;Slab allocators;Similar;0.5332908034324646
This affects RCU handling;somewhat;improved alignment handling;improved;Similar;0.6323242783546448
This affects RCU handling;somewhat;remove bigblock tracking;slob;Similar;0.5338497161865234
This affects RCU handling;somewhat;rework freelist handling;slob;Similar;0.5801690220832825
This affects RCU handling;somewhat;"align them to word size
";it is best in practice;Similar;0.5725295543670654
This affects RCU handling;somewhat;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5153082609176636
This affects RCU handling;somewhat;"Taking a spinlock in a destructor is a bit risky
";since the slab allocators may run the destructors anytime they decide a slab is no longer needed;Similar;0.5128710269927979
This affects RCU handling;somewhat;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6562726497650146
This affects RCU handling;somewhat;SLOB to be used on SMP  ;allows;Similar;0.6144192814826965
This affects RCU handling;somewhat;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.531704306602478
This affects RCU handling;somewhat;handle SLAB_PANIC flag;slob;Similar;0.6060364246368408
This affects RCU handling;somewhat;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5532656908035278
This affects RCU handling;somewhat;Redo a lot of comments;Also;Similar;0.6521633267402649
This affects RCU handling;somewhat;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5583878755569458
This affects RCU handling;somewhat;simplifies SLOB;at this point slob may be broken;Similar;0.7137308120727539
This affects RCU handling;somewhat;fix;SLOB=y && SMP=y fix;Similar;0.744585394859314
This affects RCU handling;somewhat;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5151195526123047
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Remove various small accessors;various small;Similar;0.5522129535675049
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();They are no longer needed;They have become so simple;Similar;0.6025368571281433
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Adding this mask;fixes the bug;Similar;0.6124695539474487
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.7465468645095825
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();refactor code for future changes;Impact;Similar;0.5049502849578857
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();use tracepoints;kmemtrace;Similar;0.5842208862304688
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.8300827741622925
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();enable and use this tracer;To enable and use this tracer;Similar;0.7208558320999146
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();I find it more readable  ;personal opinion;Similar;0.7635710835456848
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Drop it  ;if you want;Similar;0.5781809091567993
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5777139663696289
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5076974034309387
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();improved alignment handling;improved;Similar;0.6912703514099121
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();rework freelist handling;slob;Similar;0.6212587356567383
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();can be slightly faster too;skip almost-full freelist pages completely;Similar;0.7331100702285767
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();"align them to word size
";it is best in practice;Similar;0.5818613171577454
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Code size goes up about 1K;dynamic memory usage should be lower even on relatively small memory systems;Similar;0.529751718044281
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6091629862785339
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Redo a lot of comments;Also;Similar;0.532988429069519
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();simplifies SLOB;at this point slob may be broken;Similar;0.6843883395195007
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();fix;SLOB=y && SMP=y fix;Similar;0.7010115385055542
Fix early boot kernel crash;Slob;cleans up some bitrot in slob.c;Also;Similar;0.5792322158813477
Fix early boot kernel crash;Slob;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.6529893279075623
Fix early boot kernel crash;Slob;Fix gfp flags passed to lockdep;lockdep;Similar;0.5619293451309204
Fix early boot kernel crash;Slob;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5364265441894531
Fix early boot kernel crash;Slob;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5954797267913818
Fix early boot kernel crash;Slob;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5132254362106323
Fix early boot kernel crash;Slob;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6644262075424194
Fix early boot kernel crash;Slob;refactor code for future changes;Impact;Similar;0.5228259563446045
Fix early boot kernel crash;Slob;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5609124898910522
Fix early boot kernel crash;Slob;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6191955804824829
Fix early boot kernel crash;Slob;fix typo in mm/slob.c;build fix;Similar;0.6080679893493652
Fix early boot kernel crash;Slob;fix lockup in slob_free()  ;lockup;Similar;0.560185432434082
Fix early boot kernel crash;Slob;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5145715475082397
Fix early boot kernel crash;Slob;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6070069074630737
Fix early boot kernel crash;Slob;fix bogus ksize calculation fix  ;SLOB;Similar;0.5979639887809753
Fix early boot kernel crash;Slob;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5112342834472656
Fix early boot kernel crash;Slob;fix bogus ksize calculation;bogus;Similar;0.5750559568405151
Fix early boot kernel crash;Slob;Fix to return wrong pointer;Fix;Similar;0.6395159959793091
Fix early boot kernel crash;Slob;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5436620712280273
Fix early boot kernel crash;Slob;fix memory corruption;memory corruption;Similar;0.652658224105835
Fix early boot kernel crash;Slob;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.558678388595581
Fix early boot kernel crash;Slob;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.518123984336853
Fix early boot kernel crash;Slob;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6705292463302612
Fix early boot kernel crash;Slob;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5689314603805542
Fix early boot kernel crash;Slob;fix page order calculation on not 4KB page;-;Similar;0.528319239616394
Fix early boot kernel crash;Slob;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5925816297531128
Fix early boot kernel crash;Slob;handle SLAB_PANIC flag;slob;Similar;0.5000298619270325
Fix early boot kernel crash;Slob;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.542109489440918
Fix early boot kernel crash;Slob;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5972516536712646
Fix early boot kernel crash;Slob;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5843779444694519
Fix early boot kernel crash;Slob;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5493091940879822
Fix early boot kernel crash;Slob;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.567022442817688
Fix early boot kernel crash;Slob;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5316941142082214
Fix early boot kernel crash;Slob;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5687961578369141
Fix early boot kernel crash;Slob;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5939558148384094
Remove various small accessors;various small;They are no longer needed;They have become so simple;Similar;0.7835296392440796
Remove various small accessors;various small;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.6046992540359497
Remove various small accessors;various small;cleans up some bitrot in slob.c;Also;Similar;0.5125391483306885
Remove various small accessors;various small;Fix gfp flags passed to lockdep;lockdep;Similar;0.526138424873352
Remove various small accessors;various small;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.5080170035362244
Remove various small accessors;various small;Adding this mask;fixes the bug;Similar;0.581012487411499
Remove various small accessors;various small;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5441678762435913
Remove various small accessors;various small;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5051201581954956
Remove various small accessors;various small;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5006843209266663
Remove various small accessors;various small;Remove kmemtrace ftrace plugin;tracing;Similar;0.6804966926574707
Remove various small accessors;various small;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.7981492877006531
Remove various small accessors;various small;refactor code for future changes;Impact;Similar;0.5168971419334412
Remove various small accessors;various small;use tracepoints;kmemtrace;Similar;0.5875897407531738
Remove various small accessors;various small;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5470868349075317
Remove various small accessors;various small;fix typo in mm/slob.c;build fix;Similar;0.5778298377990723
Remove various small accessors;various small;fix lockup in slob_free()  ;lockup;Similar;0.5866258144378662
Remove various small accessors;various small;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.6504172682762146
Remove various small accessors;various small;enable and use this tracer;To enable and use this tracer;Similar;0.5492194294929504
Remove various small accessors;various small;I find it more readable  ;personal opinion;Similar;0.5699406862258911
Remove various small accessors;various small;Drop it  ;if you want;Similar;0.7780832648277283
Remove various small accessors;various small;fix bogus ksize calculation fix  ;SLOB;Similar;0.6378891468048096
Remove various small accessors;various small;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5779514312744141
Remove various small accessors;various small;fix bogus ksize calculation;bogus;Similar;0.6829465627670288
Remove various small accessors;various small;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.7120645046234131
Remove various small accessors;various small;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5913379788398743
Remove various small accessors;various small;Fix to return wrong pointer;Fix;Similar;0.6157758235931396
Remove various small accessors;various small;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5948908925056458
Remove various small accessors;various small;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.7290365695953369
Remove various small accessors;various small;fix memory corruption;memory corruption;Similar;0.5662018656730652
Remove various small accessors;various small;remove useless ctor parameter and reorder parameters;useless;Similar;0.7329088449478149
Remove various small accessors;various small;Handle that separately in krealloc();separately;Similar;0.684145987033844
Remove various small accessors;various small;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.543061375617981
Remove various small accessors;various small;reduce list scanning;slob;Similar;0.8466690182685852
Remove various small accessors;various small;Cleanup zeroing allocations;Slab allocators;Similar;0.7689000368118286
Remove various small accessors;various small;improved alignment handling;improved;Similar;0.515316903591156
Remove various small accessors;various small;remove bigblock tracking;slob;Similar;0.6765748262405396
Remove various small accessors;various small;rework freelist handling;slob;Similar;0.6025105714797974
Remove various small accessors;various small;can be slightly faster too;skip almost-full freelist pages completely;Similar;0.5528656244277954
Remove various small accessors;various small;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6255776882171631
Remove various small accessors;various small;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5825910568237305
Remove various small accessors;various small;SLOB to be used on SMP  ;allows;Similar;0.6199861764907837
Remove various small accessors;various small;fix page order calculation on not 4KB page;-;Similar;0.5797516703605652
Remove various small accessors;various small;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5979280471801758
Remove various small accessors;various small;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6363162398338318
Remove various small accessors;various small;handle SLAB_PANIC flag;slob;Similar;0.5427201986312866
Remove various small accessors;various small;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.543026328086853
Remove various small accessors;various small;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5862393379211426
Remove various small accessors;various small;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5018030405044556
Remove various small accessors;various small;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5114424228668213
Remove various small accessors;various small;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6143209934234619
Remove various small accessors;various small;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.5203630924224854
Remove various small accessors;various small;Redo a lot of comments;Also;Similar;0.608427882194519
Remove various small accessors;various small;simplifies SLOB;at this point slob may be broken;Similar;0.6613414287567139
Remove various small accessors;various small;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6004294753074646
Remove various small accessors;various small;fix;SLOB=y && SMP=y fix;Similar;0.7109504342079163
Remove various small accessors;various small;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5029600858688354
They are no longer needed;They have become so simple;No need to zero mapping since it is no longer in use;it is no longer in use;Similar;0.7900137305259705
They are no longer needed;They have become so simple;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5946136713027954
They are no longer needed;They have become so simple;Remove kmemtrace ftrace plugin;tracing;Similar;0.5652726888656616
They are no longer needed;They have become so simple;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.7435311079025269
They are no longer needed;They have become so simple;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.6536173820495605
They are no longer needed;They have become so simple;I find it more readable  ;personal opinion;Similar;0.6002644896507263
They are no longer needed;They have become so simple;Drop it  ;if you want;Similar;0.7741975784301758
They are no longer needed;They have become so simple;fix bogus ksize calculation fix  ;SLOB;Similar;0.5388190746307373
They are no longer needed;They have become so simple;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5553984642028809
They are no longer needed;They have become so simple;fix bogus ksize calculation;bogus;Similar;0.5884469747543335
They are no longer needed;They have become so simple;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.749584436416626
They are no longer needed;They have become so simple;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.6177773475646973
They are no longer needed;They have become so simple;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.515673041343689
They are no longer needed;They have become so simple;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.719393789768219
They are no longer needed;They have become so simple;remove useless ctor parameter and reorder parameters;useless;Similar;0.7038241624832153
They are no longer needed;They have become so simple;Handle that separately in krealloc();separately;Similar;0.5400971174240112
They are no longer needed;They have become so simple;reduce list scanning;slob;Similar;0.7529069781303406
They are no longer needed;They have become so simple;Cleanup zeroing allocations;Slab allocators;Similar;0.7169575691223145
They are no longer needed;They have become so simple;improved alignment handling;improved;Similar;0.5159581303596497
They are no longer needed;They have become so simple;remove bigblock tracking;slob;Similar;0.5686545372009277
They are no longer needed;They have become so simple;can be slightly faster too;skip almost-full freelist pages completely;Similar;0.5101858377456665
They are no longer needed;They have become so simple;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5619246959686279
They are no longer needed;They have become so simple;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5724800825119019
They are no longer needed;They have become so simple;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5148825645446777
They are no longer needed;They have become so simple;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.7004369497299194
They are no longer needed;They have become so simple;simplifies SLOB;at this point slob may be broken;Similar;0.5953748226165771
They are no longer needed;They have become so simple;fix;SLOB=y && SMP=y fix;Similar;0.6477549076080322
No need to zero mapping since it is no longer in use;it is no longer in use;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.7219869494438171
No need to zero mapping since it is no longer in use;it is no longer in use;Remove kmemtrace ftrace plugin;tracing;Similar;0.5827893018722534
No need to zero mapping since it is no longer in use;it is no longer in use;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.6672636866569519
No need to zero mapping since it is no longer in use;it is no longer in use;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.569450855255127
No need to zero mapping since it is no longer in use;it is no longer in use;fix lockup in slob_free()  ;lockup;Similar;0.519055962562561
No need to zero mapping since it is no longer in use;it is no longer in use;Drop it  ;if you want;Similar;0.6368701457977295
No need to zero mapping since it is no longer in use;it is no longer in use;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.7938886880874634
No need to zero mapping since it is no longer in use;it is no longer in use;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.6052131652832031
No need to zero mapping since it is no longer in use;it is no longer in use;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5323513150215149
No need to zero mapping since it is no longer in use;it is no longer in use;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.6234913468360901
No need to zero mapping since it is no longer in use;it is no longer in use;remove useless ctor parameter and reorder parameters;useless;Similar;0.7608774900436401
No need to zero mapping since it is no longer in use;it is no longer in use;reduce list scanning;slob;Similar;0.6865296959877014
No need to zero mapping since it is no longer in use;it is no longer in use;Cleanup zeroing allocations;Slab allocators;Similar;0.7087505459785461
No need to zero mapping since it is no longer in use;it is no longer in use;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6370828151702881
No need to zero mapping since it is no longer in use;it is no longer in use;fix page order calculation on not 4KB page;-;Similar;0.5442445874214172
No need to zero mapping since it is no longer in use;it is no longer in use;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5911637544631958
No need to zero mapping since it is no longer in use;it is no longer in use;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6637426614761353
No need to zero mapping since it is no longer in use;it is no longer in use;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.8062189817428589
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;cleans up some bitrot in slob.c;Also;Similar;0.6467103958129883
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5313875675201416
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5095868706703186
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5109281539916992
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5603172779083252
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6375869512557983
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.575610339641571
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.6756010055541992
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5540536642074585
cleans up some bitrot in slob.c;Also;move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Similar;0.5355368852615356
cleans up some bitrot in slob.c;Also;Fix gfp flags passed to lockdep;lockdep;Similar;0.5965545773506165
cleans up some bitrot in slob.c;Also;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6131433248519897
cleans up some bitrot in slob.c;Also;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.5380313992500305
cleans up some bitrot in slob.c;Also;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5899215936660767
cleans up some bitrot in slob.c;Also;Remove kmemtrace ftrace plugin;tracing;Similar;0.5039551854133606
cleans up some bitrot in slob.c;Also;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5842630863189697
cleans up some bitrot in slob.c;Also;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5132530331611633
cleans up some bitrot in slob.c;Also;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6565768718719482
cleans up some bitrot in slob.c;Also;fix typo in mm/slob.c;build fix;Similar;0.7331709861755371
cleans up some bitrot in slob.c;Also;fix lockup in slob_free()  ;lockup;Similar;0.6947813630104065
cleans up some bitrot in slob.c;Also;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5972985625267029
cleans up some bitrot in slob.c;Also;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6047333478927612
cleans up some bitrot in slob.c;Also;enable and use this tracer;To enable and use this tracer;Similar;0.5294214487075806
cleans up some bitrot in slob.c;Also;fix bogus ksize calculation fix  ;SLOB;Similar;0.5429599285125732
cleans up some bitrot in slob.c;Also;fix bogus ksize calculation;bogus;Similar;0.529833197593689
cleans up some bitrot in slob.c;Also;Fix to return wrong pointer;Fix;Similar;0.5648968815803528
cleans up some bitrot in slob.c;Also;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5739606618881226
cleans up some bitrot in slob.c;Also;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5059983134269714
cleans up some bitrot in slob.c;Also;fix memory corruption;memory corruption;Similar;0.649770200252533
cleans up some bitrot in slob.c;Also;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5948891639709473
cleans up some bitrot in slob.c;Also;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5028654932975769
cleans up some bitrot in slob.c;Also;rework freelist handling;slob;Similar;0.5420320630073547
cleans up some bitrot in slob.c;Also;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5379810333251953
cleans up some bitrot in slob.c;Also;"Taking a spinlock in a destructor is a bit risky
";since the slab allocators may run the destructors anytime they decide a slab is no longer needed;Similar;0.5695160627365112
cleans up some bitrot in slob.c;Also;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6049262285232544
cleans up some bitrot in slob.c;Also;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5237756967544556
cleans up some bitrot in slob.c;Also;SLOB to be used on SMP  ;allows;Similar;0.5575002431869507
cleans up some bitrot in slob.c;Also;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5905051231384277
cleans up some bitrot in slob.c;Also;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5940364599227905
cleans up some bitrot in slob.c;Also;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.6303051710128784
cleans up some bitrot in slob.c;Also;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.633537232875824
cleans up some bitrot in slob.c;Also;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.6890157461166382
cleans up some bitrot in slob.c;Also;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5567137002944946
cleans up some bitrot in slob.c;Also;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.552223801612854
cleans up some bitrot in slob.c;Also;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.542263925075531
cleans up some bitrot in slob.c;Also;simplifies SLOB;at this point slob may be broken;Similar;0.5257788300514221
cleans up some bitrot in slob.c;Also;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5225793719291687
cleans up some bitrot in slob.c;Also;SLOB saving nearly half a megabyte of RAM;Comparison for otherwise identical builds;Similar;0.5031108260154724
cleans up some bitrot in slob.c;Also;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5490350723266602
cleans up some bitrot in slob.c;Also;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.6414768099784851
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Fix gfp flags passed to lockdep;lockdep;Similar;0.6228647232055664
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.6407105922698975
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.6821609139442444
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.5713086724281311
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Remove kmemtrace ftrace plugin;tracing;Similar;0.517219066619873
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5850659608840942
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;refactor code for future changes;Impact;Similar;0.5003035068511963
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.8208510875701904
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6039571762084961
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;fix typo in mm/slob.c;build fix;Similar;0.63266921043396
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;fix lockup in slob_free()  ;lockup;Similar;0.5153471231460571
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6779284477233887
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6923073530197144
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;fix bogus ksize calculation fix  ;SLOB;Similar;0.5132973790168762
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;fix bogus ksize calculation;bogus;Similar;0.5008271336555481
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;fix memory corruption;memory corruption;Similar;0.5196047425270081
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5416035056114197
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5999577641487122
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5413807034492493
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5125296115875244
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5022640228271484
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5519294738769531
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;rework freelist handling;slob;Similar;0.5419515371322632
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5000593662261963
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5960903763771057
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;fix page order calculation on not 4KB page;-;Similar;0.5547780990600586
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5953975915908813
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5215591788291931
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5684961080551147
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5948092937469482
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5081503391265869
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5603376626968384
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5526415109634399
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.647621214389801
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.65330570936203
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6211508512496948
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6247081756591797
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.6076860427856445
Fix gfp flags passed to lockdep;lockdep;Ran a ktest.pl config_bisect;Came up with this config as the problem;Similar;0.699015736579895
Fix gfp flags passed to lockdep;lockdep;Adding this mask;fixes the bug;Similar;0.5374348163604736
Fix gfp flags passed to lockdep;lockdep;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.7711009979248047
Fix gfp flags passed to lockdep;lockdep;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.7403160929679871
Fix gfp flags passed to lockdep;lockdep;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.5105071067810059
Fix gfp flags passed to lockdep;lockdep;Remove kmemtrace ftrace plugin;tracing;Similar;0.6376867890357971
Fix gfp flags passed to lockdep;lockdep;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5376410484313965
Fix gfp flags passed to lockdep;lockdep;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6628240346908569
Fix gfp flags passed to lockdep;lockdep;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5390428304672241
Fix gfp flags passed to lockdep;lockdep;refactor code for future changes;Impact;Similar;0.5589736700057983
Fix gfp flags passed to lockdep;lockdep;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.6279395818710327
Fix gfp flags passed to lockdep;lockdep;use tracepoints;kmemtrace;Similar;0.6146785020828247
Fix gfp flags passed to lockdep;lockdep;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.7777502536773682
Fix gfp flags passed to lockdep;lockdep;fix typo in mm/slob.c;build fix;Similar;0.767754077911377
Fix gfp flags passed to lockdep;lockdep;fix lockup in slob_free()  ;lockup;Similar;0.6658125519752502
Fix gfp flags passed to lockdep;lockdep;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6616925001144409
Fix gfp flags passed to lockdep;lockdep;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.652653157711029
Fix gfp flags passed to lockdep;lockdep;enable and use this tracer;To enable and use this tracer;Similar;0.5804210901260376
Fix gfp flags passed to lockdep;lockdep;fix bogus ksize calculation fix  ;SLOB;Similar;0.6809258460998535
Fix gfp flags passed to lockdep;lockdep;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5723940134048462
Fix gfp flags passed to lockdep;lockdep;fix bogus ksize calculation;bogus;Similar;0.6667011976242065
Fix gfp flags passed to lockdep;lockdep;record page flag overlays explicitly;slob;Similar;0.5314251780509949
Fix gfp flags passed to lockdep;lockdep;Fix to return wrong pointer;Fix;Similar;0.6255384683609009
Fix gfp flags passed to lockdep;lockdep;reduce external fragmentation by using three free lists;slob;Similar;0.5389577150344849
Fix gfp flags passed to lockdep;lockdep;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5376701354980469
Fix gfp flags passed to lockdep;lockdep;fix memory corruption;memory corruption;Similar;0.5560680627822876
Fix gfp flags passed to lockdep;lockdep;Handle that separately in krealloc();separately;Similar;0.5741317272186279
Fix gfp flags passed to lockdep;lockdep;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6270039081573486
Fix gfp flags passed to lockdep;lockdep;reduce list scanning;slob;Similar;0.589203953742981
Fix gfp flags passed to lockdep;lockdep;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5180879235267639
Fix gfp flags passed to lockdep;lockdep;Cleanup zeroing allocations;Slab allocators;Similar;0.5230274200439453
Fix gfp flags passed to lockdep;lockdep;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5387164354324341
Fix gfp flags passed to lockdep;lockdep;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5383424758911133
Fix gfp flags passed to lockdep;lockdep;remove bigblock tracking;slob;Similar;0.6176021099090576
Fix gfp flags passed to lockdep;lockdep;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5239408016204834
Fix gfp flags passed to lockdep;lockdep;rework freelist handling;slob;Similar;0.6591470241546631
Fix gfp flags passed to lockdep;lockdep;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6613888144493103
Fix gfp flags passed to lockdep;lockdep;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6685859560966492
Fix gfp flags passed to lockdep;lockdep;SLOB to be used on SMP  ;allows;Similar;0.630568265914917
Fix gfp flags passed to lockdep;lockdep;fix page order calculation on not 4KB page;-;Similar;0.5692324042320251
Fix gfp flags passed to lockdep;lockdep;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6830132007598877
Fix gfp flags passed to lockdep;lockdep;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6008737087249756
Fix gfp flags passed to lockdep;lockdep;handle SLAB_PANIC flag;slob;Similar;0.5819165706634521
Fix gfp flags passed to lockdep;lockdep;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6130391955375671
Fix gfp flags passed to lockdep;lockdep;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6123945713043213
Fix gfp flags passed to lockdep;lockdep;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6077077388763428
Fix gfp flags passed to lockdep;lockdep;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5250492095947266
Fix gfp flags passed to lockdep;lockdep;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5666744709014893
Fix gfp flags passed to lockdep;lockdep;Redo a lot of comments;Also;Similar;0.5220710635185242
Fix gfp flags passed to lockdep;lockdep;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.684581995010376
Fix gfp flags passed to lockdep;lockdep;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5487638711929321
Fix gfp flags passed to lockdep;lockdep;simplifies SLOB;at this point slob may be broken;Similar;0.5494478940963745
Fix gfp flags passed to lockdep;lockdep;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.603573203086853
Fix gfp flags passed to lockdep;lockdep;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6451401710510254
Fix gfp flags passed to lockdep;lockdep;fix;SLOB=y && SMP=y fix;Similar;0.5936909914016724
Fix gfp flags passed to lockdep;lockdep;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6022986173629761
Fix gfp flags passed to lockdep;lockdep;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5704770088195801
Ran a ktest.pl config_bisect;Came up with this config as the problem;Adding this mask;fixes the bug;Similar;0.5510475635528564
Ran a ktest.pl config_bisect;Came up with this config as the problem;fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Similar;0.6143535375595093
Ran a ktest.pl config_bisect;Came up with this config as the problem;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.7537873387336731
Ran a ktest.pl config_bisect;Came up with this config as the problem;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5231233835220337
Ran a ktest.pl config_bisect;Came up with this config as the problem;Remove kmemtrace ftrace plugin;tracing;Similar;0.6370514035224915
Ran a ktest.pl config_bisect;Came up with this config as the problem;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.598490834236145
Ran a ktest.pl config_bisect;Came up with this config as the problem;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.565306544303894
Ran a ktest.pl config_bisect;Came up with this config as the problem;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5183016657829285
Ran a ktest.pl config_bisect;Came up with this config as the problem;refactor code for future changes;Impact;Similar;0.5259075164794922
Ran a ktest.pl config_bisect;Came up with this config as the problem;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.6535464525222778
Ran a ktest.pl config_bisect;Came up with this config as the problem;use tracepoints;kmemtrace;Similar;0.6637743711471558
Ran a ktest.pl config_bisect;Came up with this config as the problem;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6757253408432007
Ran a ktest.pl config_bisect;Came up with this config as the problem;fix typo in mm/slob.c;build fix;Similar;0.7412323355674744
Ran a ktest.pl config_bisect;Came up with this config as the problem;fix lockup in slob_free()  ;lockup;Similar;0.5555030107498169
Ran a ktest.pl config_bisect;Came up with this config as the problem;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.7442991733551025
Ran a ktest.pl config_bisect;Came up with this config as the problem;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6518248319625854
Ran a ktest.pl config_bisect;Came up with this config as the problem;enable and use this tracer;To enable and use this tracer;Similar;0.6259180307388306
Ran a ktest.pl config_bisect;Came up with this config as the problem;fix bogus ksize calculation fix  ;SLOB;Similar;0.6368552446365356
Ran a ktest.pl config_bisect;Came up with this config as the problem;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5528773665428162
Ran a ktest.pl config_bisect;Came up with this config as the problem;fix bogus ksize calculation;bogus;Similar;0.623319149017334
Ran a ktest.pl config_bisect;Came up with this config as the problem;Fix to return wrong pointer;Fix;Similar;0.5624821782112122
Ran a ktest.pl config_bisect;Came up with this config as the problem;Handle that separately in krealloc();separately;Similar;0.6894513368606567
Ran a ktest.pl config_bisect;Came up with this config as the problem;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6378103494644165
Ran a ktest.pl config_bisect;Came up with this config as the problem;reduce list scanning;slob;Similar;0.5306833386421204
Ran a ktest.pl config_bisect;Came up with this config as the problem;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.56983882188797
Ran a ktest.pl config_bisect;Came up with this config as the problem;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5823159217834473
Ran a ktest.pl config_bisect;Came up with this config as the problem;remove bigblock tracking;slob;Similar;0.6365147829055786
Ran a ktest.pl config_bisect;Came up with this config as the problem;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5495227575302124
Ran a ktest.pl config_bisect;Came up with this config as the problem;rework freelist handling;slob;Similar;0.6170710325241089
Ran a ktest.pl config_bisect;Came up with this config as the problem;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.504030168056488
Ran a ktest.pl config_bisect;Came up with this config as the problem;"align them to word size
";it is best in practice;Similar;0.6248716115951538
Ran a ktest.pl config_bisect;Came up with this config as the problem;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.7132652401924133
Ran a ktest.pl config_bisect;Came up with this config as the problem;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.593381404876709
Ran a ktest.pl config_bisect;Came up with this config as the problem;SLOB to be used on SMP  ;allows;Similar;0.6811349391937256
Ran a ktest.pl config_bisect;Came up with this config as the problem;fix page order calculation on not 4KB page;-;Similar;0.5508476495742798
Ran a ktest.pl config_bisect;Came up with this config as the problem;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5791814923286438
Ran a ktest.pl config_bisect;Came up with this config as the problem;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.606429934501648
Ran a ktest.pl config_bisect;Came up with this config as the problem;handle SLAB_PANIC flag;slob;Similar;0.5799947381019592
Ran a ktest.pl config_bisect;Came up with this config as the problem;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.7816914319992065
Ran a ktest.pl config_bisect;Came up with this config as the problem;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5485213994979858
Ran a ktest.pl config_bisect;Came up with this config as the problem;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.665050745010376
Ran a ktest.pl config_bisect;Came up with this config as the problem;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5502704381942749
Ran a ktest.pl config_bisect;Came up with this config as the problem;Redo a lot of comments;Also;Similar;0.5427634716033936
Ran a ktest.pl config_bisect;Came up with this config as the problem;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5495632886886597
Ran a ktest.pl config_bisect;Came up with this config as the problem;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5381687879562378
Ran a ktest.pl config_bisect;Came up with this config as the problem;simplifies SLOB;at this point slob may be broken;Similar;0.5633447170257568
Ran a ktest.pl config_bisect;Came up with this config as the problem;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5405277013778687
Ran a ktest.pl config_bisect;Came up with this config as the problem;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6660381555557251
Ran a ktest.pl config_bisect;Came up with this config as the problem;fix;SLOB=y && SMP=y fix;Similar;0.5671454668045044
Ran a ktest.pl config_bisect;Came up with this config as the problem;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.7142512798309326
Adding this mask;fixes the bug;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6116942167282104
Adding this mask;fixes the bug;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5898917317390442
Adding this mask;fixes the bug;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.5200691819190979
Adding this mask;fixes the bug;refactor code for future changes;Impact;Similar;0.5540046691894531
Adding this mask;fixes the bug;use tracepoints;kmemtrace;Similar;0.6405752301216125
Adding this mask;fixes the bug;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5476303100585938
Adding this mask;fixes the bug;fix typo in mm/slob.c;build fix;Similar;0.5667304992675781
Adding this mask;fixes the bug;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.590420126914978
Adding this mask;fixes the bug;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.6279380917549133
Adding this mask;fixes the bug;enable and use this tracer;To enable and use this tracer;Similar;0.7195020914077759
Adding this mask;fixes the bug;I find it more readable  ;personal opinion;Similar;0.5957940816879272
Adding this mask;fixes the bug;Drop it  ;if you want;Similar;0.531079113483429
Adding this mask;fixes the bug;fix bogus ksize calculation fix  ;SLOB;Similar;0.6044949293136597
Adding this mask;fixes the bug;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5672358274459839
Adding this mask;fixes the bug;fix bogus ksize calculation;bogus;Similar;0.616387128829956
Adding this mask;fixes the bug;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5337338447570801
Adding this mask;fixes the bug;record page flag overlays explicitly;slob;Similar;0.5157747864723206
Adding this mask;fixes the bug;Fix to return wrong pointer;Fix;Similar;0.5454527735710144
Adding this mask;fixes the bug;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5095690488815308
Adding this mask;fixes the bug;fix memory corruption;memory corruption;Similar;0.5521057844161987
Adding this mask;fixes the bug;Handle that separately in krealloc();separately;Similar;0.5329810380935669
Adding this mask;fixes the bug;reduce list scanning;slob;Similar;0.525873064994812
Adding this mask;fixes the bug;improved alignment handling;improved;Similar;0.6263259649276733
Adding this mask;fixes the bug;remove bigblock tracking;slob;Similar;0.610741138458252
Adding this mask;fixes the bug;rework freelist handling;slob;Similar;0.5287877321243286
Adding this mask;fixes the bug;"align them to word size
";it is best in practice;Similar;0.6200419664382935
Adding this mask;fixes the bug;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5252838730812073
Adding this mask;fixes the bug;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5803155899047852
Adding this mask;fixes the bug;SLOB to be used on SMP  ;allows;Similar;0.602087140083313
Adding this mask;fixes the bug;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5207461714744568
Adding this mask;fixes the bug;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5040044188499451
Adding this mask;fixes the bug;handle SLAB_PANIC flag;slob;Similar;0.6076642870903015
Adding this mask;fixes the bug;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.7171112895011902
Adding this mask;fixes the bug;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.542231559753418
Adding this mask;fixes the bug;Redo a lot of comments;Also;Similar;0.6217906475067139
Adding this mask;fixes the bug;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5010459423065186
Adding this mask;fixes the bug;simplifies SLOB;at this point slob may be broken;Similar;0.6912732720375061
Adding this mask;fixes the bug;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5614374876022339
Adding this mask;fixes the bug;fix;SLOB=y && SMP=y fix;Similar;0.7058418989181519
Adding this mask;fixes the bug;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5526472330093384
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.6400161981582642
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Remove kmemtrace ftrace plugin;tracing;Similar;0.520384669303894
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5951477289199829
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.6274529695510864
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;refactor code for future changes;Impact;Similar;0.5140479803085327
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.6598020792007446
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;use tracepoints;kmemtrace;Similar;0.5080254077911377
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.7116087079048157
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;fix typo in mm/slob.c;build fix;Similar;0.690224826335907
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;fix lockup in slob_free()  ;lockup;Similar;0.6097941994667053
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6000262498855591
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5140811204910278
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6289738416671753
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;enable and use this tracer;To enable and use this tracer;Similar;0.5032364130020142
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;fix bogus ksize calculation fix  ;SLOB;Similar;0.6024818420410156
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;fix bogus ksize calculation;bogus;Similar;0.5969340801239014
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Fix to return wrong pointer;Fix;Similar;0.5871868133544922
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;reduce external fragmentation by using three free lists;slob;Similar;0.5169888734817505
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5243483781814575
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;fix memory corruption;memory corruption;Similar;0.5446485280990601
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5947058200836182
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;reduce list scanning;slob;Similar;0.5192126631736755
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5544592142105103
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5085222721099854
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.6274410486221313
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5153125524520874
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5038378238677979
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.6042851209640503
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;rework freelist handling;slob;Similar;0.5726460814476013
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;"align them to word size
";it is best in practice;Similar;0.514687180519104
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5258898138999939
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6091192960739136
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;SLOB to be used on SMP  ;allows;Similar;0.5044630765914917
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;fix page order calculation on not 4KB page;-;Similar;0.6474254131317139
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6659548282623291
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6320593357086182
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5688397288322449
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.532015860080719
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5352806448936462
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6586062908172607
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6035234928131104
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6153157949447632
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6454076170921326
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5808608531951904
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.6079250574111938
The bit should be passed to trace_kmalloc_node()  ;as well;The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Similar;0.5473284721374512
The bit should be passed to trace_kmalloc_node()  ;as well;we dont care about the RCU head state before passing it to call_rcu();anyway;Similar;0.524713933467865
The bit should be passed to trace_kmalloc_node()  ;as well;Remove kmemtrace ftrace plugin;tracing;Similar;0.6398517489433289
The bit should be passed to trace_kmalloc_node()  ;as well;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.6182538270950317
The bit should be passed to trace_kmalloc_node()  ;as well;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.6645318269729614
The bit should be passed to trace_kmalloc_node()  ;as well;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5792990922927856
The bit should be passed to trace_kmalloc_node()  ;as well;refactor code for future changes;Impact;Similar;0.6234621405601501
The bit should be passed to trace_kmalloc_node()  ;as well;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.6303244829177856
The bit should be passed to trace_kmalloc_node()  ;as well;use tracepoints;kmemtrace;Similar;0.7700308561325073
The bit should be passed to trace_kmalloc_node()  ;as well;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.7122964859008789
The bit should be passed to trace_kmalloc_node()  ;as well;fix typo in mm/slob.c;build fix;Similar;0.7597086429595947
The bit should be passed to trace_kmalloc_node()  ;as well;fix lockup in slob_free()  ;lockup;Similar;0.654280424118042
The bit should be passed to trace_kmalloc_node()  ;as well;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6942946910858154
The bit should be passed to trace_kmalloc_node()  ;as well;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5034841895103455
The bit should be passed to trace_kmalloc_node()  ;as well;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6299751996994019
The bit should be passed to trace_kmalloc_node()  ;as well;enable and use this tracer;To enable and use this tracer;Similar;0.7354115843772888
The bit should be passed to trace_kmalloc_node()  ;as well;Drop it  ;if you want;Similar;0.5233348608016968
The bit should be passed to trace_kmalloc_node()  ;as well;fix bogus ksize calculation fix  ;SLOB;Similar;0.6723580360412598
The bit should be passed to trace_kmalloc_node()  ;as well;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.6410335302352905
The bit should be passed to trace_kmalloc_node()  ;as well;fix bogus ksize calculation;bogus;Similar;0.6732041835784912
The bit should be passed to trace_kmalloc_node()  ;as well;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5468316674232483
The bit should be passed to trace_kmalloc_node()  ;as well;record page flag overlays explicitly;slob;Similar;0.5260908603668213
The bit should be passed to trace_kmalloc_node()  ;as well;Fix to return wrong pointer;Fix;Similar;0.6809283494949341
The bit should be passed to trace_kmalloc_node()  ;as well;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5548557043075562
The bit should be passed to trace_kmalloc_node()  ;as well;fix memory corruption;memory corruption;Similar;0.5706833600997925
The bit should be passed to trace_kmalloc_node()  ;as well;Handle that separately in krealloc();separately;Similar;0.6466548442840576
The bit should be passed to trace_kmalloc_node()  ;as well;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6987708210945129
The bit should be passed to trace_kmalloc_node()  ;as well;reduce list scanning;slob;Similar;0.6030880212783813
The bit should be passed to trace_kmalloc_node()  ;as well;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.633296549320221
The bit should be passed to trace_kmalloc_node()  ;as well;Cleanup zeroing allocations;Slab allocators;Similar;0.5121636390686035
The bit should be passed to trace_kmalloc_node()  ;as well;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5156935453414917
The bit should be passed to trace_kmalloc_node()  ;as well;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.6388622522354126
The bit should be passed to trace_kmalloc_node()  ;as well;remove bigblock tracking;slob;Similar;0.6452657580375671
The bit should be passed to trace_kmalloc_node()  ;as well;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.552313506603241
The bit should be passed to trace_kmalloc_node()  ;as well;rework freelist handling;slob;Similar;0.6315250992774963
The bit should be passed to trace_kmalloc_node()  ;as well;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5030789375305176
The bit should be passed to trace_kmalloc_node()  ;as well;"align them to word size
";it is best in practice;Similar;0.5847214460372925
The bit should be passed to trace_kmalloc_node()  ;as well;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.7548837661743164
The bit should be passed to trace_kmalloc_node()  ;as well;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6149508953094482
The bit should be passed to trace_kmalloc_node()  ;as well;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5043039917945862
The bit should be passed to trace_kmalloc_node()  ;as well;SLOB to be used on SMP  ;allows;Similar;0.7400435209274292
The bit should be passed to trace_kmalloc_node()  ;as well;fix page order calculation on not 4KB page;-;Similar;0.5686985850334167
The bit should be passed to trace_kmalloc_node()  ;as well;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6566274166107178
The bit should be passed to trace_kmalloc_node()  ;as well;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6195693612098694
The bit should be passed to trace_kmalloc_node()  ;as well;handle SLAB_PANIC flag;slob;Similar;0.6092555522918701
The bit should be passed to trace_kmalloc_node()  ;as well;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6868966817855835
The bit should be passed to trace_kmalloc_node()  ;as well;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.614995539188385
The bit should be passed to trace_kmalloc_node()  ;as well;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5255188941955566
The bit should be passed to trace_kmalloc_node()  ;as well;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6207987666130066
The bit should be passed to trace_kmalloc_node()  ;as well;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5760854482650757
The bit should be passed to trace_kmalloc_node()  ;as well;Redo a lot of comments;Also;Similar;0.5308433771133423
The bit should be passed to trace_kmalloc_node()  ;as well;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6342990398406982
The bit should be passed to trace_kmalloc_node()  ;as well;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5453811883926392
The bit should be passed to trace_kmalloc_node()  ;as well;simplifies SLOB;at this point slob may be broken;Similar;0.6567139625549316
The bit should be passed to trace_kmalloc_node()  ;as well;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5007479190826416
The bit should be passed to trace_kmalloc_node()  ;as well;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.71785569190979
The bit should be passed to trace_kmalloc_node()  ;as well;fix;SLOB=y && SMP=y fix;Similar;0.6153544783592224
The bit should be passed to trace_kmalloc_node()  ;as well;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6854111552238464
The bit should be passed to trace_kmalloc_node()  ;as well;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5113948583602905
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;refactor code for future changes;Impact;Similar;0.5202924013137817
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;use tracepoints;kmemtrace;Similar;0.542500376701355
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;fix typo in mm/slob.c;build fix;Similar;0.5025244951248169
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;fix lockup in slob_free()  ;lockup;Similar;0.5561653971672058
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.7030514478683472
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;enable and use this tracer;To enable and use this tracer;Similar;0.6658666133880615
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;I find it more readable  ;personal opinion;Similar;0.6364632844924927
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;fix bogus ksize calculation fix  ;SLOB;Similar;0.5448564291000366
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5789510011672974
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;fix bogus ksize calculation;bogus;Similar;0.5614833831787109
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5442583560943604
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;improved alignment handling;improved;Similar;0.6236982345581055
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;remove bigblock tracking;slob;Similar;0.5630533695220947
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;rework freelist handling;slob;Similar;0.5501878261566162
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;can be slightly faster too;skip almost-full freelist pages completely;Similar;0.6389653086662292
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;Similar;0.6131922006607056
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;"align them to word size
";it is best in practice;Similar;0.5626217126846313
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Code size goes up about 1K;dynamic memory usage should be lower even on relatively small memory systems;Similar;0.7735005021095276
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;SLOB to be used on SMP  ;allows;Similar;0.5250346064567566
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5909498929977417
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5010515451431274
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;simplifies SLOB;at this point slob may be broken;Similar;0.6450796127319336
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;fix;SLOB=y && SMP=y fix;Similar;0.6334819793701172
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.547932505607605
we dont care about the RCU head state before passing it to call_rcu();anyway;Remove kmemtrace ftrace plugin;tracing;Similar;0.6418838500976562
we dont care about the RCU head state before passing it to call_rcu();anyway;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.6808735132217407
we dont care about the RCU head state before passing it to call_rcu();anyway;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5004159808158875
we dont care about the RCU head state before passing it to call_rcu();anyway;fix lockup in slob_free()  ;lockup;Similar;0.5306380987167358
we dont care about the RCU head state before passing it to call_rcu();anyway;Drop it  ;if you want;Similar;0.6335605382919312
we dont care about the RCU head state before passing it to call_rcu();anyway;fix bogus ksize calculation fix  ;SLOB;Similar;0.5171699523925781
we dont care about the RCU head state before passing it to call_rcu();anyway;fix bogus ksize calculation;bogus;Similar;0.5356450080871582
we dont care about the RCU head state before passing it to call_rcu();anyway;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.6681930422782898
we dont care about the RCU head state before passing it to call_rcu();anyway;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.6552228927612305
we dont care about the RCU head state before passing it to call_rcu();anyway;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5047379732131958
we dont care about the RCU head state before passing it to call_rcu();anyway;remove useless ctor parameter and reorder parameters;useless;Similar;0.6824696063995361
we dont care about the RCU head state before passing it to call_rcu();anyway;Handle that separately in krealloc();separately;Similar;0.6239124536514282
we dont care about the RCU head state before passing it to call_rcu();anyway;reduce list scanning;slob;Similar;0.6246147155761719
we dont care about the RCU head state before passing it to call_rcu();anyway;Cleanup zeroing allocations;Slab allocators;Similar;0.6147975325584412
we dont care about the RCU head state before passing it to call_rcu();anyway;remove bigblock tracking;slob;Similar;0.5213618874549866
we dont care about the RCU head state before passing it to call_rcu();anyway;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.7407040596008301
we dont care about the RCU head state before passing it to call_rcu();anyway;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5686135292053223
we dont care about the RCU head state before passing it to call_rcu();anyway;fix page order calculation on not 4KB page;-;Similar;0.6112549901008606
we dont care about the RCU head state before passing it to call_rcu();anyway;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6081589460372925
we dont care about the RCU head state before passing it to call_rcu();anyway;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.7014851570129395
we dont care about the RCU head state before passing it to call_rcu();anyway;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.7153649926185608
we dont care about the RCU head state before passing it to call_rcu();anyway;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5970492959022522
we dont care about the RCU head state before passing it to call_rcu();anyway;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6040120124816895
Remove kmemtrace ftrace plugin;tracing;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.8647539615631104
Remove kmemtrace ftrace plugin;tracing;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.7307064533233643
Remove kmemtrace ftrace plugin;tracing;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.54802405834198
Remove kmemtrace ftrace plugin;tracing;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5171470046043396
Remove kmemtrace ftrace plugin;tracing;use tracepoints;kmemtrace;Similar;0.551573634147644
Remove kmemtrace ftrace plugin;tracing;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.7058289051055908
Remove kmemtrace ftrace plugin;tracing;fix typo in mm/slob.c;build fix;Similar;0.6608676910400391
Remove kmemtrace ftrace plugin;tracing;fix lockup in slob_free()  ;lockup;Similar;0.6135485172271729
Remove kmemtrace ftrace plugin;tracing;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6260070204734802
Remove kmemtrace ftrace plugin;tracing;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5104379057884216
Remove kmemtrace ftrace plugin;tracing;Drop it  ;if you want;Similar;0.6686207056045532
Remove kmemtrace ftrace plugin;tracing;fix bogus ksize calculation fix  ;SLOB;Similar;0.6690852642059326
Remove kmemtrace ftrace plugin;tracing;fix bogus ksize calculation;bogus;Similar;0.6822094917297363
Remove kmemtrace ftrace plugin;tracing;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.7850580215454102
Remove kmemtrace ftrace plugin;tracing;Fix to return wrong pointer;Fix;Similar;0.5996798872947693
Remove kmemtrace ftrace plugin;tracing;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5152574777603149
Remove kmemtrace ftrace plugin;tracing;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5289402008056641
Remove kmemtrace ftrace plugin;tracing;remove useless ctor parameter and reorder parameters;useless;Similar;0.7980963587760925
Remove kmemtrace ftrace plugin;tracing;Handle that separately in krealloc();separately;Similar;0.6517317891120911
Remove kmemtrace ftrace plugin;tracing;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5290581583976746
Remove kmemtrace ftrace plugin;tracing;reduce list scanning;slob;Similar;0.7791879773139954
Remove kmemtrace ftrace plugin;tracing;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5462985038757324
Remove kmemtrace ftrace plugin;tracing;Cleanup zeroing allocations;Slab allocators;Similar;0.7480922937393188
Remove kmemtrace ftrace plugin;tracing;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.51954185962677
Remove kmemtrace ftrace plugin;tracing;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6137045621871948
Remove kmemtrace ftrace plugin;tracing;remove bigblock tracking;slob;Similar;0.8015408515930176
Remove kmemtrace ftrace plugin;tracing;rework freelist handling;slob;Similar;0.53685462474823
Remove kmemtrace ftrace plugin;tracing;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6168155074119568
Remove kmemtrace ftrace plugin;tracing;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6755820512771606
Remove kmemtrace ftrace plugin;tracing;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5491212606430054
Remove kmemtrace ftrace plugin;tracing;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5898277759552002
Remove kmemtrace ftrace plugin;tracing;SLOB to be used on SMP  ;allows;Similar;0.5839502811431885
Remove kmemtrace ftrace plugin;tracing;fix page order calculation on not 4KB page;-;Similar;0.5706282258033752
Remove kmemtrace ftrace plugin;tracing;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5971742272377014
Remove kmemtrace ftrace plugin;tracing;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.8483313918113708
Remove kmemtrace ftrace plugin;tracing;handle SLAB_PANIC flag;slob;Similar;0.6448246240615845
Remove kmemtrace ftrace plugin;tracing;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5654881000518799
Remove kmemtrace ftrace plugin;tracing;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5580719709396362
Remove kmemtrace ftrace plugin;tracing;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5888407230377197
Remove kmemtrace ftrace plugin;tracing;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6566689014434814
Remove kmemtrace ftrace plugin;tracing;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.725860595703125
Remove kmemtrace ftrace plugin;tracing;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5233528017997742
Remove kmemtrace ftrace plugin;tracing;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6311919689178467
Remove kmemtrace ftrace plugin;tracing;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5285583734512329
Remove kmemtrace ftrace plugin;tracing;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.767729640007019
Remove kmemtrace ftrace plugin;tracing;fix;SLOB=y && SMP=y fix;Similar;0.5410808324813843
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.5663597583770752
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;use tracepoints;kmemtrace;Similar;0.5647094249725342
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5578376054763794
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;fix typo in mm/slob.c;build fix;Similar;0.5973421335220337
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;fix lockup in slob_free()  ;lockup;Similar;0.5607991218566895
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5008916258811951
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Drop it  ;if you want;Similar;0.8542777299880981
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;fix bogus ksize calculation fix  ;SLOB;Similar;0.6348923444747925
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5408123731613159
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;fix bogus ksize calculation;bogus;Similar;0.6762529611587524
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.8357622623443604
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5335320234298706
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Fix to return wrong pointer;Fix;Similar;0.5488483905792236
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.6262269020080566
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;remove useless ctor parameter and reorder parameters;useless;Similar;0.8131567239761353
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Handle that separately in krealloc();separately;Similar;0.7141659259796143
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;reduce list scanning;slob;Similar;0.8518403768539429
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.511634349822998
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Cleanup zeroing allocations;Slab allocators;Similar;0.791874349117279
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;remove bigblock tracking;slob;Similar;0.8058533668518066
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;rework freelist handling;slob;Similar;0.5094059109687805
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6484415531158447
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6117463707923889
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;SLOB to be used on SMP  ;allows;Similar;0.6281412243843079
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;fix page order calculation on not 4KB page;-;Similar;0.5547726154327393
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5143537521362305
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.7945252060890198
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;handle SLAB_PANIC flag;slob;Similar;0.5975412726402283
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5975467562675476
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5581304430961609
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5324177742004395
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6406561136245728
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.520929753780365
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Redo a lot of comments;Also;Similar;0.5059616565704346
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;simplifies SLOB;at this point slob may be broken;Similar;0.5939644575119019
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6996239423751831
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;fix;SLOB=y && SMP=y fix;Similar;0.6479848027229309
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Similar;0.5541324019432068
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Similar;0.7011369466781616
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.5627260208129883
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.7740997672080994
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;fix typo in mm/slob.c;build fix;Similar;0.6958227157592773
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;fix lockup in slob_free()  ;lockup;Similar;0.6288915872573853
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6419094800949097
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5319854021072388
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5876964330673218
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;fix bogus ksize calculation fix  ;SLOB;Similar;0.6823554039001465
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;fix bogus ksize calculation;bogus;Similar;0.6720414757728577
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5377181172370911
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Fix to return wrong pointer;Fix;Similar;0.7015718817710876
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5804691314697266
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5614000558853149
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;fix memory corruption;memory corruption;Similar;0.6545578837394714
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;remove useless ctor parameter and reorder parameters;useless;Similar;0.6162929534912109
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Handle that separately in krealloc();separately;Similar;0.5122544169425964
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6220453977584839
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;reduce list scanning;slob;Similar;0.5917952060699463
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5111207962036133
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.6375749111175537
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Cleanup zeroing allocations;Slab allocators;Similar;0.5861374139785767
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5520899295806885
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.733144998550415
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;remove bigblock tracking;slob;Similar;0.6975076794624329
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.655479907989502
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6800723075866699
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.8012063503265381
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;SLOB to be used on SMP  ;allows;Similar;0.5085238814353943
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;fix page order calculation on not 4KB page;-;Similar;0.5638092756271362
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6665747761726379
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.7588673830032349
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;handle SLAB_PANIC flag;slob;Similar;0.6807161569595337
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5200970768928528
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.6455806493759155
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6034003496170044
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6435978412628174
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6709202527999878
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6684409379959106
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6878703236579895
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.624664306640625
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7620975971221924
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.6562786102294922
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;use tracepoints;kmemtrace;Similar;0.5145369172096252
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6188682317733765
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;fix typo in mm/slob.c;build fix;Similar;0.5467050075531006
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;fix lockup in slob_free()  ;lockup;Similar;0.5936610102653503
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5162062644958496
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.5921326875686646
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5288263559341431
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;fix bogus ksize calculation fix  ;SLOB;Similar;0.5684455633163452
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;fix bogus ksize calculation;bogus;Similar;0.5577940940856934
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.6452242136001587
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;record page flag overlays explicitly;slob;Similar;0.5204900503158569
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Fix to return wrong pointer;Fix;Similar;0.5136737823486328
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5695160627365112
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5717027187347412
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5510056018829346
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5734726190567017
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.7361640930175781
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5752836465835571
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5437630414962769
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5562905669212341
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;rework freelist handling;slob;Similar;0.5030388832092285
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5677024126052856
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.537780225276947
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.550292432308197
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5143025517463684
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;fix page order calculation on not 4KB page;-;Similar;0.6369814872741699
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5863369703292847
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5682786107063293
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5759105682373047
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5033574104309082
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5086252689361572
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5661565065383911
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.652906060218811
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6597639322280884
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5110775232315063
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;fix bogus ksize calculation fix  ;SLOB;Similar;0.5083763599395752
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;fix bogus ksize calculation;bogus;Similar;0.5142725110054016
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5641028881072998
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Fix to return wrong pointer;Fix;Similar;0.6112328767776489
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5336094498634338
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;fix memory corruption;memory corruption;Similar;0.5798321962356567
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;remove useless ctor parameter and reorder parameters;useless;Similar;0.6476579904556274
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;reduce list scanning;slob;Similar;0.5257086753845215
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5495815277099609
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Cleanup zeroing allocations;Slab allocators;Similar;0.5330138206481934
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5046246647834778
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.7744957804679871
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;"Taking a spinlock in a destructor is a bit risky
";since the slab allocators may run the destructors anytime they decide a slab is no longer needed;Similar;0.559779703617096
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5379021763801575
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.749940037727356
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5547012686729431
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6117100119590759
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.6166778802871704
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6298995614051819
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5137560367584229
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.61518394947052
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5398709177970886
refactor code for future changes;Impact;use tracepoints;kmemtrace;Similar;0.533867597579956
refactor code for future changes;Impact;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.5846238136291504
refactor code for future changes;Impact;fix typo in mm/slob.c;build fix;Similar;0.5393821001052856
refactor code for future changes;Impact;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5657025575637817
refactor code for future changes;Impact;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.6232054233551025
refactor code for future changes;Impact;enable and use this tracer;To enable and use this tracer;Similar;0.5717372298240662
refactor code for future changes;Impact;I find it more readable  ;personal opinion;Similar;0.5154100060462952
refactor code for future changes;Impact;fix bogus ksize calculation fix  ;SLOB;Similar;0.5487310886383057
refactor code for future changes;Impact;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.6097862124443054
refactor code for future changes;Impact;fix bogus ksize calculation;bogus;Similar;0.5388266444206238
refactor code for future changes;Impact;Fix to return wrong pointer;Fix;Similar;0.5773688554763794
refactor code for future changes;Impact;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.506819486618042
refactor code for future changes;Impact;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5067537426948547
refactor code for future changes;Impact;reduce list scanning;slob;Similar;0.5108307600021362
refactor code for future changes;Impact;improved alignment handling;improved;Similar;0.5581335425376892
refactor code for future changes;Impact;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5287664532661438
refactor code for future changes;Impact;rework freelist handling;slob;Similar;0.6220054626464844
refactor code for future changes;Impact;"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;Similar;0.5082278847694397
refactor code for future changes;Impact;"align them to word size
";it is best in practice;Similar;0.5165549516677856
refactor code for future changes;Impact;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5688449740409851
refactor code for future changes;Impact;SLOB to be used on SMP  ;allows;Similar;0.5170453190803528
refactor code for future changes;Impact;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5516971349716187
refactor code for future changes;Impact;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5703123807907104
refactor code for future changes;Impact;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5519979000091553
refactor code for future changes;Impact;Redo a lot of comments;Also;Similar;0.5535529851913452
refactor code for future changes;Impact;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5091460347175598
refactor code for future changes;Impact;simplifies SLOB;at this point slob may be broken;Similar;0.5059483051300049
refactor code for future changes;Impact;fix;SLOB=y && SMP=y fix;Similar;0.5761313438415527
refactor code for future changes;Impact;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5341863632202148
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;use tracepoints;kmemtrace;Similar;0.5286674499511719
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6111152172088623
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;fix typo in mm/slob.c;build fix;Similar;0.6722227334976196
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;fix lockup in slob_free()  ;lockup;Similar;0.5222054123878479
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6739011406898499
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.7018084526062012
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;record page flag overlays explicitly;slob;Similar;0.5028392672538757
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6130905151367188
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5240448117256165
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5206499099731445
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;rework freelist handling;slob;Similar;0.5145227313041687
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5781505703926086
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5349590182304382
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;SLOB to be used on SMP  ;allows;Similar;0.5612401962280273
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5193070769309998
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5346333980560303
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5387715697288513
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5830225944519043
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5347493290901184
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.561342716217041
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5694655776023865
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5956560969352722
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.611378014087677
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6747475862503052
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5835409164428711
use tracepoints;kmemtrace;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Similar;0.6056109666824341
use tracepoints;kmemtrace;fix typo in mm/slob.c;build fix;Similar;0.6649806499481201
use tracepoints;kmemtrace;fix lockup in slob_free()  ;lockup;Similar;0.5936208963394165
use tracepoints;kmemtrace;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5781749486923218
use tracepoints;kmemtrace;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5800659656524658
use tracepoints;kmemtrace;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5461063981056213
use tracepoints;kmemtrace;enable and use this tracer;To enable and use this tracer;Similar;0.8726144433021545
use tracepoints;kmemtrace;I find it more readable  ;personal opinion;Similar;0.5834825038909912
use tracepoints;kmemtrace;Drop it  ;if you want;Similar;0.5714551210403442
use tracepoints;kmemtrace;fix bogus ksize calculation fix  ;SLOB;Similar;0.670924186706543
use tracepoints;kmemtrace;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.6326407194137573
use tracepoints;kmemtrace;fix bogus ksize calculation;bogus;Similar;0.6774364113807678
use tracepoints;kmemtrace;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.6067820191383362
use tracepoints;kmemtrace;record page flag overlays explicitly;slob;Similar;0.5839093923568726
use tracepoints;kmemtrace;Fix to return wrong pointer;Fix;Similar;0.578264594078064
use tracepoints;kmemtrace;fix memory corruption;memory corruption;Similar;0.5482211709022522
use tracepoints;kmemtrace;Handle that separately in krealloc();separately;Similar;0.6021565198898315
use tracepoints;kmemtrace;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6041335463523865
use tracepoints;kmemtrace;reduce list scanning;slob;Similar;0.5515943765640259
use tracepoints;kmemtrace;Cleanup zeroing allocations;Slab allocators;Similar;0.5759517550468445
use tracepoints;kmemtrace;improved alignment handling;improved;Similar;0.5532745122909546
use tracepoints;kmemtrace;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5401826500892639
use tracepoints;kmemtrace;remove bigblock tracking;slob;Similar;0.5903027057647705
use tracepoints;kmemtrace;rework freelist handling;slob;Similar;0.7195349931716919
use tracepoints;kmemtrace;"align them to word size
";it is best in practice;Similar;0.630976676940918
use tracepoints;kmemtrace;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6019923686981201
use tracepoints;kmemtrace;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5663583278656006
use tracepoints;kmemtrace;SLOB to be used on SMP  ;allows;Similar;0.7052001953125
use tracepoints;kmemtrace;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5477024912834167
use tracepoints;kmemtrace;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5139607191085815
use tracepoints;kmemtrace;handle SLAB_PANIC flag;slob;Similar;0.5924116373062134
use tracepoints;kmemtrace;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6986020803451538
use tracepoints;kmemtrace;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5508377552032471
use tracepoints;kmemtrace;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5828543901443481
use tracepoints;kmemtrace;Redo a lot of comments;Also;Similar;0.5641982555389404
use tracepoints;kmemtrace;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5065152645111084
use tracepoints;kmemtrace;simplifies SLOB;at this point slob may be broken;Similar;0.7323966026306152
use tracepoints;kmemtrace;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6179234981536865
use tracepoints;kmemtrace;fix;SLOB=y && SMP=y fix;Similar;0.7269116044044495
use tracepoints;kmemtrace;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5805060863494873
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;fix typo in mm/slob.c;build fix;Similar;0.7608485817909241
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;fix lockup in slob_free()  ;lockup;Similar;0.7466703653335571
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.7616321444511414
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6971476674079895
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;enable and use this tracer;To enable and use this tracer;Similar;0.5876321196556091
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;fix bogus ksize calculation fix  ;SLOB;Similar;0.7561166882514954
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.557142972946167
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;fix bogus ksize calculation;bogus;Similar;0.7310390472412109
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Fix to return wrong pointer;Fix;Similar;0.6932568550109863
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5710481405258179
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;reduce external fragmentation by using three free lists;slob;Similar;0.5121638774871826
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.6190198659896851
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;fix memory corruption;memory corruption;Similar;0.6149019002914429
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;remove useless ctor parameter and reorder parameters;useless;Similar;0.5322034955024719
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Handle that separately in krealloc();separately;Similar;0.547018826007843
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6346527338027954
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;reduce list scanning;slob;Similar;0.5920592546463013
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5487026572227478
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.551800549030304
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Cleanup zeroing allocations;Slab allocators;Similar;0.625482976436615
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.627447783946991
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5376418828964233
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5355575084686279
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;remove bigblock tracking;slob;Similar;0.6343632936477661
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;rework freelist handling;slob;Similar;0.6771997213363647
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5037425756454468
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6769673824310303
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6697850823402405
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.6387215852737427
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;SLOB to be used on SMP  ;allows;Similar;0.605424165725708
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;fix page order calculation on not 4KB page;-;Similar;0.5931446552276611
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.7804960608482361
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6895563006401062
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;handle SLAB_PANIC flag;slob;Similar;0.6142963767051697
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5681149959564209
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6321500539779663
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.627468466758728
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5011784434318542
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.750016450881958
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.6666874885559082
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6298284530639648
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6672496199607849
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6925129890441895
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;simplifies SLOB;at this point slob may be broken;Similar;0.5721540451049805
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6194216012954712
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.692574679851532
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;fix;SLOB=y && SMP=y fix;Similar;0.6153337955474854
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5649808645248413
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5378357768058777
fix typo in mm/slob.c;build fix;fix lockup in slob_free()  ;lockup;Similar;0.7161304950714111
fix typo in mm/slob.c;build fix;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.6784539818763733
fix typo in mm/slob.c;build fix;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6110095977783203
fix typo in mm/slob.c;build fix;enable and use this tracer;To enable and use this tracer;Similar;0.6132171154022217
fix typo in mm/slob.c;build fix;Drop it  ;if you want;Similar;0.5041813850402832
fix typo in mm/slob.c;build fix;fix bogus ksize calculation fix  ;SLOB;Similar;0.7522804737091064
fix typo in mm/slob.c;build fix;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5885498523712158
fix typo in mm/slob.c;build fix;fix bogus ksize calculation;bogus;Similar;0.7438898086547852
fix typo in mm/slob.c;build fix;Fix to return wrong pointer;Fix;Similar;0.6625499129295349
fix typo in mm/slob.c;build fix;reduce external fragmentation by using three free lists;slob;Similar;0.5045569539070129
fix typo in mm/slob.c;build fix;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5100449919700623
fix typo in mm/slob.c;build fix;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5195735692977905
fix typo in mm/slob.c;build fix;fix memory corruption;memory corruption;Similar;0.6623485088348389
fix typo in mm/slob.c;build fix;Handle that separately in krealloc();separately;Similar;0.6527976393699646
fix typo in mm/slob.c;build fix;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.7228200435638428
fix typo in mm/slob.c;build fix;reduce list scanning;slob;Similar;0.6121950149536133
fix typo in mm/slob.c;build fix;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5302908420562744
fix typo in mm/slob.c;build fix;Cleanup zeroing allocations;Slab allocators;Similar;0.5649195313453674
fix typo in mm/slob.c;build fix;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5508996248245239
fix typo in mm/slob.c;build fix;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5272964239120483
fix typo in mm/slob.c;build fix;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5871299505233765
fix typo in mm/slob.c;build fix;remove bigblock tracking;slob;Similar;0.6336190700531006
fix typo in mm/slob.c;build fix;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5649687051773071
fix typo in mm/slob.c;build fix;rework freelist handling;slob;Similar;0.6353100538253784
fix typo in mm/slob.c;build fix;"align them to word size
";it is best in practice;Similar;0.5771596431732178
fix typo in mm/slob.c;build fix;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6986263394355774
fix typo in mm/slob.c;build fix;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6902234554290771
fix typo in mm/slob.c;build fix;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5366116166114807
fix typo in mm/slob.c;build fix;SLOB to be used on SMP  ;allows;Similar;0.7404804229736328
fix typo in mm/slob.c;build fix;fix page order calculation on not 4KB page;-;Similar;0.6295401453971863
fix typo in mm/slob.c;build fix;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6531343460083008
fix typo in mm/slob.c;build fix;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6439456343650818
fix typo in mm/slob.c;build fix;handle SLAB_PANIC flag;slob;Similar;0.6484529972076416
fix typo in mm/slob.c;build fix;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6319930553436279
fix typo in mm/slob.c;build fix;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6919940710067749
fix typo in mm/slob.c;build fix;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5415668487548828
fix typo in mm/slob.c;build fix;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6682800054550171
fix typo in mm/slob.c;build fix;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5439255237579346
fix typo in mm/slob.c;build fix;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6206057071685791
fix typo in mm/slob.c;build fix;Redo a lot of comments;Also;Similar;0.5191491842269897
fix typo in mm/slob.c;build fix;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6945354342460632
fix typo in mm/slob.c;build fix;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5568237900733948
fix typo in mm/slob.c;build fix;simplifies SLOB;at this point slob may be broken;Similar;0.672788143157959
fix typo in mm/slob.c;build fix;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5898169279098511
fix typo in mm/slob.c;build fix;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6774739027023315
fix typo in mm/slob.c;build fix;fix;SLOB=y && SMP=y fix;Similar;0.6478818655014038
fix typo in mm/slob.c;build fix;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.7221212387084961
fix typo in mm/slob.c;build fix;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.7337555289268494
fix lockup in slob_free()  ;lockup;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Similar;0.5330052375793457
fix lockup in slob_free()  ;lockup;have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.5092434287071228
fix lockup in slob_free()  ;lockup;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Similar;0.6220136880874634
fix lockup in slob_free()  ;lockup;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.6132428646087646
fix lockup in slob_free()  ;lockup;enable and use this tracer;To enable and use this tracer;Similar;0.5901334881782532
fix lockup in slob_free()  ;lockup;Drop it  ;if you want;Similar;0.5171099901199341
fix lockup in slob_free()  ;lockup;fix bogus ksize calculation fix  ;SLOB;Similar;0.6922134160995483
fix lockup in slob_free()  ;lockup;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5418422222137451
fix lockup in slob_free()  ;lockup;fix bogus ksize calculation;bogus;Similar;0.6858342885971069
fix lockup in slob_free()  ;lockup;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5313584208488464
fix lockup in slob_free()  ;lockup;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5592261552810669
fix lockup in slob_free()  ;lockup;Fix to return wrong pointer;Fix;Similar;0.6303655505180359
fix lockup in slob_free()  ;lockup;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.7200626134872437
fix lockup in slob_free()  ;lockup;reduce external fragmentation by using three free lists;slob;Similar;0.584805965423584
fix lockup in slob_free()  ;lockup;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.6379307508468628
fix lockup in slob_free()  ;lockup;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.6705765724182129
fix lockup in slob_free()  ;lockup;fix memory corruption;memory corruption;Similar;0.6536105275154114
fix lockup in slob_free()  ;lockup;remove useless ctor parameter and reorder parameters;useless;Similar;0.5027400851249695
fix lockup in slob_free()  ;lockup;Handle that separately in krealloc();separately;Similar;0.5637794733047485
fix lockup in slob_free()  ;lockup;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.648196816444397
fix lockup in slob_free()  ;lockup;reduce list scanning;slob;Similar;0.6008286476135254
fix lockup in slob_free()  ;lockup;Cleanup zeroing allocations;Slab allocators;Similar;0.6506075859069824
fix lockup in slob_free()  ;lockup;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.6019042730331421
fix lockup in slob_free()  ;lockup;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5017499923706055
fix lockup in slob_free()  ;lockup;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5096380710601807
fix lockup in slob_free()  ;lockup;remove bigblock tracking;slob;Similar;0.5523298978805542
fix lockup in slob_free()  ;lockup;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.516743540763855
fix lockup in slob_free()  ;lockup;rework freelist handling;slob;Similar;0.7950189113616943
fix lockup in slob_free()  ;lockup;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.526565670967102
fix lockup in slob_free()  ;lockup;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5960080623626709
fix lockup in slob_free()  ;lockup;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5874790549278259
fix lockup in slob_free()  ;lockup;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5275287628173828
fix lockup in slob_free()  ;lockup;SLOB to be used on SMP  ;allows;Similar;0.5748189687728882
fix lockup in slob_free()  ;lockup;fix page order calculation on not 4KB page;-;Similar;0.6301116943359375
fix lockup in slob_free()  ;lockup;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6966266632080078
fix lockup in slob_free()  ;lockup;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6002951860427856
fix lockup in slob_free()  ;lockup;handle SLAB_PANIC flag;slob;Similar;0.5076098442077637
fix lockup in slob_free()  ;lockup;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.736020565032959
fix lockup in slob_free()  ;lockup;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5403409004211426
fix lockup in slob_free()  ;lockup;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6092701554298401
fix lockup in slob_free()  ;lockup;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.6249420642852783
fix lockup in slob_free()  ;lockup;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5986316204071045
fix lockup in slob_free()  ;lockup;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5897084474563599
fix lockup in slob_free()  ;lockup;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5279518365859985
fix lockup in slob_free()  ;lockup;simplifies SLOB;at this point slob may be broken;Similar;0.6713619232177734
fix lockup in slob_free()  ;lockup;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6108742356300354
fix lockup in slob_free()  ;lockup;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6665799617767334
fix lockup in slob_free()  ;lockup;fix;SLOB=y && SMP=y fix;Similar;0.5872527956962585
fix lockup in slob_free()  ;lockup;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5254995822906494
fix lockup in slob_free()  ;lockup;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.561301589012146
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.761735200881958
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;enable and use this tracer;To enable and use this tracer;Similar;0.580382227897644
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;fix bogus ksize calculation fix  ;SLOB;Similar;0.5944914817810059
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;fix bogus ksize calculation;bogus;Similar;0.5717941522598267
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Fix to return wrong pointer;Fix;Similar;0.5351783037185669
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5013730525970459
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;fix memory corruption;memory corruption;Similar;0.5057752132415771
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Handle that separately in krealloc();separately;Similar;0.5014264583587646
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6270889043807983
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5010845065116882
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5180808305740356
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.6074912548065186
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;remove bigblock tracking;slob;Similar;0.5929715037345886
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;rework freelist handling;slob;Similar;0.548337459564209
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6106506586074829
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5713027715682983
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5396851301193237
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;SLOB to be used on SMP  ;allows;Similar;0.5865482687950134
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6069245338439941
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6036936044692993
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;handle SLAB_PANIC flag;slob;Similar;0.5705745816230774
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5983529090881348
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5236910581588745
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.6171917915344238
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.7722221612930298
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.6541504263877869
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6204370260238647
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Redo a lot of comments;Also;Similar;0.5285277366638184
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5383085012435913
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.7417484521865845
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;simplifies SLOB;at this point slob may be broken;Similar;0.502487301826477
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5049436092376709
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6446651220321655
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6746182441711426
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;enable and use this tracer;To enable and use this tracer;Similar;0.7144997715950012
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;I find it more readable  ;personal opinion;Similar;0.8534252643585205
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Drop it  ;if you want;Similar;0.5677167177200317
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;fix bogus ksize calculation fix  ;SLOB;Similar;0.5533046722412109
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.6549593210220337
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;fix bogus ksize calculation;bogus;Similar;0.5749142169952393
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Fix to return wrong pointer;Fix;Similar;0.5194014310836792
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5092755556106567
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.6013944149017334
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;fix memory corruption;memory corruption;Similar;0.5190712213516235
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Handle that separately in krealloc();separately;Similar;0.5035675168037415
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;reduce list scanning;slob;Similar;0.5282589197158813
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;improved alignment handling;improved;Similar;0.8334860801696777
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;rework freelist handling;slob;Similar;0.6418048143386841
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;can be slightly faster too;skip almost-full freelist pages completely;Similar;0.7743940353393555
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;"align them to word size
";it is best in practice;Similar;0.5752761960029602
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Code size goes up about 1K;dynamic memory usage should be lower even on relatively small memory systems;Similar;0.5516729354858398
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5154404044151306
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5433258414268494
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.578994870185852
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Redo a lot of comments;Also;Similar;0.6038469076156616
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;simplifies SLOB;at this point slob may be broken;Similar;0.7567147016525269
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;fix;SLOB=y && SMP=y fix;Similar;0.7691922187805176
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Similar;0.5404934883117676
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.6260396242141724
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5414894819259644
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.6202366948127747
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5040616989135742
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();fix page order calculation on not 4KB page;-;Similar;0.5364689826965332
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5825181007385254
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.573241114616394
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5506044626235962
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5721244215965271
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6268973350524902
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5944291353225708
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;enable and use this tracer;To enable and use this tracer;Similar;0.6113827228546143
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;fix bogus ksize calculation fix  ;SLOB;Similar;0.5328179597854614
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;fix bogus ksize calculation;bogus;Similar;0.5098806619644165
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Fix to return wrong pointer;Fix;Similar;0.5053746700286865
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5603888034820557
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;reduce external fragmentation by using three free lists;slob;Similar;0.5055325031280518
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.502784013748169
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;fix memory corruption;memory corruption;Similar;0.5566189289093018
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5403016805648804
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.6459406614303589
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.571373701095581
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;rework freelist handling;slob;Similar;0.6418461799621582
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5322502851486206
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5902981758117676
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5225406289100647
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6787681579589844
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5136243104934692
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6208667755126953
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5259605646133423
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.655204713344574
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.7023472785949707
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5894709825515747
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.590347170829773
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5362271666526794
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6023558378219604
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5133458375930786
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5222237706184387
enable and use this tracer;To enable and use this tracer;I find it more readable  ;personal opinion;Similar;0.7155536413192749
enable and use this tracer;To enable and use this tracer;Drop it  ;if you want;Similar;0.5069776773452759
enable and use this tracer;To enable and use this tracer;fix bogus ksize calculation fix  ;SLOB;Similar;0.6132631301879883
enable and use this tracer;To enable and use this tracer;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.671329140663147
enable and use this tracer;To enable and use this tracer;fix bogus ksize calculation;bogus;Similar;0.6209300756454468
enable and use this tracer;To enable and use this tracer;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5874813795089722
enable and use this tracer;To enable and use this tracer;record page flag overlays explicitly;slob;Similar;0.6010432243347168
enable and use this tracer;To enable and use this tracer;Fix to return wrong pointer;Fix;Similar;0.5649738311767578
enable and use this tracer;To enable and use this tracer;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5126445293426514
enable and use this tracer;To enable and use this tracer;fix memory corruption;memory corruption;Similar;0.5761250257492065
enable and use this tracer;To enable and use this tracer;Handle that separately in krealloc();separately;Similar;0.5326727032661438
enable and use this tracer;To enable and use this tracer;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.524770975112915
enable and use this tracer;To enable and use this tracer;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.511599063873291
enable and use this tracer;To enable and use this tracer;improved alignment handling;improved;Similar;0.7267922163009644
enable and use this tracer;To enable and use this tracer;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5531231164932251
enable and use this tracer;To enable and use this tracer;remove bigblock tracking;slob;Similar;0.5242620706558228
enable and use this tracer;To enable and use this tracer;rework freelist handling;slob;Similar;0.7125687003135681
enable and use this tracer;To enable and use this tracer;can be slightly faster too;skip almost-full freelist pages completely;Similar;0.5212559700012207
enable and use this tracer;To enable and use this tracer;"align them to word size
";it is best in practice;Similar;0.6809834241867065
enable and use this tracer;To enable and use this tracer;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5956267714500427
enable and use this tracer;To enable and use this tracer;SLOB to be used on SMP  ;allows;Similar;0.6338965892791748
enable and use this tracer;To enable and use this tracer;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5497020483016968
enable and use this tracer;To enable and use this tracer;handle SLAB_PANIC flag;slob;Similar;0.5128819942474365
enable and use this tracer;To enable and use this tracer;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6956983804702759
enable and use this tracer;To enable and use this tracer;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5967389941215515
enable and use this tracer;To enable and use this tracer;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5626741647720337
enable and use this tracer;To enable and use this tracer;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.546249508857727
enable and use this tracer;To enable and use this tracer;Redo a lot of comments;Also;Similar;0.640982449054718
enable and use this tracer;To enable and use this tracer;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5262048244476318
enable and use this tracer;To enable and use this tracer;simplifies SLOB;at this point slob may be broken;Similar;0.7431272864341736
enable and use this tracer;To enable and use this tracer;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5378093719482422
enable and use this tracer;To enable and use this tracer;fix;SLOB=y && SMP=y fix;Similar;0.7276898622512817
enable and use this tracer;To enable and use this tracer;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5897808074951172
I find it more readable  ;personal opinion;Drop it  ;if you want;Similar;0.5282422304153442
I find it more readable  ;personal opinion;fix bogus ksize calculation fix  ;SLOB;Similar;0.5039373636245728
I find it more readable  ;personal opinion;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.5783940553665161
I find it more readable  ;personal opinion;fix bogus ksize calculation;bogus;Similar;0.5299745202064514
I find it more readable  ;personal opinion;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.561566948890686
I find it more readable  ;personal opinion;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.6019780039787292
I find it more readable  ;personal opinion;reduce list scanning;slob;Similar;0.5201597213745117
I find it more readable  ;personal opinion;improved alignment handling;improved;Similar;0.8286669254302979
I find it more readable  ;personal opinion;rework freelist handling;slob;Similar;0.6247607469558716
I find it more readable  ;personal opinion;can be slightly faster too;skip almost-full freelist pages completely;Similar;0.7169649600982666
I find it more readable  ;personal opinion;"align them to word size
";it is best in practice;Similar;0.6581640839576721
I find it more readable  ;personal opinion;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5092107653617859
I find it more readable  ;personal opinion;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5286060571670532
I find it more readable  ;personal opinion;Redo a lot of comments;Also;Similar;0.6817988753318787
I find it more readable  ;personal opinion;simplifies SLOB;at this point slob may be broken;Similar;0.7352710962295532
I find it more readable  ;personal opinion;fix;SLOB=y && SMP=y fix;Similar;0.7005342841148376
Drop it  ;if you want;fix bogus ksize calculation fix  ;SLOB;Similar;0.5859967470169067
Drop it  ;if you want;fix bogus ksize calculation;bogus;Similar;0.6369749307632446
Drop it  ;if you want;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.6779112815856934
Drop it  ;if you want;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5661376714706421
Drop it  ;if you want;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.6345682144165039
Drop it  ;if you want;remove useless ctor parameter and reorder parameters;useless;Similar;0.7140110731124878
Drop it  ;if you want;Handle that separately in krealloc();separately;Similar;0.7157474756240845
Drop it  ;if you want;reduce list scanning;slob;Similar;0.8101249933242798
Drop it  ;if you want;Cleanup zeroing allocations;Slab allocators;Similar;0.732966423034668
Drop it  ;if you want;remove bigblock tracking;slob;Similar;0.6773765087127686
Drop it  ;if you want;rework freelist handling;slob;Similar;0.5154646635055542
Drop it  ;if you want;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5845667123794556
Drop it  ;if you want;SLOB to be used on SMP  ;allows;Similar;0.5885142087936401
Drop it  ;if you want;fix page order calculation on not 4KB page;-;Similar;0.5301454067230225
Drop it  ;if you want;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.655204713344574
Drop it  ;if you want;handle SLAB_PANIC flag;slob;Similar;0.6221182346343994
Drop it  ;if you want;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5869635343551636
Drop it  ;if you want;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.5548914074897766
Drop it  ;if you want;Redo a lot of comments;Also;Similar;0.5189282894134521
Drop it  ;if you want;simplifies SLOB;at this point slob may be broken;Similar;0.6407483816146851
Drop it  ;if you want;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5862945318222046
Drop it  ;if you want;fix;SLOB=y && SMP=y fix;Similar;0.7099547386169434
fix bogus ksize calculation fix  ;SLOB;fixes the previous fix  ;completely wrong on closer inspection;Similar;0.6477146148681641
fix bogus ksize calculation fix  ;SLOB;fix bogus ksize calculation;bogus;Similar;0.9872711896896362
fix bogus ksize calculation fix  ;SLOB;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5425063371658325
fix bogus ksize calculation fix  ;SLOB;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5770286321640015
fix bogus ksize calculation fix  ;SLOB;Fix to return wrong pointer;Fix;Similar;0.772837221622467
fix bogus ksize calculation fix  ;SLOB;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.519737958908081
fix bogus ksize calculation fix  ;SLOB;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5752255916595459
fix bogus ksize calculation fix  ;SLOB;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.562598705291748
fix bogus ksize calculation fix  ;SLOB;fix memory corruption;memory corruption;Similar;0.7183713912963867
fix bogus ksize calculation fix  ;SLOB;remove useless ctor parameter and reorder parameters;useless;Similar;0.5697977542877197
fix bogus ksize calculation fix  ;SLOB;Handle that separately in krealloc();separately;Similar;0.6684585213661194
fix bogus ksize calculation fix  ;SLOB;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6595150232315063
fix bogus ksize calculation fix  ;SLOB;reduce list scanning;slob;Similar;0.6639313697814941
fix bogus ksize calculation fix  ;SLOB;Cleanup zeroing allocations;Slab allocators;Similar;0.6873775124549866
fix bogus ksize calculation fix  ;SLOB;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5883878469467163
fix bogus ksize calculation fix  ;SLOB;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.527682900428772
fix bogus ksize calculation fix  ;SLOB;improved alignment handling;improved;Similar;0.505852222442627
fix bogus ksize calculation fix  ;SLOB;remove bigblock tracking;slob;Similar;0.6813101768493652
fix bogus ksize calculation fix  ;SLOB;rework freelist handling;slob;Similar;0.6417206525802612
fix bogus ksize calculation fix  ;SLOB;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5435230135917664
fix bogus ksize calculation fix  ;SLOB;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6004783511161804
fix bogus ksize calculation fix  ;SLOB;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6999042630195618
fix bogus ksize calculation fix  ;SLOB;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5813031792640686
fix bogus ksize calculation fix  ;SLOB;SLOB to be used on SMP  ;allows;Similar;0.6273294687271118
fix bogus ksize calculation fix  ;SLOB;fix page order calculation on not 4KB page;-;Similar;0.6696613430976868
fix bogus ksize calculation fix  ;SLOB;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.7148979902267456
fix bogus ksize calculation fix  ;SLOB;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.685287594795227
fix bogus ksize calculation fix  ;SLOB;handle SLAB_PANIC flag;slob;Similar;0.6501283645629883
fix bogus ksize calculation fix  ;SLOB;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6239383220672607
fix bogus ksize calculation fix  ;SLOB;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6285595893859863
fix bogus ksize calculation fix  ;SLOB;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5552302002906799
fix bogus ksize calculation fix  ;SLOB;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6282285451889038
fix bogus ksize calculation fix  ;SLOB;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5301509499549866
fix bogus ksize calculation fix  ;SLOB;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6448997259140015
fix bogus ksize calculation fix  ;SLOB;simplifies SLOB;at this point slob may be broken;Similar;0.7137105464935303
fix bogus ksize calculation fix  ;SLOB;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.554632306098938
fix bogus ksize calculation fix  ;SLOB;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6732553243637085
fix bogus ksize calculation fix  ;SLOB;fix;SLOB=y && SMP=y fix;Similar;0.8096705675125122
fix bogus ksize calculation fix  ;SLOB;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.500527024269104
fixes the previous fix  ;completely wrong on closer inspection;fix bogus ksize calculation;bogus;Similar;0.6515060663223267
fixes the previous fix  ;completely wrong on closer inspection;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5432859659194946
fixes the previous fix  ;completely wrong on closer inspection;Fix to return wrong pointer;Fix;Similar;0.6508023738861084
fixes the previous fix  ;completely wrong on closer inspection;fix memory corruption;memory corruption;Similar;0.5494386553764343
fixes the previous fix  ;completely wrong on closer inspection;Handle that separately in krealloc();separately;Similar;0.5328727960586548
fixes the previous fix  ;completely wrong on closer inspection;reduce list scanning;slob;Similar;0.5336512327194214
fixes the previous fix  ;completely wrong on closer inspection;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.647506833076477
fixes the previous fix  ;completely wrong on closer inspection;improved alignment handling;improved;Similar;0.5922549962997437
fixes the previous fix  ;completely wrong on closer inspection;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5328754186630249
fixes the previous fix  ;completely wrong on closer inspection;remove bigblock tracking;slob;Similar;0.5163670778274536
fixes the previous fix  ;completely wrong on closer inspection;rework freelist handling;slob;Similar;0.6266211867332458
fixes the previous fix  ;completely wrong on closer inspection;"align them to word size
";it is best in practice;Similar;0.5392544865608215
fixes the previous fix  ;completely wrong on closer inspection;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5238337516784668
fixes the previous fix  ;completely wrong on closer inspection;SLOB to be used on SMP  ;allows;Similar;0.5531843900680542
fixes the previous fix  ;completely wrong on closer inspection;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6367601156234741
fixes the previous fix  ;completely wrong on closer inspection;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5249871611595154
fixes the previous fix  ;completely wrong on closer inspection;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5203130841255188
fixes the previous fix  ;completely wrong on closer inspection;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5620828866958618
fixes the previous fix  ;completely wrong on closer inspection;Redo a lot of comments;Also;Similar;0.531784176826477
fixes the previous fix  ;completely wrong on closer inspection;simplifies SLOB;at this point slob may be broken;Similar;0.6422265768051147
fixes the previous fix  ;completely wrong on closer inspection;fix;SLOB=y && SMP=y fix;Similar;0.7408163547515869
fix bogus ksize calculation;bogus;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.5799300670623779
fix bogus ksize calculation;bogus;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5970524549484253
fix bogus ksize calculation;bogus;Fix to return wrong pointer;Fix;Similar;0.7724260091781616
fix bogus ksize calculation;bogus;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5067456364631653
fix bogus ksize calculation;bogus;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5899355411529541
fix bogus ksize calculation;bogus;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.601977527141571
fix bogus ksize calculation;bogus;fix memory corruption;memory corruption;Similar;0.730093240737915
fix bogus ksize calculation;bogus;remove useless ctor parameter and reorder parameters;useless;Similar;0.6062419414520264
fix bogus ksize calculation;bogus;Handle that separately in krealloc();separately;Similar;0.6952236890792847
fix bogus ksize calculation;bogus;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.6536031365394592
fix bogus ksize calculation;bogus;reduce list scanning;slob;Similar;0.699425220489502
fix bogus ksize calculation;bogus;Cleanup zeroing allocations;Slab allocators;Similar;0.7089712619781494
fix bogus ksize calculation;bogus;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.572645902633667
fix bogus ksize calculation;bogus;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5396699905395508
fix bogus ksize calculation;bogus;improved alignment handling;improved;Similar;0.5185025334358215
fix bogus ksize calculation;bogus;remove bigblock tracking;slob;Similar;0.7007017731666565
fix bogus ksize calculation;bogus;rework freelist handling;slob;Similar;0.628551721572876
fix bogus ksize calculation;bogus;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5646179914474487
fix bogus ksize calculation;bogus;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5983450412750244
fix bogus ksize calculation;bogus;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6893291473388672
fix bogus ksize calculation;bogus;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5634727478027344
fix bogus ksize calculation;bogus;SLOB to be used on SMP  ;allows;Similar;0.6346339583396912
fix bogus ksize calculation;bogus;fix page order calculation on not 4KB page;-;Similar;0.6787554025650024
fix bogus ksize calculation;bogus;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.7068049907684326
fix bogus ksize calculation;bogus;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6907052397727966
fix bogus ksize calculation;bogus;handle SLAB_PANIC flag;slob;Similar;0.6533260941505432
fix bogus ksize calculation;bogus;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.628532886505127
fix bogus ksize calculation;bogus;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6313855648040771
fix bogus ksize calculation;bogus;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5603151321411133
fix bogus ksize calculation;bogus;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6032791137695312
fix bogus ksize calculation;bogus;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5001155138015747
fix bogus ksize calculation;bogus;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6330317854881287
fix bogus ksize calculation;bogus;simplifies SLOB;at this point slob may be broken;Similar;0.7268956303596497
fix bogus ksize calculation;bogus;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5497686862945557
fix bogus ksize calculation;bogus;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6907002925872803
fix bogus ksize calculation;bogus;fix;SLOB=y && SMP=y fix;Similar;0.822981595993042
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Similar;0.5088707804679871
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5454003810882568
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.6511549353599548
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;remove useless ctor parameter and reorder parameters;useless;Similar;0.8588235378265381
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Handle that separately in krealloc();separately;Similar;0.5771188735961914
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;reduce list scanning;slob;Similar;0.7791812419891357
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5065181255340576
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Cleanup zeroing allocations;Slab allocators;Similar;0.737581193447113
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5604003667831421
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;remove bigblock tracking;slob;Similar;0.689572811126709
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6536187529563904
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;fix page order calculation on not 4KB page;-;Similar;0.555508553981781
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5128206014633179
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.7352467775344849
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6331593990325928
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6623623371124268
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.5902478694915771
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5597397089004517
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6310358643531799
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Fix to return wrong pointer;Fix;Similar;0.5566637516021729
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";remove useless ctor parameter and reorder parameters;useless;Similar;0.5421584844589233
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Handle that separately in krealloc();separately;Similar;0.5692120790481567
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";reduce list scanning;slob;Similar;0.5383581519126892
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Cleanup zeroing allocations;Slab allocators;Similar;0.5926988124847412
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5667024254798889
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";rework freelist handling;slob;Similar;0.5631771087646484
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6079376339912415
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5148102641105652
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";fix page order calculation on not 4KB page;-;Similar;0.5340849161148071
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.534321665763855
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5898733139038086
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5849745869636536
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Slob does not need any special definitions;since we introduce a fallback case;Similar;0.6189399361610413
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5059167742729187
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5946601629257202
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";fix;SLOB=y && SMP=y fix;Similar;0.580817461013794
record page flag overlays explicitly;slob;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5012591481208801
record page flag overlays explicitly;slob;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5564250349998474
record page flag overlays explicitly;slob;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5181366801261902
record page flag overlays explicitly;slob;"align them to word size
";it is best in practice;Similar;0.6081055402755737
Fix to return wrong pointer;Fix;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Similar;0.5151722431182861
Fix to return wrong pointer;Fix;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5130795836448669
Fix to return wrong pointer;Fix;fix memory corruption;memory corruption;Similar;0.7393372654914856
Fix to return wrong pointer;Fix;remove useless ctor parameter and reorder parameters;useless;Similar;0.5406661629676819
Fix to return wrong pointer;Fix;Handle that separately in krealloc();separately;Similar;0.6142128705978394
Fix to return wrong pointer;Fix;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5815677642822266
Fix to return wrong pointer;Fix;reduce list scanning;slob;Similar;0.6280657052993774
Fix to return wrong pointer;Fix;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5310434103012085
Fix to return wrong pointer;Fix;Cleanup zeroing allocations;Slab allocators;Similar;0.5994623899459839
Fix to return wrong pointer;Fix;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5953364372253418
Fix to return wrong pointer;Fix;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5358903408050537
Fix to return wrong pointer;Fix;remove bigblock tracking;slob;Similar;0.579856276512146
Fix to return wrong pointer;Fix;rework freelist handling;slob;Similar;0.5441843271255493
Fix to return wrong pointer;Fix;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5503040552139282
Fix to return wrong pointer;Fix;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.7396653890609741
Fix to return wrong pointer;Fix;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.58149254322052
Fix to return wrong pointer;Fix;SLOB to be used on SMP  ;allows;Similar;0.515960693359375
Fix to return wrong pointer;Fix;fix page order calculation on not 4KB page;-;Similar;0.5951768159866333
Fix to return wrong pointer;Fix;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.7685164213180542
Fix to return wrong pointer;Fix;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6516420841217041
Fix to return wrong pointer;Fix;handle SLAB_PANIC flag;slob;Similar;0.5843283534049988
Fix to return wrong pointer;Fix;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6126131415367126
Fix to return wrong pointer;Fix;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5919407606124878
Fix to return wrong pointer;Fix;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5677895545959473
Fix to return wrong pointer;Fix;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5188146829605103
Fix to return wrong pointer;Fix;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6734126806259155
Fix to return wrong pointer;Fix;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5178029537200928
Fix to return wrong pointer;Fix;simplifies SLOB;at this point slob may be broken;Similar;0.5809415578842163
Fix to return wrong pointer;Fix;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6170523166656494
Fix to return wrong pointer;Fix;fix;SLOB=y && SMP=y fix;Similar;0.6751600503921509
Fix to return wrong pointer;Fix;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5456067323684692
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5243233442306519
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5200638175010681
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;fix memory corruption;memory corruption;Similar;0.6058825254440308
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Similar;0.5032624006271362
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Cleanup zeroing allocations;Slab allocators;Similar;0.5021179914474487
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5496144890785217
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.548633873462677
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;rework freelist handling;slob;Similar;0.5291507244110107
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5542769432067871
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.545020580291748
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5152300596237183
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;fix page order calculation on not 4KB page;-;Similar;0.6483830809593201
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6497001647949219
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6701441407203674
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.609015703201294
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.564496636390686
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.5224352478981018
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5175933241844177
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6786059141159058
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5467355251312256
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5526412129402161
reduce external fragmentation by using three free lists;slob;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Similar;0.5777717232704163
reduce external fragmentation by using three free lists;slob;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.5332388877868652
reduce external fragmentation by using three free lists;slob;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.565934419631958
reduce external fragmentation by using three free lists;slob;reduce list scanning;slob;Similar;0.5001193881034851
reduce external fragmentation by using three free lists;slob;rework freelist handling;slob;Similar;0.6208698749542236
reduce external fragmentation by using three free lists;slob;SLOB to be used on SMP  ;allows;Similar;0.5338922739028931
reduce external fragmentation by using three free lists;slob;fix page order calculation on not 4KB page;-;Similar;0.5072750449180603
reduce external fragmentation by using three free lists;slob;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5448688864707947
reduce external fragmentation by using three free lists;slob;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.604949951171875
reduce external fragmentation by using three free lists;slob;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5588427782058716
reduce external fragmentation by using three free lists;slob;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.53260737657547
reduce external fragmentation by using three free lists;slob;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5195239782333374
reduce external fragmentation by using three free lists;slob;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5028170347213745
reduce external fragmentation by using three free lists;slob;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5324141383171082
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Similar;0.6913097500801086
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;fix memory corruption;memory corruption;Similar;0.596451461315155
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;remove useless ctor parameter and reorder parameters;useless;Similar;0.5075125098228455
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;reduce list scanning;slob;Similar;0.6511577367782593
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Cleanup zeroing allocations;Slab allocators;Similar;0.5758169293403625
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5346989035606384
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;improved alignment handling;improved;Similar;0.50986647605896
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;remove bigblock tracking;slob;Similar;0.5651980042457581
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;rework freelist handling;slob;Similar;0.6108115911483765
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;fix page order calculation on not 4KB page;-;Similar;0.5323822498321533
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6496756076812744
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5379198789596558
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6090258359909058
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5448025465011597
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.53722083568573
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5061649084091187
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;simplifies SLOB;at this point slob may be broken;Similar;0.6112610101699829
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5503767728805542
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;fix memory corruption;memory corruption;Similar;0.5863379240036011
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;remove useless ctor parameter and reorder parameters;useless;Similar;0.6035265922546387
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Handle that separately in krealloc();separately;Similar;0.5270099639892578
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;reduce list scanning;slob;Similar;0.7584264874458313
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Cleanup zeroing allocations;Slab allocators;Similar;0.6889666318893433
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;improved alignment handling;improved;Similar;0.5686057209968567
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;remove bigblock tracking;slob;Similar;0.5744084119796753
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;rework freelist handling;slob;Similar;0.573331356048584
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;can be slightly faster too;skip almost-full freelist pages completely;Similar;0.5501883029937744
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5184363126754761
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;SLOB to be used on SMP  ;allows;Similar;0.5418898463249207
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;fix page order calculation on not 4KB page;-;Similar;0.5822675228118896
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5334330797195435
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5342836380004883
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6069760322570801
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5056170225143433
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.5155455470085144
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;simplifies SLOB;at this point slob may be broken;Similar;0.7102705240249634
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5138906240463257
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;fix;SLOB=y && SMP=y fix;Similar;0.5397595167160034
fix memory corruption;memory corruption;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.5256814956665039
fix memory corruption;memory corruption;reduce list scanning;slob;Similar;0.588387131690979
fix memory corruption;memory corruption;Cleanup zeroing allocations;Slab allocators;Similar;0.5827594995498657
fix memory corruption;memory corruption;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5872398614883423
fix memory corruption;memory corruption;improved alignment handling;improved;Similar;0.5578323602676392
fix memory corruption;memory corruption;remove bigblock tracking;slob;Similar;0.5274354219436646
fix memory corruption;memory corruption;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5135886669158936
fix memory corruption;memory corruption;rework freelist handling;slob;Similar;0.5469450950622559
fix memory corruption;memory corruption;"Taking a spinlock in a destructor is a bit risky
";since the slab allocators may run the destructors anytime they decide a slab is no longer needed;Similar;0.5453935265541077
fix memory corruption;memory corruption;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.7077859044075012
fix memory corruption;memory corruption;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5556028485298157
fix memory corruption;memory corruption;fix page order calculation on not 4KB page;-;Similar;0.5574254989624023
fix memory corruption;memory corruption;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6776164770126343
fix memory corruption;memory corruption;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5114538073539734
fix memory corruption;memory corruption;handle SLAB_PANIC flag;slob;Similar;0.5288497805595398
fix memory corruption;memory corruption;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6006121635437012
fix memory corruption;memory corruption;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5389342308044434
fix memory corruption;memory corruption;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5740677118301392
fix memory corruption;memory corruption;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6374902725219727
fix memory corruption;memory corruption;simplifies SLOB;at this point slob may be broken;Similar;0.6139030456542969
fix memory corruption;memory corruption;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.512710690498352
fix memory corruption;memory corruption;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6206914186477661
fix memory corruption;memory corruption;fix;SLOB=y && SMP=y fix;Similar;0.6091183423995972
fix memory corruption;memory corruption;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.6284921169281006
remove useless ctor parameter and reorder parameters;useless;Handle that separately in krealloc();separately;Similar;0.6260116696357727
remove useless ctor parameter and reorder parameters;useless;reduce list scanning;slob;Similar;0.8205522298812866
remove useless ctor parameter and reorder parameters;useless;Cleanup zeroing allocations;Slab allocators;Similar;0.7833558320999146
remove useless ctor parameter and reorder parameters;useless;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6445651054382324
remove useless ctor parameter and reorder parameters;useless;remove bigblock tracking;slob;Similar;0.7223581075668335
remove useless ctor parameter and reorder parameters;useless;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6947708129882812
remove useless ctor parameter and reorder parameters;useless;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.517136812210083
remove useless ctor parameter and reorder parameters;useless;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5247106552124023
remove useless ctor parameter and reorder parameters;useless;fix page order calculation on not 4KB page;-;Similar;0.5818914771080017
remove useless ctor parameter and reorder parameters;useless;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5214153528213501
remove useless ctor parameter and reorder parameters;useless;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.803428590297699
remove useless ctor parameter and reorder parameters;useless;handle SLAB_PANIC flag;slob;Similar;0.5546167492866516
remove useless ctor parameter and reorder parameters;useless;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6423898935317993
remove useless ctor parameter and reorder parameters;useless;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5036128759384155
remove useless ctor parameter and reorder parameters;useless;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6792670488357544
remove useless ctor parameter and reorder parameters;useless;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.6137977838516235
remove useless ctor parameter and reorder parameters;useless;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5924373269081116
remove useless ctor parameter and reorder parameters;useless;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5324509143829346
remove useless ctor parameter and reorder parameters;useless;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.668351411819458
Handle that separately in krealloc();separately;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Similar;0.652472734451294
Handle that separately in krealloc();separately;reduce list scanning;slob;Similar;0.7065733671188354
Handle that separately in krealloc();separately;Cleanup zeroing allocations;Slab allocators;Similar;0.6077989935874939
Handle that separately in krealloc();separately;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5197654366493225
Handle that separately in krealloc();separately;remove bigblock tracking;slob;Similar;0.6093336343765259
Handle that separately in krealloc();separately;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5356399416923523
Handle that separately in krealloc();separately;rework freelist handling;slob;Similar;0.5954873561859131
Handle that separately in krealloc();separately;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6721616983413696
Handle that separately in krealloc();separately;"align them to word size
";it is best in practice;Similar;0.5064973831176758
Handle that separately in krealloc();separately;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.63478684425354
Handle that separately in krealloc();separately;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5728441476821899
Handle that separately in krealloc();separately;SLOB to be used on SMP  ;allows;Similar;0.6491095423698425
Handle that separately in krealloc();separately;fix page order calculation on not 4KB page;-;Similar;0.6683138608932495
Handle that separately in krealloc();separately;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5663782358169556
Handle that separately in krealloc();separately;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6298730373382568
Handle that separately in krealloc();separately;handle SLAB_PANIC flag;slob;Similar;0.5863794088363647
Handle that separately in krealloc();separately;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.754412829875946
Handle that separately in krealloc();separately;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5841896533966064
Handle that separately in krealloc();separately;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5992836356163025
Handle that separately in krealloc();separately;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5652420520782471
Handle that separately in krealloc();separately;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5157225131988525
Handle that separately in krealloc();separately;simplifies SLOB;at this point slob may be broken;Similar;0.6119639277458191
Handle that separately in krealloc();separately;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5614228844642639
Handle that separately in krealloc();separately;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6398800015449524
Handle that separately in krealloc();separately;fix;SLOB=y && SMP=y fix;Similar;0.6339063048362732
Handle that separately in krealloc();separately;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5931383371353149
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;reduce list scanning;slob;Similar;0.5180951952934265
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5504987239837646
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5695306062698364
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;remove bigblock tracking;slob;Similar;0.5240179896354675
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5183017253875732
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;rework freelist handling;slob;Similar;0.5630726218223572
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5729948282241821
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6365076303482056
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.6026917099952698
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;SLOB to be used on SMP  ;allows;Similar;0.7308889627456665
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;fix page order calculation on not 4KB page;-;Similar;0.5747557878494263
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.569779634475708
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;handle SLAB_PANIC flag;slob;Similar;0.56951504945755
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5948311686515808
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6240789890289307
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.540299654006958
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6169936656951904
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6237529516220093
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5151116251945496
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;simplifies SLOB;at this point slob may be broken;Similar;0.6158075332641602
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5242881178855896
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6066211462020874
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6917821168899536
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5508067607879639
reduce list scanning;slob;starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Similar;0.5440797805786133
reduce list scanning;slob;Cleanup zeroing allocations;Slab allocators;Similar;0.8207494616508484
reduce list scanning;slob;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5484679937362671
reduce list scanning;slob;remove bigblock tracking;slob;Similar;0.7670767307281494
reduce list scanning;slob;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5137618780136108
reduce list scanning;slob;rework freelist handling;slob;Similar;0.5832397937774658
reduce list scanning;slob;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.6411310434341431
reduce list scanning;slob;"align them to word size
";it is best in practice;Similar;0.5079482793807983
reduce list scanning;slob;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5292293429374695
reduce list scanning;slob;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5947977304458618
reduce list scanning;slob;SLOB to be used on SMP  ;allows;Similar;0.6119680404663086
reduce list scanning;slob;fix page order calculation on not 4KB page;-;Similar;0.7041504383087158
reduce list scanning;slob;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6218209266662598
reduce list scanning;slob;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.7709744572639465
reduce list scanning;slob;handle SLAB_PANIC flag;slob;Similar;0.608927309513092
reduce list scanning;slob;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5109423398971558
reduce list scanning;slob;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5518561601638794
reduce list scanning;slob;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5830138921737671
reduce list scanning;slob;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5383644104003906
reduce list scanning;slob;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6314935684204102
reduce list scanning;slob;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.5356485843658447
reduce list scanning;slob;Redo a lot of comments;Also;Similar;0.5682411193847656
reduce list scanning;slob;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5568734407424927
reduce list scanning;slob;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5048809051513672
reduce list scanning;slob;simplifies SLOB;at this point slob may be broken;Similar;0.6174719929695129
reduce list scanning;slob;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6595554351806641
reduce list scanning;slob;fix;SLOB=y && SMP=y fix;Similar;0.6515815258026123
starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5324946045875549
starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;remove bigblock tracking;slob;Similar;0.5292213559150696
starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5971709489822388
starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.524621844291687
starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;fix page order calculation on not 4KB page;-;Similar;0.551417350769043
starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5633372068405151
starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5657544136047363
starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5345310568809509
starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5139230489730835
starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5679552555084229
starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5551736354827881
starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5578827261924744
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.603850245475769
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6330865025520325
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5023550987243652
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.65519779920578
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5266914963722229
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5317271947860718
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.514833927154541
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5533245205879211
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6037514209747314
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5609006881713867
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5929707288742065
Cleanup zeroing allocations;Slab allocators;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Similar;0.5649886727333069
Cleanup zeroing allocations;Slab allocators;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.5527750253677368
Cleanup zeroing allocations;Slab allocators;remove bigblock tracking;slob;Similar;0.6809520721435547
Cleanup zeroing allocations;Slab allocators;rework freelist handling;slob;Similar;0.5743606090545654
Cleanup zeroing allocations;Slab allocators;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.619050920009613
Cleanup zeroing allocations;Slab allocators;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5125257968902588
Cleanup zeroing allocations;Slab allocators;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5558770895004272
Cleanup zeroing allocations;Slab allocators;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5458984375
Cleanup zeroing allocations;Slab allocators;SLOB to be used on SMP  ;allows;Similar;0.5366234183311462
Cleanup zeroing allocations;Slab allocators;fix page order calculation on not 4KB page;-;Similar;0.5682001709938049
Cleanup zeroing allocations;Slab allocators;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5894927978515625
Cleanup zeroing allocations;Slab allocators;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.7179887294769287
Cleanup zeroing allocations;Slab allocators;handle SLAB_PANIC flag;slob;Similar;0.5716555714607239
Cleanup zeroing allocations;Slab allocators;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5156049728393555
Cleanup zeroing allocations;Slab allocators;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5260573625564575
Cleanup zeroing allocations;Slab allocators;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5542194247245789
Cleanup zeroing allocations;Slab allocators;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6562734842300415
Cleanup zeroing allocations;Slab allocators;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5119450688362122
Cleanup zeroing allocations;Slab allocators;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5558330416679382
Cleanup zeroing allocations;Slab allocators;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.5744532346725464
Cleanup zeroing allocations;Slab allocators;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5005127787590027
Cleanup zeroing allocations;Slab allocators;simplifies SLOB;at this point slob may be broken;Similar;0.5934774279594421
Cleanup zeroing allocations;Slab allocators;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6979897022247314
Cleanup zeroing allocations;Slab allocators;fix;SLOB=y && SMP=y fix;Similar;0.6260143518447876
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Similar;0.6157383918762207
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Similar;0.5226345062255859
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;remove bigblock tracking;slob;Similar;0.5012837052345276
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;rework freelist handling;slob;Similar;0.5032674670219421
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5255469083786011
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5009690523147583
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.6396313905715942
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;fix page order calculation on not 4KB page;-;Similar;0.51617431640625
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.570213794708252
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5680797100067139
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6223098039627075
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5880500078201294
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5857727527618408
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6015924215316772
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.546865701675415
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6576159000396729
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6212988495826721
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6936061382293701
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;remove bigblock tracking;slob;Similar;0.5712620615959167
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5929744243621826
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.550473153591156
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5561880469322205
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.6915605068206787
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;fix page order calculation on not 4KB page;-;Similar;0.5898364782333374
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5739152431488037
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6091310977935791
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;handle SLAB_PANIC flag;slob;Similar;0.5384576320648193
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5133176445960999
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5896739959716797
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6981250643730164
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5639636516571045
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6633180975914001
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5846974849700928
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7184098958969116
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;SLOB saving nearly half a megabyte of RAM;Comparison for otherwise identical builds;Similar;0.506946325302124
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5764720439910889
improved alignment handling;improved;rework freelist handling;slob;Similar;0.6177104711532593
improved alignment handling;improved;can be slightly faster too;skip almost-full freelist pages completely;Similar;0.6603459119796753
improved alignment handling;improved;"align them to word size
";it is best in practice;Similar;0.6142822504043579
improved alignment handling;improved;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5191000699996948
improved alignment handling;improved;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5806406140327454
improved alignment handling;improved;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5962774753570557
improved alignment handling;improved;Redo a lot of comments;Also;Similar;0.5999007821083069
improved alignment handling;improved;simplifies SLOB;at this point slob may be broken;Similar;0.6944571137428284
improved alignment handling;improved;fix;SLOB=y && SMP=y fix;Similar;0.6555034518241882
Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Allocation size is stored in page->private;makes ksize more accurate than it previously was;Similar;0.5045467615127563
Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;rework freelist handling;slob;Similar;0.5336922407150269
Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;"align them to word size
";it is best in practice;Similar;0.6227267980575562
Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5198012590408325
Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;fix page order calculation on not 4KB page;-;Similar;0.5313620567321777
Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5088993310928345
Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5673091411590576
Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;handle SLAB_PANIC flag;slob;Similar;0.5066041946411133
Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5194675922393799
Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5435901880264282
Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5103670358657837
Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5113523006439209
Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5103307366371155
Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6224522590637207
remove bigblock tracking;slob;rework freelist handling;slob;Similar;0.5072580575942993
remove bigblock tracking;slob;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Similar;0.5709676742553711
remove bigblock tracking;slob;"align them to word size
";it is best in practice;Similar;0.5363657474517822
remove bigblock tracking;slob;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.6370601058006287
remove bigblock tracking;slob;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5977551937103271
remove bigblock tracking;slob;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5794380307197571
remove bigblock tracking;slob;SLOB to be used on SMP  ;allows;Similar;0.6175469160079956
remove bigblock tracking;slob;fix page order calculation on not 4KB page;-;Similar;0.5469038486480713
remove bigblock tracking;slob;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5362152457237244
remove bigblock tracking;slob;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.8203655481338501
remove bigblock tracking;slob;handle SLAB_PANIC flag;slob;Similar;0.7290717959403992
remove bigblock tracking;slob;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6176160573959351
remove bigblock tracking;slob;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.566214919090271
remove bigblock tracking;slob;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5473092794418335
remove bigblock tracking;slob;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6134579181671143
remove bigblock tracking;slob;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6200439929962158
remove bigblock tracking;slob;Redo a lot of comments;Also;Similar;0.5850920081138611
remove bigblock tracking;slob;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5261533260345459
remove bigblock tracking;slob;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6008021831512451
remove bigblock tracking;slob;simplifies SLOB;at this point slob may be broken;Similar;0.6313366889953613
remove bigblock tracking;slob;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7087195515632629
remove bigblock tracking;slob;fix;SLOB=y && SMP=y fix;Similar;0.6420215368270874
remove bigblock tracking;slob;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5410535931587219
Allocation size is stored in page->private;makes ksize more accurate than it previously was;rework freelist handling;slob;Similar;0.5172083377838135
Allocation size is stored in page->private;makes ksize more accurate than it previously was;"align them to word size
";it is best in practice;Similar;0.5059752464294434
Allocation size is stored in page->private;makes ksize more accurate than it previously was;SLOB to be used on SMP  ;allows;Similar;0.5032792091369629
Allocation size is stored in page->private;makes ksize more accurate than it previously was;fix page order calculation on not 4KB page;-;Similar;0.5998876690864563
Allocation size is stored in page->private;makes ksize more accurate than it previously was;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.533785343170166
Allocation size is stored in page->private;makes ksize more accurate than it previously was;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5037214756011963
Allocation size is stored in page->private;makes ksize more accurate than it previously was;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5351262092590332
Allocation size is stored in page->private;makes ksize more accurate than it previously was;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5694410800933838
Allocation size is stored in page->private;makes ksize more accurate than it previously was;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5438780784606934
Allocation size is stored in page->private;makes ksize more accurate than it previously was;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5308979153633118
rework freelist handling;slob;"align them to word size
";it is best in practice;Similar;0.5571517944335938
rework freelist handling;slob;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5269898772239685
rework freelist handling;slob;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5981409549713135
rework freelist handling;slob;SLOB to be used on SMP  ;allows;Similar;0.6045259237289429
rework freelist handling;slob;fix page order calculation on not 4KB page;-;Similar;0.5529261827468872
rework freelist handling;slob;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6410253047943115
rework freelist handling;slob;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6145009994506836
rework freelist handling;slob;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.7208669185638428
rework freelist handling;slob;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5956196188926697
rework freelist handling;slob;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5675649642944336
rework freelist handling;slob;Redo a lot of comments;Also;Similar;0.5345760583877563
rework freelist handling;slob;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5481821298599243
rework freelist handling;slob;simplifies SLOB;at this point slob may be broken;Similar;0.6947121620178223
rework freelist handling;slob;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5140081644058228
rework freelist handling;slob;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5550789833068848
rework freelist handling;slob;fix;SLOB=y && SMP=y fix;Similar;0.6555020809173584
rework freelist handling;slob;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5618253946304321
can be slightly faster too;skip almost-full freelist pages completely;Code size goes up about 1K;dynamic memory usage should be lower even on relatively small memory systems;Similar;0.5728548765182495
can be slightly faster too;skip almost-full freelist pages completely;Redo a lot of comments;Also;Similar;0.5386312007904053
can be slightly faster too;skip almost-full freelist pages completely;simplifies SLOB;at this point slob may be broken;Similar;0.5913529396057129
can be slightly faster too;skip almost-full freelist pages completely;fix;SLOB=y && SMP=y fix;Similar;0.5175341367721558
we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Similar;0.5965982675552368
we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";SLOB to be used on SMP  ;allows;Similar;0.5489501953125
we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";fix page order calculation on not 4KB page;-;Similar;0.698905885219574
we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5094524025917053
we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5873398780822754
we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5010702610015869
we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6610053181648254
we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.614595890045166
we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Slob does not need any special definitions;since we introduce a fallback case;Similar;0.6951006650924683
we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5035500526428223
we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6882513761520386
we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6736687421798706
"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;"align them to word size
";it is best in practice;Similar;0.5292142033576965
"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;Code size goes up about 1K;dynamic memory usage should be lower even on relatively small memory systems;Similar;0.6939777731895447
"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5499143600463867
"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;SLOB saving nearly half a megabyte of RAM;Comparison for otherwise identical builds;Similar;0.5252658128738403
"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6242039799690247
"align them to word size
";it is best in practice;SLOB to be used on SMP  ;allows;Similar;0.5733203291893005
"align them to word size
";it is best in practice;handle SLAB_PANIC flag;slob;Similar;0.5159846544265747
"align them to word size
";it is best in practice;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6604804992675781
"align them to word size
";it is best in practice;Redo a lot of comments;Also;Similar;0.6940250396728516
"align them to word size
";it is best in practice;simplifies SLOB;at this point slob may be broken;Similar;0.6093096733093262
"align them to word size
";it is best in practice;fix;SLOB=y && SMP=y fix;Similar;0.6297057867050171
"align them to word size
";it is best in practice;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.7235427498817444
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.5453635454177856
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;SLOB to be used on SMP  ;allows;Similar;0.7249648571014404
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;fix page order calculation on not 4KB page;-;Similar;0.5279649496078491
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5547429323196411
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5905061364173889
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;handle SLAB_PANIC flag;slob;Similar;0.6097280383110046
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.618201494216919
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5040844678878784
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5724660158157349
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6070868968963623
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5734671354293823
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5336501598358154
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;simplifies SLOB;at this point slob may be broken;Similar;0.5701292753219604
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5505618453025818
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7324494123458862
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6026749610900879
Code size goes up about 1K;dynamic memory usage should be lower even on relatively small memory systems;SLOB saving nearly half a megabyte of RAM;Comparison for otherwise identical builds;Similar;0.6824935078620911
Code size goes up about 1K;dynamic memory usage should be lower even on relatively small memory systems;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5393450260162354
"Taking a spinlock in a destructor is a bit risky
";since the slab allocators may run the destructors anytime they decide a slab is no longer needed;"
Use a destructor will BUG()
";"
Any attempt to";Similar;0.5764120221138
"
Use a destructor will BUG()
";"
Any attempt to";The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.6124634742736816
"
Use a destructor will BUG()
";"
Any attempt to";SLOB to be used on SMP  ;allows;Similar;0.5575757622718811
"
Use a destructor will BUG()
";"
Any attempt to";fix page order calculation on not 4KB page;-;Similar;0.5529290437698364
"
Use a destructor will BUG()
";"
Any attempt to";This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.6767969727516174
"
Use a destructor will BUG()
";"
Any attempt to";Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5588077306747437
"
Use a destructor will BUG()
";"
Any attempt to";handle SLAB_PANIC flag;slob;Similar;0.6770630478858948
"
Use a destructor will BUG()
";"
Any attempt to";Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.5353723168373108
"
Use a destructor will BUG()
";"
Any attempt to";added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5852134823799133
"
Use a destructor will BUG()
";"
Any attempt to";[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5139896273612976
"
Use a destructor will BUG()
";"
Any attempt to";"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5408638119697571
"
Use a destructor will BUG()
";"
Any attempt to";Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5988516211509705
"
Use a destructor will BUG()
";"
Any attempt to";This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5597349405288696
"
Use a destructor will BUG()
";"
Any attempt to";Redo a lot of comments;Also;Similar;0.5411133766174316
"
Use a destructor will BUG()
";"
Any attempt to";If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.7463010549545288
"
Use a destructor will BUG()
";"
Any attempt to";simplifies SLOB;at this point slob may be broken;Similar;0.5546952486038208
"
Use a destructor will BUG()
";"
Any attempt to";[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5283493995666504
"
Use a destructor will BUG()
";"
Any attempt to";Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5690815448760986
"
Use a destructor will BUG()
";"
Any attempt to";fix;SLOB=y && SMP=y fix;Similar;0.625076174736023
"
Use a destructor will BUG()
";"
Any attempt to";Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.6137710213661194
The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5252019166946411
The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6494982242584229
The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;handle SLAB_PANIC flag;slob;Similar;0.6189633011817932
The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.6835027933120728
The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5031075477600098
The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6099697947502136
The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.543790340423584
The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5007708668708801
The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5860936641693115
The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6216602325439453
The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6033684015274048
The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5084109902381897
SLOB to be used on SMP  ;allows;fix page order calculation on not 4KB page;-;Similar;0.528947114944458
SLOB to be used on SMP  ;allows;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.5036923289299011
SLOB to be used on SMP  ;allows;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.5220754146575928
SLOB to be used on SMP  ;allows;handle SLAB_PANIC flag;slob;Similar;0.6128886938095093
SLOB to be used on SMP  ;allows;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6931465864181519
SLOB to be used on SMP  ;allows;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.5560100078582764
SLOB to be used on SMP  ;allows;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6093624830245972
SLOB to be used on SMP  ;allows;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5640023946762085
SLOB to be used on SMP  ;allows;Redo a lot of comments;Also;Similar;0.5377361178398132
SLOB to be used on SMP  ;allows;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5590227842330933
SLOB to be used on SMP  ;allows;simplifies SLOB;at this point slob may be broken;Similar;0.7485079765319824
SLOB to be used on SMP  ;allows;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6277427077293396
SLOB to be used on SMP  ;allows;fix;SLOB=y && SMP=y fix;Similar;0.6382821202278137
SLOB to be used on SMP  ;allows;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.7580345869064331
fix page order calculation on not 4KB page;-;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Similar;0.680827260017395
fix page order calculation on not 4KB page;-;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6071174144744873
fix page order calculation on not 4KB page;-;handle SLAB_PANIC flag;slob;Similar;0.5232030749320984
fix page order calculation on not 4KB page;-;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.6341683268547058
fix page order calculation on not 4KB page;-;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6489236950874329
fix page order calculation on not 4KB page;-;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.570929765701294
fix page order calculation on not 4KB page;-;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.526619553565979
fix page order calculation on not 4KB page;-;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5568275451660156
fix page order calculation on not 4KB page;-;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.7183567881584167
fix page order calculation on not 4KB page;-;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6289718151092529
fix page order calculation on not 4KB page;-;fix;SLOB=y && SMP=y fix;Similar;0.5240165591239929
fix page order calculation on not 4KB page;-;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5154430866241455
fix page order calculation on not 4KB page;-;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5092983245849609
This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.6056227684020996
This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Similar;0.7043258547782898
This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5357798933982849
This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.556359589099884
This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6010540127754211
This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.6722124218940735
This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5724549293518066
This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.7089695930480957
This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6300684213638306
This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();simplifies SLOB;at this point slob may be broken;Similar;0.5252799987792969
This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6374378204345703
This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6315358877182007
This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();fix;SLOB=y && SMP=y fix;Similar;0.598459005355835
This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5316319465637207
Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;handle SLAB_PANIC flag;slob;Similar;0.6819397807121277
Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.531326413154602
Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.6414902210235596
Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.6124061346054077
Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6632611751556396
Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.5007439851760864
Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.6773940324783325
Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5174039602279663
Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6867232918739319
Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;simplifies SLOB;at this point slob may be broken;Similar;0.5389322638511658
Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5624812245368958
Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7302661538124084
Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;fix;SLOB=y && SMP=y fix;Similar;0.5867489576339722
Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5052375793457031
handle SLAB_PANIC flag;slob;Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.6133708357810974
handle SLAB_PANIC flag;slob;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Similar;0.5076306462287903
handle SLAB_PANIC flag;slob;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6232635974884033
handle SLAB_PANIC flag;slob;Redo a lot of comments;Also;Similar;0.5508100986480713
handle SLAB_PANIC flag;slob;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5478643178939819
handle SLAB_PANIC flag;slob;simplifies SLOB;at this point slob may be broken;Similar;0.6134809851646423
handle SLAB_PANIC flag;slob;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6198041439056396
handle SLAB_PANIC flag;slob;fix;SLOB=y && SMP=y fix;Similar;0.6117315292358398
handle SLAB_PANIC flag;slob;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5425312519073486
Introduce krealloc();reallocates memory while keeping the contents unchanged;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5429873466491699
Introduce krealloc();reallocates memory while keeping the contents unchanged;Redo a lot of comments;Also;Similar;0.5680865049362183
Introduce krealloc();reallocates memory while keeping the contents unchanged;simplifies SLOB;at this point slob may be broken;Similar;0.6677430272102356
Introduce krealloc();reallocates memory while keeping the contents unchanged;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5982533097267151
Introduce krealloc();reallocates memory while keeping the contents unchanged;fix;SLOB=y && SMP=y fix;Similar;0.6839190721511841
Introduce krealloc();reallocates memory while keeping the contents unchanged;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.6787188053131104
added a simple non-optimized version for mm/slob.c for compatibility;compatibility;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Similar;0.5402557849884033
added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.5497841835021973
added a simple non-optimized version for mm/slob.c for compatibility;compatibility;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.558230996131897
added a simple non-optimized version for mm/slob.c for compatibility;compatibility;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5791857242584229
added a simple non-optimized version for mm/slob.c for compatibility;compatibility;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.6405228972434998
added a simple non-optimized version for mm/slob.c for compatibility;compatibility;simplifies SLOB;at this point slob may be broken;Similar;0.6005606055259705
added a simple non-optimized version for mm/slob.c for compatibility;compatibility;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.6247386932373047
added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5763066411018372
added a simple non-optimized version for mm/slob.c for compatibility;compatibility;fix;SLOB=y && SMP=y fix;Similar;0.5308641791343689
added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5515652894973755
added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.6034368872642517
[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Similar;0.6892554759979248
[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.7408126592636108
[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6661182641983032
"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5655742883682251
"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.5154925584793091
"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5021470785140991
"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5776498317718506
"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5881868600845337
Cleanup slab headers / API  ;to allow easy addition of new slab allocators;This patch cleans up the slab header definitions;cleans up the slab header definitions;Similar;0.753875732421875
Cleanup slab headers / API  ;to allow easy addition of new slab allocators;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Similar;0.5428023338317871
Cleanup slab headers / API  ;to allow easy addition of new slab allocators;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5693780183792114
Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.6626077890396118
Cleanup slab headers / API  ;to allow easy addition of new slab allocators;simplifies SLOB;at this point slob may be broken;Similar;0.5670061111450195
Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6310261487960815
Cleanup slab headers / API  ;to allow easy addition of new slab allocators;fix;SLOB=y && SMP=y fix;Similar;0.5572385787963867
Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.618495762348175
This patch cleans up the slab header definitions;cleans up the slab header definitions;If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Similar;0.5196956396102905
This patch cleans up the slab header definitions;cleans up the slab header definitions;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.5928129553794861
This patch cleans up the slab header definitions;cleans up the slab header definitions;simplifies SLOB;at this point slob may be broken;Similar;0.5089219808578491
add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Similar;0.7410144805908203
add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.62159663438797
add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7124669551849365
add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5909818410873413
add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.554796576499939
Redo a lot of comments;Also;simplifies SLOB;at this point slob may be broken;Similar;0.623164176940918
Redo a lot of comments;Also;fix;SLOB=y && SMP=y fix;Similar;0.6046826243400574
Redo a lot of comments;Also;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5710586905479431
If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;simplifies SLOB;at this point slob may be broken;Similar;0.5393209457397461
If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5227445363998413
If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5652545690536499
If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;fix;SLOB=y && SMP=y fix;Similar;0.5238366723060608
If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.511884331703186
If it will be decided to BUG in this case, BUG should be put in generic code;put in generic code;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.6681174039840698
Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Similar;0.5982012748718262
Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.6977354884147644
Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5713806748390198
simplifies SLOB;at this point slob may be broken;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.5430497527122498
simplifies SLOB;at this point slob may be broken;fix;SLOB=y && SMP=y fix;Similar;0.7866841554641724
simplifies SLOB;at this point slob may be broken;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.619371771812439
[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.7272013425827026
[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5399655699729919
Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;fix;SLOB=y && SMP=y fix;Similar;0.5279186964035034
Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5695160627365112
Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5444735884666443
fix;SLOB=y && SMP=y fix;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5074789524078369
SLOB saving nearly half a megabyte of RAM;Comparison for otherwise identical builds;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Similar;0.5466260313987732
SLOB saving nearly half a megabyte of RAM;Comparison for otherwise identical builds;Fix 32-bitness bugs in mm/slob.c;bugs;Similar;0.5033378601074219
