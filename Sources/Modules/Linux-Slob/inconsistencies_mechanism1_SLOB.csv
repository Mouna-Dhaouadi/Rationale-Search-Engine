Decision1;Rationale1;Decision2;Rationale2;Relationship;Alpha;similarity_rationales;contradiction_rationales
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Similar;0.8149130344390869;0.5648862719535828;
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.8136509656906128;0.4753948748111725;
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Introduce krealloc();reallocates memory while keeping the contents unchanged;Similar;0.813710629940033;0.4669268727302551;
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Remove kmemtrace ftrace plugin;tracing;Similar;0.8384854793548584;0.4034021198749542;
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.8083182573318481;0.6438542604446411;
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.9312577247619628;0.3261217474937439;
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Remove kmemtrace ftrace plugin;tracing;Similar;0.8214273452758789;0.44089698791503906;
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.8133531808853149;0.6538981795310974;
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.8133531808853149;0.2297077775001526;
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.8084738254547119;0.27476879954338074;
Remove unnecessary page_mapcount_reset() function call;unnecessary;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.8394695520401001;1.0;
Remove unnecessary page_mapcount_reset() function call;unnecessary;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.8120056390762329;0.5877834558486938;
Remove unnecessary page_mapcount_reset() function call;unnecessary;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.8113442659378052;0.599732518196106;
Remove unnecessary page_mapcount_reset() function call;unnecessary;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.8182327747344971;0.9154572486877441;
use struct folio instead of struct page;Where non-slab page can appear;By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Similar;0.8281378746032715;0.4968287944793701;
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Similar;0.8277384042739868;0.46874770522117615;
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Similar;0.8131656646728516;0.5163740515708923;
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Similar;0.8202522993087769;0.37149232625961304;
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;reduce list scanning;slob;Similar;0.8099866509437561;0.40326541662216187;
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;fix lockup in slob_free()  ;lockup;Similar;0.801626980304718;0.4862172305583954;
Add mem_dump_obj() to print source of memory block;to print source of memory block;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.8057849407196045;0.6530901789665222;
Add mem_dump_obj() to print source of memory block;to print source of memory block;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.8057849407196045;0.3818187713623047;
Add mem_dump_obj() to print source of memory block;to print source of memory block;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.8035950660705566;0.3812965750694275;
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Add a return parameter to slob_page_alloc();to signal that the list was modified;Similar;0.80347740650177;0.33856403827667236;
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;0.80347740650177;0.40814173221588135;
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Fix gfp flags passed to lockdep;lockdep;Similar;0.805382490158081;0.3256775736808777;
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.8389064073562622;0.7001917362213135;
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;can be done in __kmem_cache_shutdown;What is done there;Similar;0.8336160182952881;0.6372968554496765;
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Remove kmemtrace ftrace plugin;tracing;Similar;0.8030635118484497;0.6231599450111389;
Add a return parameter to slob_page_alloc();to signal that the list was modified;Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Similar;1.000000238418579;0.41891181468963623;
Add a return parameter to slob_page_alloc();to signal that the list was modified;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.8003391027450562;0.5968138575553894;
Add a return parameter to slob_page_alloc();to signal that the list was modified;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.8592509031295776;0.4655742645263672;
Add a return parameter to slob_page_alloc();to signal that the list was modified;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.8490380048751831;0.5860772132873535;
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.8003391027450562;0.2855081856250763;
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Similar;0.8592509031295776;0.5051929950714111;
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.8490380048751831;0.12994752824306488;
remove an unnecessary check for __GFP_ZERO;unnecessary;Remove the unnecessary NULL pointer check;unnecessary;Similar;0.8974013328552246;1.0;
remove an unnecessary check for __GFP_ZERO;unnecessary;remove useless ctor parameter and reorder parameters;useless;Similar;0.8305707573890686;0.9355728030204773;
remove an unnecessary check for __GFP_ZERO;unnecessary;reduce list scanning;slob;Similar;0.823667049407959;0.8151568174362183;
remove an unnecessary check for __GFP_ZERO;unnecessary;Cleanup zeroing allocations;Slab allocators;Similar;0.8183853626251221;0.4357461631298065;
Remove the unnecessary NULL pointer check;unnecessary;remove useless ctor parameter and reorder parameters;useless;Similar;0.8053261041641235;0.9355728030204773;
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Similar;0.854971170425415;0.5514034032821655;
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Remove various small accessors;various small;Similar;0.8317462205886841;0.40481868386268616;
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.8668506145477295;0.5843202471733093;
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Drop it  ;if you want;Similar;0.804094672203064;0.4634997546672821;
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;reduce list scanning;slob;Similar;0.9132909774780272;0.45037519931793213;
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.8136236667633057;0.6325908899307251;
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.821791410446167;0.5472950339317322;
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Similar;0.805903434753418;0.33247578144073486;0.9000850319862366
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.8209226131439209;0.4703749418258667;
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.8013726472854614;0.24918362498283386;
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Similar;0.818953275680542;0.3085547685623169;
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;They are no longer needed;They have become so simple;Similar;0.8125553131103516;0.025637414306402206;0.8780893683433533
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.8038849830627441;0.4999103546142578;
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;can be done in __kmem_cache_shutdown;What is done there;Similar;0.803275465965271;0.3981854021549225;
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Remove kmemtrace ftrace plugin;tracing;Similar;0.8294204473495483;0.37487784028053284;
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;remove useless ctor parameter and reorder parameters;useless;Similar;0.8273647427558899;0.5658456683158875;
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.8448187112808228;0.6395039558410645;
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Similar;0.8565096259117126;0.6847496628761292;
Improve trace accuracy;by correctly tracing reported size;enable and use this tracer;To enable and use this tracer;Similar;0.8528776168823242;0.7510898113250732;
Improve trace accuracy;by correctly tracing reported size;improved alignment handling;improved;Similar;0.8486950397491455;0.6413378715515137;
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;The bit should be passed to trace_kmalloc_node()  ;as well;Similar;0.8170105814933777;0.32132044434547424;
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Similar;0.8026189804077148;0.5073464512825012;
This will allow us to push more processing into common code later;improve readability;refactor code for future changes;Impact;Similar;0.8159323930740356;0.5838217735290527;
can be done in __kmem_cache_shutdown;What is done there;Remove kmemtrace ftrace plugin;tracing;Similar;0.8111629486083984;0.8011128902435303;
can be done in __kmem_cache_shutdown;What is done there;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Similar;0.8259381055831909;0.5840917825698853;
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Similar;0.8300827741622925;0.473706990480423;
Remove various small accessors;various small;reduce list scanning;slob;Similar;0.8466690182685852;0.7113665342330933;
No need to zero mapping since it is no longer in use;it is no longer in use;Slob does not need any special definitions;since we introduce a fallback case;Similar;0.8062189817428589;0.539348840713501;
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Similar;0.8208510875701904;0.44414645433425903;
Remove kmemtrace ftrace plugin;tracing;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Similar;0.8647539615631104;0.6287064552307129;
Remove kmemtrace ftrace plugin;tracing;remove bigblock tracking;slob;Similar;0.8015408515930176;0.707062304019928;
Remove kmemtrace ftrace plugin;tracing;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.8483313918113708;0.6303781270980835;
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Drop it  ;if you want;Similar;0.8542777299880981;0.4595608115196228;
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Similar;0.8357622623443604;0.6107777953147888;
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;remove useless ctor parameter and reorder parameters;useless;Similar;0.8131567239761353;0.41267967224121094;0.6677667498588562
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;reduce list scanning;slob;Similar;0.8518403768539429;0.6095860600471497;
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;remove bigblock tracking;slob;Similar;0.8058533668518066;0.6095860600471497;
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Similar;0.8012063503265381;0.50240558385849;
use tracepoints;kmemtrace;enable and use this tracer;To enable and use this tracer;Similar;0.8726144433021545;0.5851861238479614;
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;I find it more readable  ;personal opinion;Similar;0.8534252643585205;0.32594358921051025;
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;improved alignment handling;improved;Similar;0.8334860801696777;0.5242627263069153;
I find it more readable  ;personal opinion;improved alignment handling;improved;Similar;0.8286669254302979;0.4809322953224182;
Drop it  ;if you want;reduce list scanning;slob;Similar;0.8101249933242798;0.6972525119781494;
fix bogus ksize calculation fix  ;SLOB;fix bogus ksize calculation;bogus;Similar;0.9872711896896362;0.846623420715332;
fix bogus ksize calculation fix  ;SLOB;fix;SLOB=y && SMP=y fix;Similar;0.8096705675125122;0.6227595210075378;
fix bogus ksize calculation;bogus;fix;SLOB=y && SMP=y fix;Similar;0.822981595993042;0.48978644609451294;
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;remove useless ctor parameter and reorder parameters;useless;Similar;0.8588235378265381;0.8851944804191589;
remove useless ctor parameter and reorder parameters;useless;reduce list scanning;slob;Similar;0.8205522298812866;0.7383036017417908;
remove useless ctor parameter and reorder parameters;useless;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.803428590297699;0.9298909306526184;
reduce list scanning;slob;Cleanup zeroing allocations;Slab allocators;Similar;0.8207494616508484;0.6148500442504883;
remove bigblock tracking;slob;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Similar;0.8203655481338501;0.7713018655776978;
