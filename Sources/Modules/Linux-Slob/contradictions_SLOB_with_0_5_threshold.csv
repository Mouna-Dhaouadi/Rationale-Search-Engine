Decision1;Rationale1;Decision2;Rationale2;Relationship;Alpha
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Use unused field for units;instead;Contradicts;0.5373821258544922
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Contradicts;0.5667910575866699
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.5729324221611023
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.656012237071991
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.7366713881492615
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;They are no longer needed;They have become so simple;Contradicts;0.6363679766654968
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.7002514004707336
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Contradicts;0.7060793042182922
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8184511065483093
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Cleanup zeroing allocations;Slab allocators;Contradicts;0.6195119619369507
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.9546252489089966
__alloc_size attribute was added to allocators;In the effort to help the compiler reason about buffer sizes;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9623862504959106
For most allocations, this works well, as the vast majority of callers are not expecting to use more memory than what they asked for;The vast majority of callers are not expecting to use more memory than what they asked for;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.6917594075202942
For most allocations, this works well, as the vast majority of callers are not expecting to use more memory than what they asked for;The vast majority of callers are not expecting to use more memory than what they asked for;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.966339647769928
For most allocations, this works well, as the vast majority of callers are not expecting to use more memory than what they asked for;The vast majority of callers are not expecting to use more memory than what they asked for;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Contradicts;0.6582044959068298
For most allocations, this works well, as the vast majority of callers are not expecting to use more memory than what they asked for;The vast majority of callers are not expecting to use more memory than what they asked for;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9795199632644653
For most allocations, this works well, as the vast majority of callers are not expecting to use more memory than what they asked for;The vast majority of callers are not expecting to use more memory than what they asked for;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.6098948121070862
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;Contradicts;0.9056867957115173
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Contradicts;0.5491878986358643
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Contradicts;0.7494640350341797
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.614800751209259
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Contradicts;0.5493662357330322
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.7961413860321045
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Make dead caches discard free slabs immediately;slub;Contradicts;0.705373227596283
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Contradicts;0.7036742568016052
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9757909178733826
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.555792510509491
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.6022250056266785
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.7438472509384155
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6169750690460205
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Remove kmemtrace ftrace plugin;tracing;Contradicts;0.5884694457054138
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Contradicts;0.5356642007827759
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Contradicts;0.8599486351013184
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9037435054779053
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Cleanup zeroing allocations;Slab allocators;Contradicts;0.7338818907737732
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8492074608802795
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8476876616477966
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9802173376083374
it wants to minimize the number of calls to krealloc(), so it just uses ksize() plus some additional bytes, forcing the realloc into the next bucket size  ;so it can learn how large it is now;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.5649020671844482
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.5885317921638489
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Contradicts;0.7174647450447083
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9744083881378174
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9120969176292419
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;Cleanup zeroing allocations;Slab allocators;Contradicts;0.7014736533164978
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.5201261043548584
Use all the space available in the chosen bucket immediately;to avoid needing to reallocate later;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5959196090698242
kmalloc(size) might give us more room than requested;might give us more room than requested;patch ensures alignment on all arches and cache sizes;Still;Contradicts;0.5286523699760437
kmalloc(size) might give us more room than requested;might give us more room than requested;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.5880364179611206
kmalloc(size) might give us more room than requested;might give us more room than requested;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.7675953507423401
kmalloc(size) might give us more room than requested;might give us more room than requested;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9772728085517883
kmalloc(size) might give us more room than requested;might give us more room than requested;They are no longer needed;They have become so simple;Contradicts;0.5656084418296814
kmalloc(size) might give us more room than requested;might give us more room than requested;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.5663026571273804
kmalloc(size) might give us more room than requested;might give us more room than requested;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6108250021934509
kmalloc(size) might give us more room than requested;might give us more room than requested;Remove kmemtrace ftrace plugin;tracing;Contradicts;0.5719647407531738
kmalloc(size) might give us more room than requested;might give us more room than requested;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.8100513219833374
kmalloc(size) might give us more room than requested;might give us more room than requested;Handle that separately in krealloc();separately;Contradicts;0.5597889423370361
kmalloc(size) might give us more room than requested;might give us more room than requested;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9337910413742065
kmalloc(size) might give us more room than requested;might give us more room than requested;Cleanup zeroing allocations;Slab allocators;Contradicts;0.907028317451477
kmalloc(size) might give us more room than requested;might give us more room than requested;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.7482835650444031
kmalloc(size) might give us more room than requested;might give us more room than requested;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8679873943328857
kmalloc(size) might give us more room than requested;might give us more room than requested;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.615911602973938
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.5381609201431274
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.6735512614250183
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.8502709865570068
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9264101982116699
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5670173764228821
Put skb_shared_info exactly at the end of allocated zone;to allow max possible filling before reallocation;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7268535494804382
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Use unused field for units;instead;Contradicts;0.5630873441696167
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.670224666595459
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5122755765914917
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9094468951225281
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5204084515571594
"the ""how much was actually allocated?"" question is answered after the allocation";the compiler hinting is not in an easy place to make the association any more;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8094385862350464
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;Use unused field for units;instead;Contradicts;0.5849506855010986
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;Contradicts;0.6967347860336304
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;patch ensures alignment on all arches and cache sizes;Still;Contradicts;0.9339113831520081
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.6803680658340454
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Contradicts;0.5430221557617188
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.754400908946991
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;Make dead caches discard free slabs immediately;slub;Contradicts;0.6404414176940918
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.5198154449462891
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Contradicts;0.6182847619056702
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Contradicts;0.6590659022331238
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.6865615248680115
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Contradicts;0.5827713012695312
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.7085406184196472
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Contradicts;0.7374900579452515
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Contradicts;0.5764950513839722
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9524972438812256
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;Cleanup zeroing allocations;Slab allocators;Contradicts;0.6711031794548035
This mismatch between the compilers view of the buffer length and the codes intention about how much it is going to actually use has already caused problems;caused problems;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7781780362129211
"reordering the use of the ""actual size"" information";fix this;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.6319221258163452
"reordering the use of the ""actual size"" information";fix this;slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Contradicts;0.5221922993659973
"reordering the use of the ""actual size"" information";fix this;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.5748732089996338
"reordering the use of the ""actual size"" information";fix this;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.6216422915458679
"reordering the use of the ""actual size"" information";fix this;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6219873428344727
"reordering the use of the ""actual size"" information";fix this;record page flag overlays explicitly;slob;Contradicts;0.5325931310653687
"reordering the use of the ""actual size"" information";fix this;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9083121418952942
"reordering the use of the ""actual size"" information";fix this;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.7084787487983704
"reordering the use of the ""actual size"" information";fix this;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5075159668922424
"reordering the use of the ""actual size"" information";fix this;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6562978029251099
"Code can ask ""how large an allocation would I get for a given size?""";instead;Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Contradicts;0.6498215794563293
"Code can ask ""how large an allocation would I get for a given size?""";instead;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.5804431438446045
"Code can ask ""how large an allocation would I get for a given size?""";instead;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.6533037424087524
"Code can ask ""how large an allocation would I get for a given size?""";instead;Make dead caches discard free slabs immediately;slub;Contradicts;0.6327791213989258
"Code can ask ""how large an allocation would I get for a given size?""";instead;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9846328496932983
"Code can ask ""how large an allocation would I get for a given size?""";instead;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Contradicts;0.6537258625030518
"Code can ask ""how large an allocation would I get for a given size?""";instead;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5799632668495178
"Code can ask ""how large an allocation would I get for a given size?""";instead;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.5021377205848694
"Code can ask ""how large an allocation would I get for a given size?""";instead;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Contradicts;0.7915301322937012
"Code can ask ""how large an allocation would I get for a given size?""";instead;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Contradicts;0.5716122984886169
"Code can ask ""how large an allocation would I get for a given size?""";instead;record page flag overlays explicitly;slob;Contradicts;0.5314375162124634
"Code can ask ""how large an allocation would I get for a given size?""";instead;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8734322190284729
"Code can ask ""how large an allocation would I get for a given size?""";instead;Cleanup zeroing allocations;Slab allocators;Contradicts;0.7923702001571655
"Code can ask ""how large an allocation would I get for a given size?""";instead;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.5184332728385925
"Code can ask ""how large an allocation would I get for a given size?""";instead;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8188288807868958
"Code can ask ""how large an allocation would I get for a given size?""";instead;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9324833154678345
"Code can ask ""how large an allocation would I get for a given size?""";instead;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.70900559425354
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.6633997559547424
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.7286154627799988
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.8906416296958923
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6244518756866455
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.8381441831588745
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7269384264945984
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Cleanup zeroing allocations;Slab allocators;Contradicts;0.6385684013366699
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.9264259338378906
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Contradicts;0.5953770875930786
Introduce kmalloc_size_roundup();"serve this function so we can start replacing the ""anticipatory resizing"" uses of ksize()";Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9250497221946716
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.7740892767906189
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Make dead caches discard free slabs immediately;slub;Contradicts;0.5861098170280457
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.8365750908851624
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.512437105178833
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;Contradicts;0.6811966896057129
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Adding this mask;fixes the bug;Contradicts;0.7161940932273865
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.702516496181488
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.908570408821106
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.6093275547027588
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Contradicts;0.761355996131897
Remove export symbol and move declaration to mm/slab.h  ;we dont want to grow its callers;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6843248605728149
unify NUMA and UMA version of tracepoints;mm/slab_common;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.6588835120201111
unify NUMA and UMA version of tracepoints;mm/slab_common;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.7486365437507629
unify NUMA and UMA version of tracepoints;mm/slab_common;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.5463024973869324
unify NUMA and UMA version of tracepoints;mm/slab_common;They are no longer needed;They have become so simple;Contradicts;0.5152578353881836
unify NUMA and UMA version of tracepoints;mm/slab_common;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.6414958834648132
unify NUMA and UMA version of tracepoints;mm/slab_common;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.7300122976303101
unify NUMA and UMA version of tracepoints;mm/slab_common;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.7853326797485352
unify NUMA and UMA version of tracepoints;mm/slab_common;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Contradicts;0.5453510880470276
unify NUMA and UMA version of tracepoints;mm/slab_common;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7450764775276184
unify NUMA and UMA version of tracepoints;mm/slab_common;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5505293011665344
unify NUMA and UMA version of tracepoints;mm/slab_common;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8351673483848572
unify NUMA and UMA version of tracepoints;mm/slab_common;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7479442954063416
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Contradicts;0.6735952496528625
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Contradicts;0.7078264951705933
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.5031030178070068
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9137552976608276
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5446622371673584
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.5402049422264099
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.6308030486106873
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.5081981420516968
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5122838616371155
Make kmalloc_track_caller() wrapper of kmalloc_node_track_caller();-;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9267287850379944
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.6540933847427368
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.7861581444740295
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.7073764801025391
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;record page flag overlays explicitly;slob;Contradicts;0.5495207905769348
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8069032430648804
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5080752372741699
Remove CONFIG_NUMA ifdefs for common kmalloc functions;Now that slab_alloc_node() is available for SLAB when CONFIG_NUMA=n;handle SLAB_PANIC flag;slob;Contradicts;0.5009456872940063
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Contradicts;0.5992593765258789
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.5852388143539429
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Make dead caches discard free slabs immediately;slub;Contradicts;0.6295731067657471
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6251569986343384
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7029181122779846
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Cleanup zeroing allocations;Slab allocators;Contradicts;0.873107373714447
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.5799148678779602
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;handle SLAB_PANIC flag;slob;Contradicts;0.7812344431877136
Slab caches marked with SLAB_ACCOUNT force accounting for every allocation from this cache  ;even if __GFP_ACCOUNT flag is not passed;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.898834228515625
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.6147570013999939
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9781634211540222
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;They are no longer needed;They have become so simple;Contradicts;0.780529260635376
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9275133609771729
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;Cleanup zeroing allocations;Slab allocators;Contradicts;0.7328052520751953
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.650513768196106
increase the minimum slab alignment to 16;When CONFIG_KASAN_HW_TAGS is enabled;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9352408051490784
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Contradicts;0.517669677734375
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.5366774797439575
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.6970361471176147
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.8928127884864807
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.6181172132492065
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5690077543258667
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.6367822289466858
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.5405483841896057
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.646992564201355
This happens even if MTE is not supported in hardware or disabled via kasan=off;which creates an unnecessary memory overhead in those cases;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6205485463142395
waiting for quiescence after a full Android boot;with a kernel built with CONFIG_KASAN_HW_TAGS;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.7817453742027283
waiting for quiescence after a full Android boot;with a kernel built with CONFIG_KASAN_HW_TAGS;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6614412665367126
waiting for quiescence after a full Android boot;with a kernel built with CONFIG_KASAN_HW_TAGS;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9376110434532166
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Contradicts;0.5054463744163513
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;patch ensures alignment on all arches and cache sizes;Still;Contradicts;0.6988194584846497
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.5366728901863098
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Contradicts;0.5320817232131958
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Contradicts;0.5874436497688293
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8587898015975952
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Contradicts;0.5840904116630554
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5026701092720032
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;handle SLAB_PANIC flag;slob;Contradicts;0.6366188526153564
list_lru does not need the capability of tracking every memcg on every superblock;For these systems with huge container counts;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.5297974348068237
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Use unused field for units;instead;Contradicts;0.583296000957489
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Contradicts;0.520256519317627
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Contradicts;0.5823036432266235
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Contradicts;0.5422288179397583
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;configure and build (select SLOB allocator);SLOB allocator;Contradicts;0.654743492603302
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.6261645555496216
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9707039594650269
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;can be done in __kmem_cache_shutdown;What is done there;Contradicts;0.6132687926292419
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;They are no longer needed;They have become so simple;Contradicts;0.6651018261909485
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6727170348167419
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Remove kmemtrace ftrace plugin;tracing;Contradicts;0.6837533116340637
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.9647458791732788
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;remove useless ctor parameter and reorder parameters;useless;Contradicts;0.5511481761932373
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7455199956893921
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Cleanup zeroing allocations;Slab allocators;Contradicts;0.8333035111427307
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.6079309582710266
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.6419770121574402
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Contradicts;0.5340884923934937
Introduce kmem_cache_alloc_lru to allocate objects and its list_lru;to allocate objects;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6795235276222229
Remove unnecessary page_mapcount_reset() function call;unnecessary;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.569562554359436
Remove unnecessary page_mapcount_reset() function call;unnecessary;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.549922525882721
Remove unnecessary page_mapcount_reset() function call;unnecessary;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Contradicts;0.8416944742202759
Remove unnecessary page_mapcount_reset() function call;unnecessary;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Contradicts;0.6517841815948486
Remove unnecessary page_mapcount_reset() function call;unnecessary;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Contradicts;0.5223180055618286
Remove unnecessary page_mapcount_reset() function call;unnecessary;Adding this mask;fixes the bug;Contradicts;0.6013787388801575
Remove unnecessary page_mapcount_reset() function call;unnecessary;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.6960336565971375
Remove unnecessary page_mapcount_reset() function call;unnecessary;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5389007329940796
Remove unnecessary page_mapcount_reset() function call;unnecessary;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5152730941772461
Use unused field for units;instead;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8524829745292664
Use unused field for units;instead;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5687386393547058
use struct folio instead of struct page;Where non-slab page can appear;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.6123788356781006
use struct folio instead of struct page;Where non-slab page can appear;renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Contradicts;0.5546106696128845
use struct folio instead of struct page;Where non-slab page can appear;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9491190910339355
use struct folio instead of struct page;Where non-slab page can appear;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.5498842000961304
use struct folio instead of struct page;Where non-slab page can appear;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Contradicts;0.9774367809295654
use struct folio instead of struct page;Where non-slab page can appear;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.944092333316803
use struct folio instead of struct page;Where non-slab page can appear;record page flag overlays explicitly;slob;Contradicts;0.5017993450164795
use struct folio instead of struct page;Where non-slab page can appear;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8705927133560181
use struct folio instead of struct page;Where non-slab page can appear;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5129035711288452
use struct folio instead of struct page;Where non-slab page can appear;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Contradicts;0.5466601848602295
use struct folio instead of struct page;Where non-slab page can appear;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7118566036224365
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Contradicts;0.6095916032791138
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Add a return parameter to slob_page_alloc();to signal that the list was modified;Contradicts;0.6267157793045044
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Contradicts;0.6631324887275696
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Contradicts;0.5032385587692261
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.6460573673248291
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";"Introduce much more complexity to both SLUB and memcg
";than this small patch;Contradicts;0.5964913964271545
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Contradicts;0.6349412202835083
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Contradicts;0.8636457920074463
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Contradicts;0.7843788862228394
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Contradicts;0.6080183386802673
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Adding this mask;fixes the bug;Contradicts;0.5186110734939575
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.5167804956436157
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Contradicts;0.8497216105461121
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Contradicts;0.6073328852653503
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.6834676861763
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Contradicts;0.5404009222984314
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Introduce krealloc();reallocates memory while keeping the contents unchanged;Contradicts;0.7381487488746643
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6074941754341125
dont introduce wrappers for PageSlobFree in mm/slab.h;"just for the single callers being wrappers in mm/slob.c

Decision: fix NULL pointer deference";Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.9624718427658081
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.6284649968147278
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Make dead caches discard free slabs immediately;slub;Contradicts;0.6448370218276978
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Contradicts;0.9691905975341797
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.9512932896614075
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Contradicts;0.5311436653137207
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9260067343711853
By using the slab type instead of the page type  ;we make it obvious that this can only be called for slabs;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5726661682128906
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Add mem_dump_obj() to print source of memory block;to print source of memory block;Contradicts;0.5973784327507019
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;Contradicts;0.5802240371704102
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;configure and build (select SLOB allocator);SLOB allocator;Contradicts;0.6377613544464111
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.7234016060829163
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Contradicts;0.6934769749641418
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Contradicts;0.6050968170166016
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Contradicts;0.8542044758796692
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Contradicts;0.8807080984115601
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Contradicts;0.5406239032745361
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;record page flag overlays explicitly;slob;Contradicts;0.7257320880889893
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.637483537197113
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5031119585037231
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Introduce krealloc();reallocates memory while keeping the contents unchanged;Contradicts;0.8021864891052246
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6261801719665527
Dont build mm_dump_obj() on CONFIG_PRINTK=n kernels;CONFIG_PRINTK=n;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.6465439200401306
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.521523118019104
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Contradicts;0.60051029920578
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7704411745071411
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5724986791610718
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Introduce krealloc();reallocates memory while keeping the contents unchanged;Contradicts;0.6568197011947632
mem_dump_obj() messages will be suppressed;kernels built with CONFIG_PRINTK=n;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6140856742858887
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.7075103521347046
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.5265877842903137
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.7260324954986572
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Contradicts;0.5293133854866028
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Adding this mask;fixes the bug;Contradicts;0.6319335103034973
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.7477927207946777
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.9579660892486572
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9537898898124695
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.7516125440597534
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5699414610862732
This patch adds the slab name to trace_kmem_cache_free();to trace_kmem_cache_free;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9784733057022095
kmem_cache_free;We can use it to understand what the RCU core is going to free;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Contradicts;0.5342904329299927
kmem_cache_free;We can use it to understand what the RCU core is going to free;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7169405817985535
kmem_cache_free;We can use it to understand what the RCU core is going to free;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5564562082290649
Add mem_dump_obj() to print source of memory block;to print source of memory block;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.5529880523681641
Add mem_dump_obj() to print source of memory block;to print source of memory block;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.6409107446670532
Add mem_dump_obj() to print source of memory block;to print source of memory block;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9758762717247009
Add mem_dump_obj() to print source of memory block;to print source of memory block;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.7403663396835327
Add mem_dump_obj() to print source of memory block;to print source of memory block;Adding this mask;fixes the bug;Contradicts;0.5097987055778503
Add mem_dump_obj() to print source of memory block;to print source of memory block;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5430025458335876
Add mem_dump_obj() to print source of memory block;to print source of memory block;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8074954152107239
Add mem_dump_obj() to print source of memory block;to print source of memory block;remove bigblock tracking;slob;Contradicts;0.5677950978279114
Add mem_dump_obj() to print source of memory block;to print source of memory block;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8011776208877563
Add mem_dump_obj() to print source of memory block;to print source of memory block;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5245586037635803
Add mem_dump_obj() to print source of memory block;to print source of memory block;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Contradicts;0.6772862672805786
Add mem_dump_obj() to print source of memory block;to print source of memory block;Introduce krealloc();reallocates memory while keeping the contents unchanged;Contradicts;0.6542348265647888
Add mem_dump_obj() to print source of memory block;to print source of memory block;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8919273614883423
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Contradicts;0.5780690312385559
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.6534929871559143
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.9435585141181946
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.5362494587898254
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.6875821352005005
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.7980445623397827
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.594833493232727
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Contradicts;0.5388739109039307
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8028975129127502
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5936536192893982
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;remove bigblock tracking;slob;Contradicts;0.5882992744445801
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5832322835922241
There are kernel facilities such as per-CPU reference counts that give error messages in generic handlers or callbacks  ;messages are unenlightening;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8899502754211426
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.6091472506523132
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Contradicts;0.7419548034667969
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9698386788368225
this is not a problem when creating a new use of this facility  ;the bug is almost certainly in the code implementing that new use;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.6730575561523438
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Contradicts;0.8366007208824158
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.758578360080719
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Contradicts;0.5042306780815125
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.9900942444801331
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Contradicts;0.6763970851898193
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Make dead caches discard free slabs immediately;slub;Contradicts;0.7772019505500793
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9990659356117249
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.6361181735992432
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.8965041041374207
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;They are no longer needed;They have become so simple;Contradicts;0.7901772856712341
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.7489897012710571
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.8760831952095032
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Remove kmemtrace ftrace plugin;tracing;Contradicts;0.7072638869285583
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.6387489438056946
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Contradicts;0.9510998129844666
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Contradicts;0.5053648352622986
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Drop it  ;if you want;Contradicts;0.7171129584312439
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Fix to return wrong pointer;Fix;Contradicts;0.5417203307151794
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9818750023841858
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Contradicts;0.5266987085342407
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;remove bigblock tracking;slob;Contradicts;0.7818430066108704
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.5983770489692688
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Contradicts;0.7928907871246338
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.7909368276596069
"This pointer can reference the middle of the block as well as the beginning of the block, as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is
";as needed by things like RCU callback functions and timer handlers that might not know where the beginning of the memory block is;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9342201948165894
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.5504202246665955
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.5759872198104858
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9939306974411011
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.856963574886322
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;They are no longer needed;They have become so simple;Contradicts;0.7559564709663391
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;The bit should be passed to trace_kmalloc_node()  ;as well;Contradicts;0.5941762924194336
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5597943663597107
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.736060380935669
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.6149070858955383
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.7269843220710754
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Contradicts;0.630947470664978
These functions and handlers can use mem_dump_obj();to print out better hints as to where the problem might lie;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7089410424232483
The information printed can depend on kernel configuration;depend on kernel configuration;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.5766916871070862
The information printed can depend on kernel configuration;depend on kernel configuration;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.7215110063552856
The information printed can depend on kernel configuration;depend on kernel configuration;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.6906814575195312
The information printed can depend on kernel configuration;depend on kernel configuration;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6753068566322327
The information printed can depend on kernel configuration;depend on kernel configuration;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.5272973775863647
The information printed can depend on kernel configuration;depend on kernel configuration;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Contradicts;0.6360561847686768
The information printed can depend on kernel configuration;depend on kernel configuration;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8987814784049988
The information printed can depend on kernel configuration;depend on kernel configuration;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8838474154472351
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Make dead caches discard free slabs immediately;slub;Contradicts;0.5693950653076172
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.5554159879684448
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;record page flag overlays explicitly;slob;Contradicts;0.5214090347290039
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.685538649559021
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5166023969650269
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5231490135192871
the allocation return address can be printed only for slab and slub  ;only when the necessary debug has been enabled;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9128775596618652
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Contradicts;0.736480176448822
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Contradicts;0.6250584125518799
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.6022226214408875
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;configure and build (select SLOB allocator);SLOB allocator;Contradicts;0.5676056146621704
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;remove an unnecessary check for __GFP_ZERO;unnecessary;Contradicts;0.5453376173973083
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Contradicts;0.6376349329948425
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.5662810206413269
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Contradicts;0.5190911293029785
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Make dead caches discard free slabs immediately;slub;Contradicts;0.6481877565383911
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9881039261817932
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.6454864740371704
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.7488633990287781
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.5951479077339172
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.7022290229797363
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.8505476713180542
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Contradicts;0.5153448581695557
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7760168313980103
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Cleanup zeroing allocations;Slab allocators;Contradicts;0.6871570944786072
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Contradicts;0.5886619687080383
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8358720541000366
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5808694958686829
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;handle SLAB_PANIC flag;slob;Contradicts;0.5014193654060364
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9890685677528381
build with CONFIG_DEBUG_SLAB=y and either use sizes with ample space to the next power of two or use the SLAB_STORE_USER when creating the kmem_cache structure;For slab;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.5214962363243103
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.8253954648971558
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9674761295318604
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.5350973010063171
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.6644866466522217
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6177457571029663
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Remove kmemtrace ftrace plugin;tracing;Contradicts;0.6003393530845642
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.9028729796409607
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;use tracepoints;kmemtrace;Contradicts;0.5395117998123169
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.5589329600334167
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;remove bigblock tracking;slob;Contradicts;0.8014223575592041
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.7572370171546936
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;handle SLAB_PANIC flag;slob;Contradicts;0.5245348215103149
Use CONFIG_STACKTRACE for slub  ;to enable printing of the allocation-time stack trace.;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.891501247882843
have some use eventually for annotations in drivers/gpu;I might;I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Contradicts;0.5873603224754333
have some use eventually for annotations in drivers/gpu;I might;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.9094549417495728
have some use eventually for annotations in drivers/gpu;I might;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9979288578033447
have some use eventually for annotations in drivers/gpu;I might;They are no longer needed;They have become so simple;Contradicts;0.9902605414390564
have some use eventually for annotations in drivers/gpu;I might;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.6078111529350281
have some use eventually for annotations in drivers/gpu;I might;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.7265130281448364
have some use eventually for annotations in drivers/gpu;I might;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.5718245506286621
have some use eventually for annotations in drivers/gpu;I might;remove useless ctor parameter and reorder parameters;useless;Contradicts;0.6740401983261108
have some use eventually for annotations in drivers/gpu;I might;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9369899034500122
have some use eventually for annotations in drivers/gpu;I might;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8265944123268127
have some use eventually for annotations in drivers/gpu;I might;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6823614835739136
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.5022885203361511
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.594865620136261
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.544318437576294
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9059980511665344
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5549352765083313
Updates gfpflags_allow_blocking();Theres a ton of callers all over the place for that already.;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7020624876022339
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.7234941124916077
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.6536169648170471
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Make dead caches discard free slabs immediately;slub;Contradicts;0.7639827132225037
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9611005783081055
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;They are no longer needed;They have become so simple;Contradicts;0.8145231604576111
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9642477035522461
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.6339960694313049
Slab pages will be shared between multiple cgroups;global and node counters will reflect the total number of slab pages;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6342512369155884
The size of slab memory shouldnt exceed 4Gb on 32-bit machines;it will fit into atomic_long_t we use for vmstats;move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Contradicts;0.5189151167869568
The size of slab memory shouldnt exceed 4Gb on 32-bit machines;it will fit into atomic_long_t we use for vmstats;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6230747699737549
The size of slab memory shouldnt exceed 4Gb on 32-bit machines;it will fit into atomic_long_t we use for vmstats;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8914187550544739
The size of slab memory shouldnt exceed 4Gb on 32-bit machines;it will fit into atomic_long_t we use for vmstats;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9921751618385315
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.624894380569458
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Contradicts;0.6230041980743408
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.5813269019126892
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9993209838867188
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.6804606318473816
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.6076342463493347
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6797012090682983
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.5473997592926025
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Contradicts;0.6803785562515259
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Contradicts;0.5537043213844299
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9068729877471924
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Cleanup zeroing allocations;Slab allocators;Contradicts;0.7163862586021423
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;remove bigblock tracking;slob;Contradicts;0.514095664024353
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.7491453289985657
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9303285479545593
I want to use this in a memory allocation tracker in drm for stuff thats tied to the lifetime of a drm_device, not the underlying struct device;slab does this already;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.7608565092086792
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;patch ensures alignment on all arches and cache sizes;Still;Contradicts;0.9797922372817993
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.5717662572860718
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.642184317111969
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Contradicts;0.5707431435585022
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;Contradicts;0.6487889289855957
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Contradicts;0.8060001730918884
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Contradicts;0.7842060923576355
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.5382251143455505
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5640799403190613
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Contradicts;0.5491282343864441
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8010009527206421
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5021255612373352
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5761238932609558
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;handle SLAB_PANIC flag;slob;Contradicts;0.6581331491470337
CONFIG_SLUB_DEBUG or CONFIG_SLOB, and blocks stop being aligned  ;blocks stop being aligned;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.986481785774231
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.7878105044364929
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.628864586353302
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9533118009567261
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Contradicts;0.9639650583267212
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.5558034777641296
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;They are no longer needed;They have become so simple;Contradicts;0.9176321029663086
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.7218859791755676
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.8713335990905762
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Contradicts;0.8567066192626953
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.845459520816803
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;Cleanup zeroing allocations;Slab allocators;Contradicts;0.785898745059967
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8488527536392212
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.6910046339035034
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Contradicts;0.6154543161392212
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Contradicts;0.5122910737991333
make the kmalloc() alignment to size explicitly guaranteed for power-of-two sizes under all configurations;provide to mm users what they need without difficult workarounds or own reimplementations;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9121804237365723
patch ensures alignment on all arches and cache sizes;Still;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.6612666249275208
patch ensures alignment on all arches and cache sizes;Still;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.558165967464447
patch ensures alignment on all arches and cache sizes;Still;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.8399131298065186
patch ensures alignment on all arches and cache sizes;Still;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Contradicts;0.5313376188278198
patch ensures alignment on all arches and cache sizes;Still;They are no longer needed;They have become so simple;Contradicts;0.681245744228363
patch ensures alignment on all arches and cache sizes;Still;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Contradicts;0.8828772902488708
patch ensures alignment on all arches and cache sizes;Still;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.969732403755188
patch ensures alignment on all arches and cache sizes;Still;Cleanup zeroing allocations;Slab allocators;Contradicts;0.6046679615974426
patch ensures alignment on all arches and cache sizes;Still;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.7870621085166931
patch ensures alignment on all arches and cache sizes;Still;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9167347550392151
improve memory accounting;improve;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5117638111114502
improve memory accounting;improve;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8786198496818542
improve memory accounting;improve;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.7035133242607117
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;"SLAB doesnt actually use page allocator directly
";no change there;Contradicts;0.5319294333457947
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.7306126356124878
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Make dead caches discard free slabs immediately;slub;Contradicts;0.9095397591590881
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9943108558654785
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Contradicts;0.5175560712814331
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.6897056698799133
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.5759497880935669
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.5307762026786804
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6560696959495544
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.6171524524688721
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Contradicts;0.7941218018531799
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Contradicts;0.687408447265625
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9480730295181274
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Cleanup zeroing allocations;Slab allocators;Contradicts;0.7125806212425232
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.7737246155738831
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.6867711544036865
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7781996726989746
Modifying a counter on page allocation and freeing should be acceptable even for the small system scenarios SLOB is intended for;SLOB is intended for;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.5331627726554871
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.7228286862373352
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Contradicts;0.7530319690704346
it might look like a memory leak  ;As they also dont appear in /proc/slabinfo;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.929329514503479
"SLAB doesnt actually use page allocator directly
";no change there;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.7358561754226685
"SLAB doesnt actually use page allocator directly
";no change there;Make dead caches discard free slabs immediately;slub;Contradicts;0.8475188612937927
"SLAB doesnt actually use page allocator directly
";no change there;resurrects approach first proposed in [1];To fix this issue;Contradicts;0.5408115386962891
"SLAB doesnt actually use page allocator directly
";no change there;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Contradicts;0.5730726718902588
"SLAB doesnt actually use page allocator directly
";no change there;Ran a ktest.pl config_bisect;Came up with this config as the problem;Contradicts;0.5239942073822021
"SLAB doesnt actually use page allocator directly
";no change there;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5510035157203674
"SLAB doesnt actually use page allocator directly
";no change there;Remove kmemtrace ftrace plugin;tracing;Contradicts;0.5485532283782959
"SLAB doesnt actually use page allocator directly
";no change there;record page flag overlays explicitly;slob;Contradicts;0.5719630718231201
"SLAB doesnt actually use page allocator directly
";no change there;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.6755780577659607
"SLAB doesnt actually use page allocator directly
";no change there;Cleanup zeroing allocations;Slab allocators;Contradicts;0.7657079100608826
"SLAB doesnt actually use page allocator directly
";no change there;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5476763248443604
"SLAB doesnt actually use page allocator directly
";no change there;Introduce krealloc();reallocates memory while keeping the contents unchanged;Contradicts;0.515902042388916
"SLAB doesnt actually use page allocator directly
";no change there;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7089638113975525
"SLAB doesnt actually use page allocator directly
";no change there;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.5601615309715271
add three helpers, convert the appropriate places;these three patches;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.5970277190208435
add three helpers, convert the appropriate places;these three patches;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.9141575694084167
add three helpers, convert the appropriate places;these three patches;They are no longer needed;They have become so simple;Contradicts;0.7896054983139038
add three helpers, convert the appropriate places;these three patches;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5447145104408264
add three helpers, convert the appropriate places;these three patches;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8615742921829224
add three helpers, convert the appropriate places;these three patches;Cleanup zeroing allocations;Slab allocators;Contradicts;0.8855900168418884
add three helpers, convert the appropriate places;these three patches;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.5375852584838867
add three helpers, convert the appropriate places;these three patches;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.515866219997406
add three helpers, convert the appropriate places;these three patches;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5394725799560547
to find out the size of a potentially huge page;Its unnecessarily hard;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9473156332969666
to find out the size of a potentially huge page;Its unnecessarily hard;Cleanup zeroing allocations;Slab allocators;Contradicts;0.7331249713897705
union of slab_list and lru;slab_list and lru are the same bits;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.754418671131134
union of slab_list and lru;slab_list and lru are the same bits;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9042767286300659
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Contradicts;0.5081178545951843
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.5747276544570923
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Make dead caches discard free slabs immediately;slub;Contradicts;0.7371703386306763
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9652844071388245
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;They are no longer needed;They have become so simple;Contradicts;0.6463083624839783
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.8581426739692688
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.8893717527389526
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.5065914392471313
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9612968564033508
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.6457554697990417
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5540286302566528
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Contradicts;0.5893628597259521
verify that this change is safe to do by examining the object file produced from slob.c before and after this patch is applied;safe to do;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9930033087730408
configure and build (select SLOB allocator);SLOB allocator;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.5417665839195251
configure and build (select SLOB allocator);SLOB allocator;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9937343001365662
configure and build (select SLOB allocator);SLOB allocator;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6773340106010437
configure and build (select SLOB allocator);SLOB allocator;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7937030792236328
configure and build (select SLOB allocator);SLOB allocator;Cleanup zeroing allocations;Slab allocators;Contradicts;0.754704475402832
configure and build (select SLOB allocator);SLOB allocator;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8081879615783691
configure and build (select SLOB allocator);SLOB allocator;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8666681051254272
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.8736962676048279
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Make dead caches discard free slabs immediately;slub;Contradicts;0.5459408760070801
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9878675937652588
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Contradicts;0.7922229766845703
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.6603769063949585
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Contradicts;0.9552050828933716
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.9920023679733276
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5527316331863403
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;use tracepoints;kmemtrace;Contradicts;0.5253307819366455
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Drop it  ;if you want;Contradicts;0.6452850699424744
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7805050015449524
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5674906969070435
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;remove bigblock tracking;slob;Contradicts;0.5382775664329529
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.578328013420105
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Contradicts;0.7679585218429565
Use slab_list list_head instead of the lru list_head for maintaining lists of slabs  ;maintain;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8461611270904541
Add a return parameter to slob_page_alloc();to signal that the list was modified;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.6771941184997559
Add a return parameter to slob_page_alloc();to signal that the list was modified;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9072942733764648
Add a return parameter to slob_page_alloc();to signal that the list was modified;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.5081700086593628
Add a return parameter to slob_page_alloc();to signal that the list was modified;The bit should be passed to trace_kmalloc_node()  ;as well;Contradicts;0.6733141541481018
Add a return parameter to slob_page_alloc();to signal that the list was modified;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.530040442943573
Add a return parameter to slob_page_alloc();to signal that the list was modified;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8916429281234741
Add a return parameter to slob_page_alloc();to signal that the list was modified;Cleanup zeroing allocations;Slab allocators;Contradicts;0.6060529351234436
Add a return parameter to slob_page_alloc();to signal that the list was modified;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8074012398719788
Add a return parameter to slob_page_alloc();to signal that the list was modified;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5047765970230103
Add a return parameter to slob_page_alloc();to signal that the list was modified;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9924339056015015
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.725719153881073
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9131327271461487
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.5644760131835938
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8327754735946655
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5560277104377747
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.6006276607513428
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5902146697044373
Add a return parameter to slob_page_alloc()  ;to signal that the allocation used up the whole page and that the page was removed from the free list;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9885826110839844
remove an unnecessary check for __GFP_ZERO;unnecessary;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.6727736592292786
remove an unnecessary check for __GFP_ZERO;unnecessary;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Contradicts;0.7527661919593811
remove an unnecessary check for __GFP_ZERO;unnecessary;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8558506965637207
remove an unnecessary check for __GFP_ZERO;unnecessary;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5397602319717407
Remove the unnecessary NULL pointer check;unnecessary;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.6589553356170654
Remove the unnecessary NULL pointer check;unnecessary;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.6756705641746521
Remove the unnecessary NULL pointer check;unnecessary;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Contradicts;0.5166616439819336
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;to be applied to the file;a file by file comparison of the scanner;Contradicts;0.8967819809913635
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.5037083625793457
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9125401377677917
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.5249657034873962
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6061062216758728
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.5397496819496155
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Contradicts;0.5206802487373352
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9123870730400085
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.7008822560310364
Add SPDX GPL-2.0 license identifier to files with no license;License cleanup;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.881475031375885
to be applied to the file;a file by file comparison of the scanner;All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.5485494136810303
to be applied to the file;a file by file comparison of the scanner;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8295587301254272
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);All documentation files were explicitly excluded;explicitly excluded;Contradicts;0.690517246723175
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Make dead caches discard free slabs immediately;slub;Contradicts;0.7348293662071228
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.793546199798584
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.7454689741134644
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.7392256855964661
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.6462834477424622
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9449973106384277
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Cleanup zeroing allocations;Slab allocators;Contradicts;0.5248233079910278
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.5504686236381531
Make and config files were included as candidates if they contained >5 lines of source;File already had some variant of a license header in it (even if <5 lines);Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8804388046264648
All documentation files were explicitly excluded;explicitly excluded;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Contradicts;0.504959762096405
All documentation files were explicitly excluded;explicitly excluded;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Contradicts;0.5079247355461121
All documentation files were explicitly excluded;explicitly excluded;record page flag overlays explicitly;slob;Contradicts;0.5033729076385498
All documentation files were explicitly excluded;explicitly excluded;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.886951744556427
All documentation files were explicitly excluded;explicitly excluded;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5015905499458313
All documentation files were explicitly excluded;explicitly excluded;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.726546049118042
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.6073489785194397
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6357958316802979
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9373584985733032
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5959734916687012
the top level COPYING file license applied;both scanners couldnt find any license traces, file was considered to have no license information;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7189528942108154
For non */uapi/* files;that summary was;record page flag overlays explicitly;slob;Contradicts;0.5344982743263245
For non */uapi/* files;that summary was;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.909846305847168
For non */uapi/* files;that summary was;Cleanup zeroing allocations;Slab allocators;Contradicts;0.595429539680481
For non */uapi/* files;that summary was;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6594884991645813
Kate also obtained a third independent scan of the 4.13 code base from FOSSology, and compared selected files where the other two scanners disagreed against that SPDX file;to see if there was new insights;Make dead caches discard free slabs immediately;slub;Contradicts;0.5571992993354797
Kate also obtained a third independent scan of the 4.13 code base from FOSSology, and compared selected files where the other two scanners disagreed against that SPDX file;to see if there was new insights;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.6410925984382629
Kate also obtained a third independent scan of the 4.13 code base from FOSSology, and compared selected files where the other two scanners disagreed against that SPDX file;to see if there was new insights;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6323954463005066
Kate also obtained a third independent scan of the 4.13 code base from FOSSology, and compared selected files where the other two scanners disagreed against that SPDX file;to see if there was new insights;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.5360458493232727
Kate also obtained a third independent scan of the 4.13 code base from FOSSology, and compared selected files where the other two scanners disagreed against that SPDX file;to see if there was new insights;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Contradicts;0.5212987661361694
Kate also obtained a third independent scan of the 4.13 code base from FOSSology, and compared selected files where the other two scanners disagreed against that SPDX file;to see if there was new insights;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9215519428253174
Kate also obtained a third independent scan of the 4.13 code base from FOSSology, and compared selected files where the other two scanners disagreed against that SPDX file;to see if there was new insights;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6188861727714539
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Make dead caches discard free slabs immediately;slub;Contradicts;0.5727008581161499
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9306683540344238
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.5482572317123413
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Ran a ktest.pl config_bisect;Came up with this config as the problem;Contradicts;0.6512941718101501
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6703615188598633
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Remove kmemtrace ftrace plugin;tracing;Contradicts;0.5810028910636902
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.608063817024231
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Contradicts;0.7673028111457825
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7760758996009827
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5152733325958252
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.600004255771637
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8930091261863708
Thomas wrote a script to parse the csv files and add the proper SPDX tag to the file  ;in the format that the file expected;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9157143235206604
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.5856475830078125
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5749587416648865
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9583714604377747
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.569076418876648
Greg ran the script using the .csv files to generate the patches;to detect more types of files automatically and to distinguish between header and source .c files;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7311025261878967
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.5566115379333496
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.5111323595046997
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8730669617652893
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Cleanup zeroing allocations;Slab allocators;Contradicts;0.6488645672798157
reducing the irq states;allows reducing the irq states and will reduce the amount of __bfs() lookups we do;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5325916409492493
And by moving this annotation out of the lockdep code;it becomes easier for the mm people to extend;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8965219259262085
"reported chasing a bug
";their assumption that SLAB_DESTROY_BY_RCU provided an existence guarantee;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.7841262221336365
"reported chasing a bug
";their assumption that SLAB_DESTROY_BY_RCU provided an existence guarantee;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Contradicts;0.5241100192070007
"reported chasing a bug
";their assumption that SLAB_DESTROY_BY_RCU provided an existence guarantee;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.6924890875816345
"reported chasing a bug
";their assumption that SLAB_DESTROY_BY_RCU provided an existence guarantee;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.7038708329200745
"reported chasing a bug
";their assumption that SLAB_DESTROY_BY_RCU provided an existence guarantee;fixes the previous fix  ;completely wrong on closer inspection;Contradicts;0.5829500555992126
"reported chasing a bug
";their assumption that SLAB_DESTROY_BY_RCU provided an existence guarantee;record page flag overlays explicitly;slob;Contradicts;0.6108285784721375
"reported chasing a bug
";their assumption that SLAB_DESTROY_BY_RCU provided an existence guarantee;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9131593704223633
"reported chasing a bug
";their assumption that SLAB_DESTROY_BY_RCU provided an existence guarantee;Cleanup zeroing allocations;Slab allocators;Contradicts;0.7587686777114868
"reported chasing a bug
";their assumption that SLAB_DESTROY_BY_RCU provided an existence guarantee;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5494828820228577
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Make dead caches discard free slabs immediately;slub;Contradicts;0.8704993724822998
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Contradicts;0.5151524543762207
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Contradicts;0.5766794085502625
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9276356101036072
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Cleanup zeroing allocations;Slab allocators;Contradicts;0.6622934341430664
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;handle SLAB_PANIC flag;slob;Contradicts;0.6956628561019897
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9816285371780396
SLAB_DESTROY_BY_RCU only prevents freeing of an entire slab of blocks;Instead;SLOB saving nearly half a megabyte of RAM;Comparison for otherwise identical builds;Contradicts;0.5293071269989014
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Contradicts;0.5298765301704407
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9215783476829529
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Contradicts;0.6011541485786438
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.693871021270752
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Contradicts;0.6243740916252136
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.937440037727356
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Contradicts;0.5160273909568787
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.6464129090309143
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Contradicts;0.6779058575630188
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;handle SLAB_PANIC flag;slob;Contradicts;0.6990261077880859
renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU;to avoid future instances of this sort of confusion;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9146199822425842
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Contradicts;0.5208610892295837
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9579802751541138
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7354674339294434
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.5173448920249939
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.7325337529182434
call synchronize_sched() just once;its enough to call it just once - after setting cpu_partial for all caches and before shrinking them;Introduce krealloc();reallocates memory while keeping the contents unchanged;Contradicts;0.5032926797866821
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.8857367634773254
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8204348087310791
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Contradicts;0.501798152923584
move it out of the slab_mutex  ;which we have to hold for iterating over the slab cache list;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7229248881340027
Adjust API to return type int instead of previously type bool;previously type bool;Make dead caches discard free slabs immediately;slub;Contradicts;0.5762251019477844
Adjust API to return type int instead of previously type bool;previously type bool;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.7151997685432434
Adjust API to return type int instead of previously type bool;previously type bool;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Contradicts;0.6354071497917175
Adjust API to return type int instead of previously type bool;previously type bool;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7848641276359558
Adjust API to return type int instead of previously type bool;previously type bool;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5994874238967896
Adjust API to return type int instead of previously type bool;previously type bool;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.5738013386726379
Adjust API to return type int instead of previously type bool;previously type bool;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6166908144950867
allow future extension of the bulk alloc API;is done to;Make dead caches discard free slabs immediately;slub;Contradicts;0.6100432872772217
allow future extension of the bulk alloc API;is done to;slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Contradicts;0.6129622459411621
allow future extension of the bulk alloc API;is done to;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.8308543562889099
allow future extension of the bulk alloc API;is done to;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9428834915161133
allow future extension of the bulk alloc API;is done to;Cleanup zeroing allocations;Slab allocators;Contradicts;0.6625176668167114
allow future extension of the bulk alloc API;is done to;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.6634401679039001
allow future extension of the bulk alloc API;is done to;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5431903004646301
We always end up returning some objects at the cost of another cmpxchg  ;Else;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7766534090042114
We always end up returning some objects at the cost of another cmpxchg  ;Else;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5481492877006531
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Make dead caches discard free slabs immediately;slub;Contradicts;0.8043422698974609
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9188286662101746
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Contradicts;0.602617084980011
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Contradicts;0.5314247608184814
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.641045331954956
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.9814220666885376
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.9362034797668457
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Remove kmemtrace ftrace plugin;tracing;Contradicts;0.8332684636116028
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.8292407989501953
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Contradicts;0.5396087169647217
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Drop it  ;if you want;Contradicts;0.8553001880645752
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Contradicts;0.8780235648155212
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.6988031268119812
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Cleanup zeroing allocations;Slab allocators;Contradicts;0.8735116124153137
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Contradicts;0.5983997583389282
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;remove bigblock tracking;slob;Contradicts;0.7552961111068726
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.9301564693450928
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5776052474975586
"To keep compatible with future users of this API linking against an older kernel when using the new flag, we need to return the number of allocated objects with this API change.
";to keep compatible;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9918644428253174
hiding potentially buggy callers;except temporarily;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Contradicts;0.5409283638000488
hiding potentially buggy callers;except temporarily;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5838075280189514
hiding potentially buggy callers;except temporarily;record page flag overlays explicitly;slob;Contradicts;0.9269780516624451
hiding potentially buggy callers;except temporarily;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9941297769546509
hiding potentially buggy callers;except temporarily;Cleanup zeroing allocations;Slab allocators;Contradicts;0.7045354247093201
hiding potentially buggy callers;except temporarily;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.6764026880264282
hiding potentially buggy callers;except temporarily;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5500184297561646
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9656724333763123
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.9246424436569214
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;They are no longer needed;They have become so simple;Contradicts;0.9011450409889221
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6958662867546082
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.826890230178833
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8134469389915466
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5178853869438171
Add the basic infrastructure for alloc/free operations on pointer arrays;for alloc/free operations on pointer arrays;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8087613582611084
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Make dead caches discard free slabs immediately;slub;Contradicts;0.8838971257209778
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Contradicts;0.5179818868637085
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9388223886489868
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;"Introduce much more complexity to both SLUB and memcg
";than this small patch;Contradicts;0.8595430254936218
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;They are no longer needed;They have become so simple;Contradicts;0.6262189745903015
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.5053678154945374
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.824357807636261
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9211874008178711
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5621338486671448
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.784638524055481
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.7444119453430176
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Contradicts;0.7989118695259094
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Contradicts;0.5318984389305115
These optimizations may avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;avoid taking locks repeatedly and bypass metadata creation if all objects in slab pages can be used to provide the objects required;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8764486908912659
Make dead caches discard free slabs immediately;slub;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9501873254776001
Make dead caches discard free slabs immediately;slub;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5276462435722351
Make dead caches discard free slabs immediately;slub;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7513158321380615
Make dead caches discard free slabs immediately;slub;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5623407363891602
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.5625158548355103
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Contradicts;0.6629330515861511
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Contradicts;0.647179901599884
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.6227638721466064
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Contradicts;0.592022716999054
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6545658111572266
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Contradicts;0.5208913087844849
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.6854087114334106
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Contradicts;0.5111916065216064
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.536994993686676
SLUB may store empty slabs in per cpu/node partial lists instead of freeing them immediately;To speed up further allocations;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9389086365699768
resurrects approach first proposed in [1];To fix this issue;Drop it  ;if you want;Contradicts;0.8065934181213379
resurrects approach first proposed in [1];To fix this issue;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.6027023792266846
resurrects approach first proposed in [1];To fix this issue;Cleanup zeroing allocations;Slab allocators;Contradicts;0.6720295548439026
resurrects approach first proposed in [1];To fix this issue;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.6707009673118591
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Contradicts;0.7313470244407654
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Contradicts;0.5158522725105286
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.945139467716217
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.5741432905197144
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.7476227283477783
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.8495438098907471
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Remove kmemtrace ftrace plugin;tracing;Contradicts;0.5676925182342529
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.6128057241439819
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Contradicts;0.7823872566223145
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7422247529029846
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8596804738044739
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8418518900871277
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Contradicts;0.7109384536743164
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9799485206604004
setting kmem_caches cpu_partial and min_partial constants to 0 and tuning put_cpu_partial();Achieving dropping frozen empty slabs immediately if cpu_partial = 0;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Contradicts;0.5053730607032776
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Contradicts;0.9665731191635132
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.6140462756156921
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6510019898414612
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.5019355416297913
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8058668375015259
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5748278498649597
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Contradicts;0.5136386752128601
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.6368547677993774
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.7657750248908997
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Contradicts;0.8056074380874634
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Introduce krealloc();reallocates memory while keeping the contents unchanged;Contradicts;0.5120747685432434
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9176263809204102
make put_cpu_partial() call unfreeze_partials() after freezing a slab that belongs to an offline memory cgroup;touch relatively cold functions, only make this specific call;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.6918994188308716
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7756388783454895
slab freezing exists;to avoid moving slabs from/to a partial list on free/alloc;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7750696539878845
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Contradicts;0.5473054647445679
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;This is possible now;the acquisition of the mutex was moved into kmem_cache_create();Contradicts;0.6514778733253479
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;record page flag overlays explicitly;slob;Contradicts;0.5279905200004578
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8234112858772278
I do not see it as an option;SLUB shrinker would be too costly to call since SLUB does not keep free slabs on a separate list;Cleanup zeroing allocations;Slab allocators;Contradicts;0.6267034411430359
"Introduce much more complexity to both SLUB and memcg
";than this small patch;turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Contradicts;0.9426321387290955
"Introduce much more complexity to both SLUB and memcg
";than this small patch;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.7881226539611816
"Introduce much more complexity to both SLUB and memcg
";than this small patch;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.621435284614563
"Introduce much more complexity to both SLUB and memcg
";than this small patch;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Contradicts;0.9651522040367126
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Contradicts;0.5617202520370483
"Introduce much more complexity to both SLUB and memcg
";than this small patch;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Contradicts;0.5950132012367249
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Remove various small accessors;various small;Contradicts;0.6882387399673462
"Introduce much more complexity to both SLUB and memcg
";than this small patch;They are no longer needed;They have become so simple;Contradicts;0.8024079203605652
"Introduce much more complexity to both SLUB and memcg
";than this small patch;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.8566843867301941
"Introduce much more complexity to both SLUB and memcg
";than this small patch;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.8074542880058289
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Remove kmemtrace ftrace plugin;tracing;Contradicts;0.7574340105056763
"Introduce much more complexity to both SLUB and memcg
";than this small patch;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.8738677501678467
"Introduce much more complexity to both SLUB and memcg
";than this small patch;annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Contradicts;0.747775673866272
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Drop it  ;if you want;Contradicts;0.8790267109870911
"Introduce much more complexity to both SLUB and memcg
";than this small patch;fixes the previous fix  ;completely wrong on closer inspection;Contradicts;0.7582770586013794
"Introduce much more complexity to both SLUB and memcg
";than this small patch;reduce external fragmentation by using three free lists;slob;Contradicts;0.5666072964668274
"Introduce much more complexity to both SLUB and memcg
";than this small patch;By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Contradicts;0.7725284695625305
"Introduce much more complexity to both SLUB and memcg
";than this small patch;reduce list scanning;slob;Contradicts;0.7068670988082886
"Introduce much more complexity to both SLUB and memcg
";than this small patch;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.6842060685157776
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Cleanup zeroing allocations;Slab allocators;Contradicts;0.9148642420768738
"Introduce much more complexity to both SLUB and memcg
";than this small patch;remove bigblock tracking;slob;Contradicts;0.6112834215164185
"Introduce much more complexity to both SLUB and memcg
";than this small patch;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.6181765198707581
"Introduce much more complexity to both SLUB and memcg
";than this small patch;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Contradicts;0.6555438041687012
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Contradicts;0.8960066437721252
"Introduce much more complexity to both SLUB and memcg
";than this small patch;added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Contradicts;0.9566624164581299
"Introduce much more complexity to both SLUB and memcg
";than this small patch;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Contradicts;0.5553997755050659
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Contradicts;0.6557457447052002
"Introduce much more complexity to both SLUB and memcg
";than this small patch;This patch cleans up the slab header definitions;cleans up the slab header definitions;Contradicts;0.9131921529769897
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9968679547309875
"Introduce much more complexity to both SLUB and memcg
";than this small patch;simplifies SLOB;at this point slob may be broken;Contradicts;0.9974291920661926
"Introduce much more complexity to both SLUB and memcg
";than this small patch;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.7484807968139648
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Contradicts;0.5345221161842346
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5636863112449646
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8664659857749939
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Cleanup zeroing allocations;Slab allocators;Contradicts;0.6455520987510681
Thanks to list_lru reparenting, we no longer keep entries for offline cgroups in per-memcg arrays;We do not have to bother if a per-memcg cache will be shrunk a bit later than it could be;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8551166653633118
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.8829828500747681
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.9354886412620544
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8945327401161194
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;remove bigblock tracking;slob;Contradicts;0.8292170166969299
Track caller if tracing or slab debugging is enabled;Tracing or slab debugging is enabled;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8872312307357788
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;Contradicts;0.8956598043441772
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8894119262695312
its okay to change this situation;default slab allocator, SLUB, doesnt use this technique;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5174862742424011
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;The new rule is: page->lru is what you use;if you want to keep your page on a list;Contradicts;0.5123478770256042
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.5158497095108032
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.7959843277931213
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9051123857498169
turn on/off CONFIG_DEBUG_SLAB without full kernel build and remove some complicated #if defintion;After this change, simplify the process and reduce complexity;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8720118403434753
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Adding this mask;fixes the bug;Contradicts;0.6635454297065735
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Remove kmemtrace ftrace plugin;tracing;Contradicts;0.5426254868507385
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.6435367465019226
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;record page flag overlays explicitly;slob;Contradicts;0.6255995035171509
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7273305654525757
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;remove bigblock tracking;slob;Contradicts;0.5152056217193604
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;SLOB to be used on SMP  ;allows;Contradicts;0.6056306958198547
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;handle SLAB_PANIC flag;slob;Contradicts;0.7300044298171997
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Contradicts;0.7568955421447754
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8932028412818909
slab_mutex for kmem_cache_shrink is removed;after its applied, there is no need in taking the slab_mutex;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.5649142861366272
unioned together;Conveniently;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.816429853439331
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;They are no longer needed;They have become so simple;Contradicts;0.5370222926139832
That code can use them interchangeably;Gets horribly confusing like with this nugget from slab.c;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8764646649360657
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;They are no longer needed;They have become so simple;Contradicts;0.5746685266494751
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.517264187335968
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Adding this mask;fixes the bug;Contradicts;0.653580904006958
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5721811056137085
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9720414876937866
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.6374254822731018
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Contradicts;0.5518561601638794
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Contradicts;0.5324779152870178
This patch makes the slab and slub code use page->lru universally instead of mixing ->list and ->lru;mixing ->list and ->lru;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8588226437568665
The new rule is: page->lru is what you use;if you want to keep your page on a list;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.941838800907135
The new rule is: page->lru is what you use;if you want to keep your page on a list;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.7371642589569092
The new rule is: page->lru is what you use;if you want to keep your page on a list;Introduce krealloc();reallocates memory while keeping the contents unchanged;Contradicts;0.6032487154006958
The new rule is: page->lru is what you use;if you want to keep your page on a list;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7414581775665283
function naming changes;requires some;kmalloc_track_caller() is correctly implemented;tracing the specified caller;Contradicts;0.5279755592346191
function naming changes;requires some;record page flag overlays explicitly;slob;Contradicts;0.5363600254058838
function naming changes;requires some;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9381111860275269
function naming changes;requires some;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6821291446685791
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5370556116104126
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.5158014893531799
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Cleanup zeroing allocations;Slab allocators;Contradicts;0.765215277671814
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8510309457778931
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Introduce krealloc();reallocates memory while keeping the contents unchanged;Contradicts;0.6950199007987976
Call the ctor() method only if allocation succeeded;The solution is to only call the ctor() method if allocation succeeded;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6993089318275452
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Contradicts;0.6333078145980835
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;Contradicts;0.701798141002655
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Contradicts;0.762467086315155
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.6988510489463806
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6806787252426147
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.5147769451141357
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Contradicts;0.5870480537414551
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.746446430683136
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8365981578826904
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.7204014658927917
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Introduce krealloc();reallocates memory while keeping the contents unchanged;Contradicts;0.5963924527168274
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9604132771492004
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Contradicts;0.5985427498817444
Rename the function names page_xchg_last_nid(), page_last_nid(), and reset_page_last_nid() to a struct_field_op style pattern;The function names were judged to be inconsistent;Compare the sizes of vmlinux-slab and vmlinux-slob;to analyze memory consumption;Contradicts;0.6800087094306946
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6184072494506836
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8926836252212524
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5800727009773254
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.647499144077301
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Contradicts;0.5343320965766907
rename reset_page_mapcount() to page_mapcount_reset();it looked jarring to have them beside each other;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9139598608016968
a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9675874710083008
a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;Cleanup zeroing allocations;Slab allocators;Contradicts;0.6299936771392822
a rename would likely cause more conflicts than it is worth  ;as it is used throughout the arch code;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Contradicts;0.9282041788101196
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Contradicts;0.5434845685958862
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;They are no longer needed;They have become so simple;Contradicts;0.8122329711914062
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.6557981967926025
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;The bit should be passed to trace_kmalloc_node()  ;as well;Contradicts;0.5564919710159302
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.7624412178993225
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.5684237480163574
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;Drop it  ;if you want;Contradicts;0.8578987717628479
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";Contradicts;0.9372754693031311
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7998515367507935
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;Cleanup zeroing allocations;Slab allocators;Contradicts;0.7244152426719666
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.7139215469360352
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.6027430891990662
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Contradicts;0.5945587754249573
If we start chaining caches, this information will always be more trustworthy than whatever is passed into the function.;this information will always be more trustworthy;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8147100806236267
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5783350467681885
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Contradicts;0.5579327940940857
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9600780606269836
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Contradicts;0.5057023763656616
The definition of ARCH_SLAB_MINALIGN is architecture dependent and can be either of type size_t or int;architecture dependent;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.995528519153595
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.950813889503479
using the size_t type to compare them is safe  ;"both are always small positive integer numbers; gets rid of the warning";Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5301423072814941
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5403754711151123
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Contradicts;0.5234773755073547
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.952382504940033
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Contradicts;0.552786111831665
this patch also drops the EXPORT_SYMBOL tag;Since now its static, inline, header-defined function;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6314594149589539
Improve trace accuracy;by correctly tracing reported size;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8202458024024963
Improve trace accuracy;by correctly tracing reported size;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5111565589904785
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Contradicts;0.5034830570220947
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.6304504871368408
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8395751714706421
ksize() can simply return PAGE_SIZE << compound_order(page);There is no need to store the allocated size;Introduce krealloc();reallocates memory while keeping the contents unchanged;Contradicts;0.6171324253082275
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;They are no longer needed;They have become so simple;Contradicts;0.5829350352287292
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Adding this mask;fixes the bug;Contradicts;0.5881181359291077
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;The bit should be passed to trace_kmalloc_node()  ;as well;Contradicts;0.5549830794334412
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Remove kmemtrace ftrace plugin;tracing;Contradicts;0.5877450108528137
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.8930714130401611
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Contradicts;0.5703516006469727
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7214019894599915
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5011724829673767
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;remove bigblock tracking;slob;Contradicts;0.8842617869377136
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8849815130233765
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Contradicts;0.5190150141716003
Add the function `__kmalloc_track_caller()`  ;This breaks Pekkas slab/next tree;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8774846792221069
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.792232096195221
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9558471441268921
Fix build breakage in __kmalloc_node_track_caller;gfp is undeclared;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6755674481391907
kmalloc_track_caller() is correctly implemented;tracing the specified caller;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.8039181232452393
kmalloc_track_caller() is correctly implemented;tracing the specified caller;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8014075756072998
kmalloc_track_caller() is correctly implemented;tracing the specified caller;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5399473905563354
kmalloc_track_caller() is correctly implemented;tracing the specified caller;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.7625993490219116
kmalloc_track_caller() is correctly implemented;tracing the specified caller;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.6648181676864624
kmalloc_track_caller() is correctly implemented;tracing the specified caller;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Contradicts;0.5404911041259766
This will allow us to push more processing into common code later;improve readability;They are no longer needed;They have become so simple;Contradicts;0.5993545651435852
This will allow us to push more processing into common code later;improve readability;No need to zero mapping since it is no longer in use;it is no longer in use;Contradicts;0.5044237375259399
This will allow us to push more processing into common code later;improve readability;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.7482886910438538
This will allow us to push more processing into common code later;improve readability;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.951223611831665
This will allow us to push more processing into common code later;improve readability;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.855420708656311
This will allow us to push more processing into common code later;improve readability;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5630333423614502
This will allow us to push more processing into common code later;improve readability;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8584675788879395
can be done in __kmem_cache_shutdown;What is done there;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5554064512252808
can be done in __kmem_cache_shutdown;What is done there;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7073094844818115
can be done in __kmem_cache_shutdown;What is done there;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5757401585578918
This affects RCU handling;somewhat;They are no longer needed;They have become so simple;Contradicts;0.5525497198104858
This affects RCU handling;somewhat;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6168181896209717
This affects RCU handling;somewhat;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9551873207092285
This affects RCU handling;somewhat;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8406051397323608
This affects RCU handling;somewhat;simplifies SLOB;at this point slob may be broken;Contradicts;0.5302174687385559
This is possible now;the acquisition of the mutex was moved into kmem_cache_create();This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8603705167770386
Fix early boot kernel crash;Slob;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6362166404724121
Fix early boot kernel crash;Slob;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.951394259929657
Fix early boot kernel crash;Slob;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.503980815410614
Remove various small accessors;various small;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7540931701660156
Remove various small accessors;various small;Cleanup zeroing allocations;Slab allocators;Contradicts;0.6401636004447937
Remove various small accessors;various small;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6076209545135498
They are no longer needed;They have become so simple;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9024390578269958
No need to zero mapping since it is no longer in use;it is no longer in use;record page flag overlays explicitly;slob;Contradicts;0.6084077954292297
No need to zero mapping since it is no longer in use;it is no longer in use;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9066739082336426
No need to zero mapping since it is no longer in use;it is no longer in use;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.8315215706825256
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Ran a ktest.pl config_bisect;Came up with this config as the problem;Contradicts;0.581570029258728
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.8638598918914795
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9150137305259705
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Cleanup zeroing allocations;Slab allocators;Contradicts;0.58708655834198
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8447117209434509
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Contradicts;0.9106647968292236
This cleans up numerous of typecasts in slob.c and makes readers aware of slobs use of page struct fields;makes readers aware of slobs use of page struct fields;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9913896918296814
cleans up some bitrot in slob.c;Also;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5542585849761963
cleans up some bitrot in slob.c;Also;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9581893086433411
cleans up some bitrot in slob.c;Also;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5868945121765137
cleans up some bitrot in slob.c;Also;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Contradicts;0.8544133305549622
cleans up some bitrot in slob.c;Also;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8053083419799805
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9009993076324463
move duplicated code in <asm/atomic.h> to <linux/atomic.h>;to avoid duplication of code;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5687344074249268
Fix gfp flags passed to lockdep;lockdep;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.6634016036987305
Fix gfp flags passed to lockdep;lockdep;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8140078186988831
Fix gfp flags passed to lockdep;lockdep;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.584738552570343
Ran a ktest.pl config_bisect;Came up with this config as the problem;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.5990659594535828
Ran a ktest.pl config_bisect;Came up with this config as the problem;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8436808586120605
Ran a ktest.pl config_bisect;Came up with this config as the problem;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6624770760536194
Adding this mask;fixes the bug;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7902057766914368
Adding this mask;fixes the bug;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.5171304941177368
Adding this mask;fixes the bug;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7333119511604309
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.544495165348053
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8692144751548767
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5473839640617371
fix gfp flags for order-0 page allocations  ;fix for order-0 page allocations;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6033430695533752
The bit should be passed to trace_kmalloc_node()  ;as well;We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Contradicts;0.7813575863838196
The bit should be passed to trace_kmalloc_node()  ;as well;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Contradicts;0.5044665336608887
The bit should be passed to trace_kmalloc_node()  ;as well;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8413199186325073
The bit should be passed to trace_kmalloc_node()  ;as well;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.7132235169410706
The bit should be passed to trace_kmalloc_node()  ;as well;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5955972075462341
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;we dont care about the RCU head state before passing it to call_rcu();anyway;Contradicts;0.672950029373169
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8314471244812012
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5937869548797607
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Contradicts;0.57355135679245
The result shows an improvement of 1 MB!;tested it on a embeded system with 64 MB, found this path is never called during kernel bootup.;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6143234968185425
we dont care about the RCU head state before passing it to call_rcu();anyway;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7774407267570496
we dont care about the RCU head state before passing it to call_rcu();anyway;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.7597309350967407
Remove kmemtrace ftrace plugin;tracing;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Contradicts;0.7739337682723999
Remove kmemtrace ftrace plugin;tracing;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Contradicts;0.5664041042327881
Remove kmemtrace ftrace plugin;tracing;record page flag overlays explicitly;slob;Contradicts;0.6371341943740845
Remove kmemtrace ftrace plugin;tracing;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8844156265258789
Remove kmemtrace ftrace plugin;tracing;Introduce krealloc();reallocates memory while keeping the contents unchanged;Contradicts;0.6120425462722778
Remove kmemtrace ftrace plugin;tracing;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5536534190177917
Remove kmemtrace ftrace plugin;tracing;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.7418060898780823
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Contradicts;0.5279965400695801
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Contradicts;0.6681429743766785
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Contradicts;0.870836079120636
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;enable and use this tracer;To enable and use this tracer;Contradicts;0.7187740206718445
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;record page flag overlays explicitly;slob;Contradicts;0.6217154264450073
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Contradicts;0.6409737467765808
We remove kmemtrace  ;kmemtrace has been superseded by kmem trace events and perf-kmem;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.6253171563148499
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Contradicts;0.500083327293396
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9393587708473206
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5513806939125061
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;remove bigblock tracking;slob;Contradicts;0.5229687690734863
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.756755530834198
fix RCU-callback-after-kmem_cache_destroy problem in sl[aou]b  ;to address the issue;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.5974043607711792
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9136688113212585
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Cleanup zeroing allocations;Slab allocators;Contradicts;0.6744027137756348
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.7634913325309753
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Contradicts;0.6483657956123352
adds alloc_pages_exact_node() that only checks the nid;To avoid a comparison and branch, this patch;Introduce krealloc();reallocates memory while keeping the contents unchanged;Contradicts;0.5581068396568298
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9327415227890015
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5121643543243408
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.6010063290596008
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9752838015556335
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;simplifies SLOB;at this point slob may be broken;Contradicts;0.6180646419525146
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.505351185798645
SLOB does not correctly account reclaim_state.reclaimed_slab, so it will break memory reclaim;SLOB does not correctly account reclaim_state.reclaimed_slab;SLOB saving nearly half a megabyte of RAM;Comparison for otherwise identical builds;Contradicts;0.633465051651001
refactor code for future changes;Impact;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9304190874099731
refactor code for future changes;Impact;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6091002225875854
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7852595448493958
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5213374495506287
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.5406312942504883
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.514861524105072
include/linux/kmemtrace.h: header file for kmemtrace, include/trace/kmem.h: definition of kmem tracepoints;header file for kmemtrace, definition of kmem tracepoints;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9273539185523987
use tracepoints;kmemtrace;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7131224274635315
use tracepoints;kmemtrace;Cleanup zeroing allocations;Slab allocators;Contradicts;0.6647970080375671
use tracepoints;kmemtrace;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.7228700518608093
use tracepoints;kmemtrace;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7184819579124451
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8945503830909729
annotate reclaim context (__GFP_NOFS), fix SLOB;lockdep;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5956865549087524
fix typo in mm/slob.c;build fix;fix bogus ksize calculation;bogus;Contradicts;0.5914949774742126
fix typo in mm/slob.c;build fix;record page flag overlays explicitly;slob;Contradicts;0.6314852237701416
fix typo in mm/slob.c;build fix;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9427270889282227
fix typo in mm/slob.c;build fix;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8657596707344055
fix lockup in slob_free()  ;lockup;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8766599297523499
fix lockup in slob_free()  ;lockup;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8968831300735474
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.758252739906311
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;remove bigblock tracking;slob;Contradicts;0.6062957644462585
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.5279821157455444
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5958897471427917
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Contradicts;0.6351557970046997
added reclaim context annotation to kswapd, and allocation tracing to slab allocators  ;which may only ever reach the page allocator in rare cases, so it is good to put annotations here too;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7836580276489258
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.5783387422561646
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;Cleanup zeroing allocations;Slab allocators;Contradicts;0.6118874549865723
have some improvement  ;the concept is no less complete than discovery of a locks interrupt contexts;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5198522806167603
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Drop it  ;if you want;Contradicts;0.5220407843589783
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8391439914703369
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();remove bigblock tracking;slob;Contradicts;0.666449785232544
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.6247799396514893
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.8328971862792969
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();handle SLAB_PANIC flag;slob;Contradicts;0.5477482676506042
"Commit 7b2cd92adc5430b0c1adeb120971852b4ea1ab08 (""crypto: api - Fix zeroing on free"")";added modular user of ksize();Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.823966920375824
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;fix bogus ksize calculation;bogus;Contradicts;0.5828122496604919
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9417216777801514
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.6803611516952515
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.6690926551818848
This patch adapts kmemtrace raw events tracing to the unified tracing API;Impact: new tracer plugin;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6276599168777466
enable and use this tracer;To enable and use this tracer;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.5218458771705627
enable and use this tracer;To enable and use this tracer;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8442893028259277
enable and use this tracer;To enable and use this tracer;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8092226386070251
I find it more readable  ;personal opinion;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8624059557914734
Drop it  ;if you want;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8768030405044556
Drop it  ;if you want;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Contradicts;0.9322961568832397
fix bogus ksize calculation fix  ;SLOB;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8521612286567688
fix bogus ksize calculation fix  ;SLOB;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.501695454120636
fixes the previous fix  ;completely wrong on closer inspection;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9093944430351257
fix bogus ksize calculation;bogus;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.8897520303726196
fix bogus ksize calculation;bogus;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5025617480278015
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.728186845779419
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;Cleanup zeroing allocations;Slab allocators;Contradicts;0.6203986406326294
removes the obsolete and no longer used exports of ksize  ;obsolete and no longer used;"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Contradicts;0.6560018658638
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";record page flag overlays explicitly;slob;Contradicts;0.532447874546051
Pass only pointer to the object  ;"Nobody uses this ""feature"", nor does anybody use passed kmem cache in a non-trivial way";This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.6523771286010742
record page flag overlays explicitly;slob;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.895017147064209
record page flag overlays explicitly;slob;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.6913235187530518
record page flag overlays explicitly;slob;handle SLAB_PANIC flag;slob;Contradicts;0.5226145386695862
record page flag overlays explicitly;slob;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6058667302131653
Fix to return wrong pointer;Fix;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.6606596112251282
Fix to return wrong pointer;Fix;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5295740365982056
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.7783966064453125
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Cleanup zeroing allocations;Slab allocators;Contradicts;0.7283831834793091
"slob: fix bug - when slob allocates ""struct kmem_cache"", it does not force alignment";it does not force alignment;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9845719337463379
reduce external fragmentation by using three free lists;slob;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.954697847366333
reduce external fragmentation by using three free lists;slob;Cleanup zeroing allocations;Slab allocators;Contradicts;0.7362632751464844
reduce external fragmentation by using three free lists;slob;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.5063672065734863
reduce external fragmentation by using three free lists;slob;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.6298173666000366
reduce external fragmentation by using three free lists;slob;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6669073104858398
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;fix memory corruption;memory corruption;Contradicts;0.5037733912467957
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9646127223968506
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Cleanup zeroing allocations;Slab allocators;Contradicts;0.6475198864936829
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.688663899898529
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Contradicts;0.6746546030044556
By putting smaller objects on their own list, we greatly reduce overall external fragmentation and increase repeatability;we greatly reduce overall external fragmentation and increase repeatability;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8450557589530945
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.935022234916687
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Cleanup zeroing allocations;Slab allocators;Contradicts;0.7351899147033691
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Contradicts;0.6325991153717041
This reduces total SLOB overhead;from > 50% to ~6% on a simple boot test;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9720154404640198
fix memory corruption;memory corruption;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9242939352989197
remove useless ctor parameter and reorder parameters;useless;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.667277455329895
remove useless ctor parameter and reorder parameters;useless;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.578799307346344
remove useless ctor parameter and reorder parameters;useless;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5040264129638672
remove useless ctor parameter and reorder parameters;useless;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6713718175888062
Handle that separately in krealloc();separately;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.737639844417572
Handle that separately in krealloc();separately;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.9680533409118652
Handle that separately in krealloc();separately;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.7707422971725464
Handle that separately in krealloc();separately;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.756890594959259
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.6361454129219055
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.7728796601295471
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Contradicts;0.7171235680580139
lets use unlikely() for the condition check in SLUBs and SLOBs kfree();Considering kfree(NULL) would normally occur only in error paths and kfree(ZERO_SIZE_PTR) is uncommon as well, so lets use unlikely() to optimize for the common case;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9586025476455688
reduce list scanning;slob;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9303761124610901
starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Contradicts;0.9313117265701294
starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Cleanup zeroing allocations;Slab allocators;Contradicts;0.5542938113212585
starting each page search where the last one left off;to evenly distribute the allocations and greatly shorten the average search;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.525165855884552
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;remove bigblock tracking;slob;Contradicts;0.5879347324371338
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;SLOB to be used on SMP  ;allows;Contradicts;0.7328894138336182
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Contradicts;0.8057064414024353
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Introduce krealloc();reallocates memory while keeping the contents unchanged;Contradicts;0.6435019969940186
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.919784665107727
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.6308109760284424
This rips out support for the dtor pointer from kmem_cache_create() completely and fixes up every single callsite in the kernel  ;there were about 224 callsites;fix;SLOB=y && SMP=y fix;Contradicts;0.7815169095993042
Cleanup zeroing allocations;Slab allocators;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.586736798286438
Cleanup zeroing allocations;Slab allocators;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5298706293106079
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.8795691728591919
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5337818264961243
add the necessary logic to all slab allocators to support __GFP_ZERO;support __GFP_ZERO;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.956187903881073
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5543853044509888
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Contradicts;0.7466922998428345
Make slub return NULL like the other allocators if a too large memory segment is requested via __kmalloc;other allocators;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9893428087234497
improved alignment handling;improved;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.743992269039154
improved alignment handling;improved;[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Contradicts;0.6778901815414429
improved alignment handling;improved;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7094081044197083
Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.667225182056427
Use __alignof__ (unsigned long) for the default alignment  ;This allows relaxed alignment architectures to take better advantage of SLOBs small minimum alignment;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.618537425994873
Allocation size is stored in page->private;makes ksize more accurate than it previously was;we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Contradicts;0.9744053483009338
Allocation size is stored in page->private;makes ksize more accurate than it previously was;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7924201488494873
Allocation size is stored in page->private;makes ksize more accurate than it previously was;Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.5467556715011597
rework freelist handling;slob;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.531612753868103
we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Introduce krealloc();reallocates memory while keeping the contents unchanged;Contradicts;0.6581851840019226
we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7539620995521545
we dont encode seperate size and next fields into each slob block;"use the sign bit to distinguish between ""size"" or ""next""";Introduce a memory-zeroing variant of kmem_cache_alloc;memory-zeroing variant;Contradicts;0.5710402131080627
"size 1 blocks contain a ""next"" offset; others contain the ""size"" in the first unit and ""next"" in the second unit  ";description of how the blocks are structured;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8264259696006775
"align them to word size
";it is best in practice;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6560104489326477
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;"
Use a destructor will BUG()
";"
Any attempt to";Contradicts;0.5673632025718689
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Contradicts;0.531838595867157
make kmalloc use its own slob_block at the front of the allocation  ;to encode allocation size, rather than rely on not overwriting slobs existing header block;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7188541293144226
"Taking a spinlock in a destructor is a bit risky
";since the slab allocators may run the destructors anytime they decide a slab is no longer needed;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9191607236862183
"Taking a spinlock in a destructor is a bit risky
";since the slab allocators may run the destructors anytime they decide a slab is no longer needed;simplifies SLOB;at this point slob may be broken;Contradicts;0.5700864791870117
"
Use a destructor will BUG()
";"
Any attempt to";Introduce krealloc();reallocates memory while keeping the contents unchanged;Contradicts;0.6897792816162109
"
Use a destructor will BUG()
";"
Any attempt to";"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Contradicts;0.681026816368103
"
Use a destructor will BUG()
";"
Any attempt to";Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.8971897959709167
The SLOB allocator should implement SLAB_DESTROY_BY_RCU correctly;even on UP, RCU freeing semantics are not equivalent to simply freeing immediately;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9983367919921875
fix page order calculation on not 4KB page;-;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.6554731130599976
This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Introduce krealloc();reallocates memory while keeping the contents unchanged;Contradicts;0.5293784141540527
This patch fixes it with using get_order() instead of find_order()  ;which is SLOB version of get_order();Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7595569491386414
Remove obsolete SLAB_MUST_HWCACHE_ALIGN;obsolete;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.5346745252609253
handle SLAB_PANIC flag;slob;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9031656384468079
Introduce krealloc();reallocates memory while keeping the contents unchanged;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.86277174949646
added a simple non-optimized version for mm/slob.c for compatibility;compatibility;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7817730903625488
[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9730241894721985
[PATCH] MM: SLOB is broken by recent cleanup of slab.h;recent cleanup of slab.h;SLOB saving nearly half a megabyte of RAM;Comparison for otherwise identical builds;Contradicts;0.7170750498771667
"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Contradicts;0.6959581971168518
"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9905718564987183
"This routine cannot be removed after init in the case of slob.c -- it serves as a timer callback
";it serves as a timer callback;Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;Contradicts;0.860260009765625
Cleanup slab headers / API  ;to allow easy addition of new slab allocators;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.7300252318382263
This patch cleans up the slab header definitions;cleans up the slab header definitions;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9885872602462769
add several empty functions to slob.c and rename slobs kmalloc to __kmalloc;In order to get a greater set of common functions;Slob does not need any special definitions;since we introduce a fallback case;Contradicts;0.9510678648948669
Remove the atomic counter for slab_reclaim_pages and replace the counter and NR_SLAB with two ZVC counter that account for unreclaimable and reclaimable slab pages: NR_SLAB_RECLAIMABLE and NR_SLAB_UNRECLAIMABLE;account for unreclaimable and reclaimable slab pages;[PATCH] mm/slob.c: for_each_possible_cpu(), not NR_CPUS  ;correction in function call;Contradicts;0.6343171000480652
